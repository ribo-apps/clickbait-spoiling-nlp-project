{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhfFBNFlg10i",
        "outputId": "d19c9bf1-c3da-43f7-e486-e4063c1a3f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP_Project/roberta"
      ],
      "metadata": {
        "id": "bUSqr23HmGSW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed582a4-dda4-402c-8503-dde126f73fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP_Project/roberta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets\n",
        "!pip install rouge_score\n",
        "!pip install dill==0.3.5.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-HHUqPIQ3AQ",
        "outputId": "eb1b878a-39a9-457b-9615-2de1c8b95a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dill==0.3.5.1 in /usr/local/lib/python3.10/dist-packages (0.3.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import transformers\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import EncoderDecoderModel\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from transformers import TrainingArguments\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import json\n",
        "import torch"
      ],
      "metadata": {
        "id": "a2sxnHZtKiSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AbTtuShgsOh"
      },
      "outputs": [],
      "source": [
        "training_data_path = '/content/drive/MyDrive/NLP_Project/train_changed.json'\n",
        "test_data_path = '/content/drive/MyDrive/NLP_Project/validation_changed.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkDChssggW8s"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "with open(training_data_path, 'r') as f:\n",
        "    for line in f:\n",
        "        json_line = json.loads(line)\n",
        "        train_data.append(json_line)\n",
        "\n",
        "with open(test_data_path, 'r') as f:\n",
        "    for line in f:\n",
        "        json_line = json.loads(line)\n",
        "        test_data.append(json_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTtCHbW6gW8s",
        "outputId": "24f22905-3779-4376-8ce7-93f938c28c97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3197"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "bGp4NCJ6gW8t",
        "outputId": "130a0ac8-5728-4e1e-dc77-b504241d325b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       targetParagraphs  \\\n",
              "0     2070 is shaping up to be a great year for Moth...   \n",
              "1     Despite common belief, money isn't the key to ...   \n",
              "2     It’s common wisdom. Near gospel really, and no...   \n",
              "3     Boiling rice may seem simple, but there is a v...   \n",
              "4     One of the biggest surprise announcements at A...   \n",
              "...                                                 ...   \n",
              "3192  A long time ago in a galaxy far, far away...Wa...   \n",
              "3193  The Kansas City, Kansas Police Department are ...   \n",
              "3194  Obama looks decades younger already, but what ...   \n",
              "3195  What the HELL?!??? 1. Unless you’re somewhere ...   \n",
              "3196  Rep. Charlie Dent (R-Pa.) came out in support ...   \n",
              "\n",
              "                                                spoiler  \n",
              "0                                                  2070  \n",
              "1                              intellectual stimulation  \n",
              "2     Purpose connects us to something bigger and in...  \n",
              "3                                      in a rice cooker  \n",
              "4     Apple says that if AirPods are lost or stolen,...  \n",
              "...                                                 ...  \n",
              "3192  it hasn’t necessarily taken the wind out of Yo...  \n",
              "3193             It read, \"Thanks for keeping us safe.\"  \n",
              "3194  1. Anti-wrinkle creams will erase the fine lin...  \n",
              "3195                                     @beyoncefan666  \n",
              "3196                          Rep. Charlie Dent (R-Pa.)  \n",
              "\n",
              "[3197 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20762787-96a7-418b-8424-8c94d81b25e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>targetParagraphs</th>\n",
              "      <th>spoiler</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2070 is shaping up to be a great year for Moth...</td>\n",
              "      <td>2070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Despite common belief, money isn't the key to ...</td>\n",
              "      <td>intellectual stimulation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It’s common wisdom. Near gospel really, and no...</td>\n",
              "      <td>Purpose connects us to something bigger and in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Boiling rice may seem simple, but there is a v...</td>\n",
              "      <td>in a rice cooker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>One of the biggest surprise announcements at A...</td>\n",
              "      <td>Apple says that if AirPods are lost or stolen,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>A long time ago in a galaxy far, far away...Wa...</td>\n",
              "      <td>it hasn’t necessarily taken the wind out of Yo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3193</th>\n",
              "      <td>The Kansas City, Kansas Police Department are ...</td>\n",
              "      <td>It read, \"Thanks for keeping us safe.\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3194</th>\n",
              "      <td>Obama looks decades younger already, but what ...</td>\n",
              "      <td>1. Anti-wrinkle creams will erase the fine lin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3195</th>\n",
              "      <td>What the HELL?!??? 1. Unless you’re somewhere ...</td>\n",
              "      <td>@beyoncefan666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3196</th>\n",
              "      <td>Rep. Charlie Dent (R-Pa.) came out in support ...</td>\n",
              "      <td>Rep. Charlie Dent (R-Pa.)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3197 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20762787-96a7-418b-8424-8c94d81b25e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20762787-96a7-418b-8424-8c94d81b25e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20762787-96a7-418b-8424-8c94d81b25e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "drop_columns = ['postText', 'postPlatform', 'targetTitle', 'targetDescription', 'targetKeywords', 'provenance', 'spoilerPositions', 'tags']\n",
        "train_df = pd.DataFrame(train_data).drop(drop_columns, axis=1)\n",
        "test_df = pd.DataFrame(test_data).drop(drop_columns, axis=1)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS_C7zXLgW8y"
      },
      "outputs": [],
      "source": [
        "train_data=Dataset.from_pandas(train_df[:3000])\n",
        "val_data=Dataset.from_pandas(train_df[3000:])\n",
        "test_data=Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hun_VejgW8y"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "tokenizer.bos_token = tokenizer.cls_token\n",
        "tokenizer.eos_token = tokenizer.sep_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=128\n",
        "encoder_max_length=40\n",
        "decoder_max_length=8"
      ],
      "metadata": {
        "id": "rl7taP2vrBqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_to_model_inputs(batch):\n",
        "  inputs = tokenizer(batch[\"targetParagraphs\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
        "  outputs = tokenizer(batch[\"spoiler\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "  batch[\"input_ids\"] = inputs.input_ids\n",
        "  batch[\"attention_mask\"] = inputs.attention_mask\n",
        "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
        "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "  batch[\"labels\"] = outputs.input_ids.copy()\n",
        "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
        "\n",
        "  return batch"
      ],
      "metadata": {
        "id": "XcCONRHzN_uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data = train_data.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"targetParagraphs\", \"spoiler\"]\n",
        ")\n",
        "train_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")\n",
        "\n",
        "\n",
        "val_data = val_data.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"targetParagraphs\", \"spoiler\"]\n",
        ")\n",
        "val_data.set_format(\n",
        "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "96725cc93eb7421eb998479d96405d18",
            "d2cc365952c44dedab6bef898ea53430",
            "5e14698d98134d1f952d0cd9b9f3f1bf",
            "8052d45e020b4d91b1e5e1375197f7f7",
            "fc369d8d05b149d792982454ba7e3e0a",
            "62715876936d44a69b78605f34a184a9",
            "1dff2d9ad0774c1187058be064b7582e",
            "2010585b383746d09ab54e16bb3c26d3",
            "659f74b2187a4cd0a466f9942b970b1b",
            "14d659e8100149c59554d386954b95a5",
            "1dfccfdc433c41fab354a3155b7d0bef",
            "8430c276fac34ac0815e768501b8aee5",
            "f13c542ec2b24418844413297c56beb0",
            "f02b8dc91c5b421b9b3ae9b5ccc7067a",
            "2f7f4ad83b0c4268bcbf71355a74398e",
            "5596b9c1741f43569f7820574f0ce620",
            "d97388821d0d468cab5f950500aa21c7",
            "79e8335658bc47e2ad5951570af4f031",
            "1fbf9e8ced3441f19f7532e8f0b2e7bb",
            "dae2f0bb662641c4829a6c2ed02ca7f0",
            "aaa8abac6d994fb0b80c3d8a9c8c1de9",
            "25b64cc026b54093a9e50127396fd0a7"
          ]
        },
        "id": "F5pFVxgFOjSM",
        "outputId": "9247fc34-b412-4f61-9fe5-57f7467cd66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96725cc93eb7421eb998479d96405d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8430c276fac34ac0815e768501b8aee5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIdkbSw0PwL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_shared = EncoderDecoderModel.from_encoder_decoder_pretrained(\"roberta-base\", \"roberta-base\", tie_encoder_decoder=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i130rzG4PwIR",
        "outputId": "6e143a78-bae1-4fbd-98bb-9a416c80b2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_shared.config.decoder_start_token_id = tokenizer.bos_token_id\n",
        "roberta_shared.config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "roberta_shared.config.max_length = 40\n",
        "roberta_shared.config.early_stopping = True\n",
        "roberta_shared.config.no_repeat_ngram_size = 3\n",
        "roberta_shared.config.length_penalty = 2.0\n",
        "roberta_shared.config.num_beams = 4\n",
        "roberta_shared.config.vocab_size = roberta_shared.config.encoder.vocab_size"
      ],
      "metadata": {
        "id": "KdoN90teP2wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = datasets.load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }"
      ],
      "metadata": {
        "id": "l5N22CrXP6gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/drive/MyDrive/NLP_Project/roberta/output\""
      ],
      "metadata": {
        "id": "TTSxLe7QSQ-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    predict_with_generate=True,\n",
        "    #evaluate_during_training=True,\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    logging_steps=2,\n",
        "    save_steps=16,\n",
        "    eval_steps=500,\n",
        "    warmup_steps=500,\n",
        "    overwrite_output_dir=True,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    num_train_epochs=10,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=roberta_shared,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        ")"
      ],
      "metadata": {
        "id": "PFeLihh2P9js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gof65lIjRyeB",
        "outputId": "c370a874-a8dc-4a60-b458-81e18c02630f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 06:29, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>11.671000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>11.390600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>11.502600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>11.503900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>11.563600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>11.585000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>11.281000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>11.379600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>11.190100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>11.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>10.815900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>10.522600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>10.208400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>10.067800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>9.930000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>9.617100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>9.422500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>9.047900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>8.857000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>8.638600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>8.392000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>8.212600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>8.155200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>7.901300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>7.774700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>7.703300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>7.651100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>7.600900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>7.581500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>7.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>7.343800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>7.388500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>7.377700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>7.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>7.255300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>7.308700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>7.076100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>7.138700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>7.114700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>7.029400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>7.064300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>7.051700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>7.011400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>6.950600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>6.941700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>6.828900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>6.902500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>6.845400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>6.733400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>6.834200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>6.758200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>6.740400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>6.766700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>6.743200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>6.740900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>6.703000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>6.769000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>6.609100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>6.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>6.760100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>6.660700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>6.609400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>6.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>6.415100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>6.508300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>6.525300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>6.562900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>6.607400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>6.467500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>6.475400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>6.513800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>6.450700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>6.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>6.392200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>6.467700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>6.329600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>6.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>6.347600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>6.374100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>6.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>6.345900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>6.326600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>6.240500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>6.267300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>6.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>6.263600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>6.274800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>6.233800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>6.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>6.248100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>6.106100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>6.058100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>6.081900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>5.998800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>6.114700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>6.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>6.052700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>6.092700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>5.922100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>5.862900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>5.922400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>5.994700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>5.858800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>5.906300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>5.855700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>5.724200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>5.764400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>5.761300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>5.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>5.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>5.734500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>5.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>5.630200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>5.737800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>5.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>5.558700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>5.616300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>5.543900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>5.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>5.654800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=240, training_loss=7.209439329306284, metrics={'train_runtime': 391.1186, 'train_samples_per_second': 76.703, 'train_steps_per_second': 0.614, 'total_flos': 825521997600000.0, 'train_loss': 7.209439329306284, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(output_dir+\"/last\")"
      ],
      "metadata": {
        "id": "7ZZvqXynQ26J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riHRVhg0dM-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_shared.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQhqHlVRdM7N",
        "outputId": "f5c35bdc-9c83-4459-bdce-ab2ee32b9e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoderModel(\n",
              "  (encoder): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): RobertaForCausalLM(\n",
              "    (roberta): RobertaModel(\n",
              "      (embeddings): RobertaEmbeddings(\n",
              "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "        (token_type_embeddings): Embedding(1, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): RobertaEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (crossattention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (lm_head): RobertaLMHead(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1"
      ],
      "metadata": {
        "id": "cBCJx9uddMxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(batch):\n",
        "    torch.cuda.empty_cache()\n",
        "    inputs = tokenizer(batch[\"targetParagraphs\"], padding=\"max_length\", truncation=True, max_length=40, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "    outputs = roberta_shared.generate(input_ids, attention_mask=attention_mask)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    batch[\"pred\"] = output_str\n",
        "    return batch"
      ],
      "metadata": {
        "id": "AdYUHMFfdf9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"targetParagraphs\"])\n",
        "pred_str = results[\"pred\"]\n",
        "label_str = results[\"spoiler\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "5e500e6899224abdb155d5e38d74d328",
            "3b7c23d7ce9643a0b9ddab5824f766c7",
            "38e0ff606bb84f4199f388d58ac1f634",
            "b84a8d79d49048ff9ecdebf463bbbf23",
            "26b9b8b97d914734b736c1844fa29d9e",
            "02612a6541b941abb99ef77bbd4f9a27",
            "cc9797d4d2c34ef0b60cdd6d72e4756e",
            "7c3301059fc84ee193a100ca59097c64",
            "ce1712204ae24f7abedf039693ea7748",
            "2c2b5efe7ae546869c5adc65e877fe5f",
            "3569943e443147c7844924c3c03d3fef"
          ]
        },
        "id": "uq3AwHlxdiML",
        "outputId": "cca55fc0-4e47-412d-98f0-f948c219a885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (40) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/800 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e500e6899224abdb155d5e38d74d328"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, pred in enumerate(pred_str):\n",
        "  print(i)\n",
        "  print(f\"pred: {pred}\")\n",
        "  print(f\"spoiler: {label_str[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBU2gUKVduFI",
        "outputId": "cf564cda-a72c-49ac-c07a-72f4a51baac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "pred: Kin million million millions of the\n",
            "spoiler: some of the plot elements are so disturbing that they are making him feel sick\n",
            "1\n",
            "pred: KKin million million million Trump million of of a a\n",
            "spoiler: \"intentionally\" could transform a court case against Phoenix-area Sheriff Joe Arpaio from civil charges to a criminal prosecution\n",
            "2\n",
            "pred: t million million million to a the\n",
            "spoiler: 20%\n",
            "3\n",
            "pred: Lin million million million Trump million the a a a\n",
            "spoiler: Alan Rickman & Rupert Grint CBGB\n",
            "4\n",
            "pred: T million million million of the\n",
            "spoiler: a man who swallowed a 64GB microSD card and then pooped it into a strainer\n",
            "5\n",
            "pred: Kin million million million to the the the\n",
            "spoiler: Sprite\n",
            "6\n",
            "pred: BBCCttt million millions the\n",
            "spoiler: Smoky Paprika-Baked Garbanzo Beans\n",
            "7\n",
            "pred: KLLtt millions of a\n",
            "spoiler: McGonagall was appointed as Dumbledore’s assistant in 1956, not as his replacement.\n",
            "8\n",
            "pred: L million million millions of of a a a\n",
            "spoiler: All the scenes are actually in the movie\n",
            "9\n",
            "pred: Aerer million million millions a a\n",
            "spoiler: \"I had fake relationships, fake fights. I don't care anymore, I can tell you.\n",
            "10\n",
            "pred: Lt million million millions of the\n",
            "spoiler: Elettra Wiedemann extra strength work, so weights, and quite a few planks for my core. My diet stayed pretty much the same, except I cut out sugar for the week of the shoot\n",
            "11\n",
            "pred: Ct Trump Trump million million million Trump million of a a\n",
            "spoiler: Anthony Bourdain\n",
            "12\n",
            "pred: BCinererer million million millioner million of of the\n",
            "spoiler: he'd eaten a peanut butter sandwich and wasn't aware of her peanut allergy\n",
            "13\n",
            "pred: LLererer Trump Trumper million million million Trump million millioner million of a a\n",
            "spoiler: Marcia Clark Does she think Simpson really did it?\n",
            "14\n",
            "pred: KKKererer million million million Trump million million of of a the\n",
            "spoiler: Mitzi Gaynor Beverly Hills, California\n",
            "15\n",
            "pred: Tt million million million Trump million million to of of of\n",
            "spoiler: Dibrom\n",
            "16\n",
            "pred: KKBACCts of a\n",
            "spoiler: They don’t fart\n",
            "17\n",
            "pred: Aterer million million million of a a\n",
            "spoiler: kicked her and got into a fight with her current boyfriend\n",
            "18\n",
            "pred: KBABTtts of a\n",
            "spoiler: Dunham picked the boy up and took him to a Subway to get something to eat. He then took him to the Franklin Police Department.\n",
            "19\n",
            "pred: KKATTtt million million millions of the\n",
            "spoiler: Not only does Aubrey have cerebral palsy, but she was neglected and abused by her biological mother.\n",
            "20\n",
            "pred: t million million million of of of a\n",
            "spoiler: starts later\n",
            "21\n",
            "pred: KBC million million millions the a a a\n",
            "spoiler: The bottom line: Unfortunately, there's not enough solid research out there on whether melatonin supplements are truly an effective and safe way to get to sleep.\n",
            "22\n",
            "pred: KLLerer Trump Trump Trump million million million Trump million of a\n",
            "spoiler: Kevin Williamson said he didn't want to write it\n",
            "23\n",
            "pred: KTt million million millions of a\n",
            "spoiler: Sophie and Riley were considered \"micro-preemies\" and suffered a slew of health issues, like chronic lung disease and holes in their hearts.\n",
            "24\n",
            "pred: ts the\n",
            "spoiler: bricking iPad Pros\n",
            "25\n",
            "pred: TrumpC Trump Trump Trump million million Trump million of a a a\n",
            "spoiler: Stace Nelson\n",
            "26\n",
            "pred: $ of.\n",
            "spoiler: Paige Hathaway – £3.8million Chantel Zales – £3.6 million Ana Cheri – £2.4 million Abigail Ratchford – £2.3 million Claudia Alende – £2.1 million\n",
            "27\n",
            "pred: Ct million million millioner million of of the\n",
            "spoiler: Buss and Jackson announced that they were mutually ending their four-year engagement\n",
            "28\n",
            "pred: Ler million million millioner million of a a\n",
            "spoiler: In the video, an abstract, wooden sculpture clock appeared not to be working, so a male tourist decided to take matters into his own hands, pulling on weights and levers for more than 30 seconds before the clock came flying off the wall and into pieces on the floor.\n",
            "29\n",
            "pred: KCttt million million millions of a\n",
            "spoiler: reduced fat sour cream\n",
            "30\n",
            "pred: KCin million million millions of of a\n",
            "spoiler: Brooks would eventually return to a role at News Corp, few expected her to land at Storyful\n",
            "31\n",
            "pred: Ktt million million million Trump million millions the a a a\n",
            "spoiler: Edward Gorey\n",
            "32\n",
            "pred: Tt million million million Trump million million of of a\n",
            "spoiler: Rag & Bone\n",
            "33\n",
            "pred: BBttt Trump Trump million million million the,\n",
            "spoiler: pixie cut\n",
            "34\n",
            "pred: K million million millions the a a a\n",
            "spoiler: homemade oven cleaner place the shelves in a resealable plastic bag, spray with oven cleaner, seal the bag, then leave to soak old toothbrush is an essential oven-cleaning tool glass scraper is ideal for removing tough stains remove greasy build-up on the hood of your oven with oil\n",
            "35\n",
            "pred: Kin million million million Trump million million to a a\n",
            "spoiler: In February, when Rep. David Jolly introduced his quixotic plan to ban members of Congress from soliciting campaign contributions, the Florida Republican had only six co-sponsors.\n",
            "36\n",
            "pred: BBBTttts the\n",
            "spoiler: southern flying squirrel\n",
            "37\n",
            "pred: Lt million million millions of a a\n",
            "spoiler: 1. \"You can't live your life according to maybes.\" 2. \"I wish there was a way to know you’re in the good old days before you’ve actually left them.\" 3. \"And what exactly do you think fairy tales are? They are a reminder that our lives will get better if we just hold on to hope. Your happy ending may not be what you expect, but that is what will make it so special.\" 4. \"To exist is to survive unfair choices.\" 5. \"I don’t want normal, and easy, and simple. I want painful, difficult, devastating, life-changing, extraordinary love. Don’t you want that, too?\"\n",
            "38\n",
            "pred: At million million million the of of a a\n",
            "spoiler: At this point, a large dog -- presumably Stella -- bounds into view, runs to one end of the fence and jumps right over it, almost without effort.\n",
            "39\n",
            "pred: Kiner million million million Trump million million the a of a a\n",
            "spoiler: Hope's antique cabinet\n",
            "40\n",
            "pred: Lin million million million of of the\n",
            "spoiler: 1. Some alt-right guy will invent the make-up gun. 2. There will be a referendum on whether or not to deport illegal immigrants. 3. Greedy, corrupt energy firms will cause an environmental catastrophe, and a dome will be built over the contaminated site. 4. The chandelier in Elton John’s private jet will malfunction. 5. It will be made illegal to teach evolution in schools.\n",
            "41\n",
            "pred: Tt million million million of a\n",
            "spoiler: Some people are just better at sports than others\n",
            "42\n",
            "pred: KC million million million Trump million million of of a a a\n",
            "spoiler: August 6th\n",
            "43\n",
            "pred: BBBttt million million millions the a\n",
            "spoiler: perfectly average\n",
            "44\n",
            "pred: Ats the,\n",
            "spoiler: \"but\"\n",
            "45\n",
            "pred: t million million the a\n",
            "spoiler: On Tuesday morning, NASA will broadcast its first-ever rocket launch livestream in 360-degree video\n",
            "46\n",
            "pred: Lt million million millioner million million the the a a\n",
            "spoiler: Vince Carter Paul Pierce Dirk Nowitzki\n",
            "47\n",
            "pred: BTttt million million millions of the a\n",
            "spoiler: Baking soda bee stings\n",
            "48\n",
            "pred: TrumpC Trump Trump Trump million million million Trump million to a to\n",
            "spoiler: The Arizona Republic\n",
            "49\n",
            "pred: Tt million million million of the\n",
            "spoiler: I don’t think we should be starting to panic\n",
            "50\n",
            "pred: KACCt million million millions of of of the a the the\n",
            "spoiler: apocalyptic omen\n",
            "51\n",
            "pred: Kin million million million the a a\n",
            "spoiler: Rudner and her husband rented ATVs. After hitting a speed bump and falling off of the vehicle, her husband shattered his shoulder and Rudner rotated her hip\n",
            "52\n",
            "pred: Ct million million millions of of a a\n",
            "spoiler: rapamycin\n",
            "53\n",
            "pred: Kin million million million of of a\n",
            "spoiler: Santa Clara University\n",
            "54\n",
            "pred: KCin million million million Trump million of to\n",
            "spoiler: Oklahoma Wesleyan University\n",
            "55\n",
            "pred: T million million million of the\n",
            "spoiler: The sadness some men feel at this point may be due to the contrast between the joy of arousal and feeling like a superhero and the sensation of the feel-good hormones wearing off.\n",
            "56\n",
            "pred: KTtt million million millions of a\n",
            "spoiler: Edan Lepucki\n",
            "57\n",
            "pred: Tt million million millioner million million the a a\n",
            "spoiler: Tim Masthay\n",
            "58\n",
            "pred: Lin million million million the a the\n",
            "spoiler: But psychedelics may not be as dangerous and addictive as our society thinks.\n",
            "59\n",
            "pred: KBBCtts of,\n",
            "spoiler: World's Largest Pumpkin\n",
            "60\n",
            "pred: KKtsss the a\n",
            "spoiler: $25K\n",
            "61\n",
            "pred: KTC million million millions of the\n",
            "spoiler: If you find yourself so moved to store ice cream in your back pocket in Alabama, you’ll pay the price, and we don’t just mean a cold rear end.\n",
            "62\n",
            "pred: $ the\n",
            "spoiler: close your eyes or simply stare at a fixed point Next, turn all focus to pulling in your breath Then, simply sigh Then start counting the breath in and out through the nose repeat this eight times\n",
            "63\n",
            "pred: KC million million million Trump million million of of of a a a\n",
            "spoiler: University of Tennessee\n",
            "64\n",
            "pred: KKCerer million million million Trump million million of a a a\n",
            "spoiler: Almario alleged physical abuse and uploading their sex video online\n",
            "65\n",
            "pred: TLt million million million Trump million million of the of of of a a\n",
            "spoiler: Crazy Frog\n",
            "66\n",
            "pred: C million million million Trump million million to of the a a\n",
            "spoiler: \"Although astrologers seek to explain the natural world, they don't usually attempt to critically evaluate whether those explanations are valid — and this is a key part of science.\"\n",
            "67\n",
            "pred: BBTCttts the\n",
            "spoiler: face\n",
            "68\n",
            "pred: KLmanmanerer million million millioner millioner Trump million million of of a to\n",
            "spoiler: Rachel Zoe Gives Birth To Her Second Baby Boy!\n",
            "69\n",
            "pred: Kin million millions the a a a\n",
            "spoiler: Amazon\n",
            "70\n",
            "pred: BCt million million millions of the a a\n",
            "spoiler: 1. Washing Your Face 2. Not Washing Your Feet 3. Not Washing or Replacing Your Loofah Regularly 4. Using a Soap Dish 5. Using Scented Soaps\n",
            "71\n",
            "pred: Kinin million million million of of a a\n",
            "spoiler: kuru\n",
            "72\n",
            "pred: BC million million millions of the the a the\n",
            "spoiler: Dieffenbachia\n",
            "73\n",
            "pred: BBtt million million millions the the\n",
            "spoiler: The answer to that is a definitive ... maybe.\n",
            "74\n",
            "pred: At million million million of of the\n",
            "spoiler: baby food\n",
            "75\n",
            "pred: C million million millions the a a\n",
            "spoiler: Michael and Kirk\n",
            "76\n",
            "pred: Tt million million millions the a a\n",
            "spoiler: 1,4-dioxane\n",
            "77\n",
            "pred: TACCCtin million million millions the\n",
            "spoiler: gut bacteria\n",
            "78\n",
            "pred: At million million million of of of the\n",
            "spoiler: from the last two weeks until the last breath, somewhere in that interval, people become too sick, or too drowsy, or too unconscious, to tell us what they’re experiencing Pre-death dreams were frequently so intense that the dream carried into wakefulness. First hunger and then thirst are lost. Speech is lost next, followed by vision. The last senses to go are usually hearing and touch. There are some kinds of conditions where pain is inevitable We generally believe that if your brain is really in a comatose kind of situation, or you’re not really responsive, that your perception—how you feel about things—may also be significantly decreased,\n",
            "79\n",
            "pred: KCCin million million millions of the\n",
            "spoiler: Wyoming\n",
            "80\n",
            "pred: L million million million of of the\n",
            "spoiler: Nvidia\n",
            "81\n",
            "pred: Kin million million million of of of the\n",
            "spoiler: it's all part of a massive effort to encourage people around the globe to donate blood\n",
            "82\n",
            "pred: LLer million million Trump million million millioner million Trump Trump million to a of a a\n",
            "spoiler: Republican voters would enthusiastically welcome a black candidate, a Donell Trump — so long as he, too, championed nationalist, politically incorrect, anti-immigrant populism.\n",
            "83\n",
            "pred: KKTKCCttt million millions of a\n",
            "spoiler: 2.1 coffee drinks\n",
            "84\n",
            "pred: KKBCTttts the\n",
            "spoiler: Yuliana Avalos Match.com’s parent company, IAC (InterActiveCorp)\n",
            "85\n",
            "pred: C million million million of of a a\n",
            "spoiler: you’d still have a hard time arguing they were even the worst band on this stage\n",
            "86\n",
            "pred: Lin million million million Trump million the a a\n",
            "spoiler: Female staffers adopted a meeting strategy they called \"amplification\": When a woman made a key point, other women would repeat it, giving credit to its author.\n",
            "87\n",
            "pred: C million million millions of of a\n",
            "spoiler: In these headphones, 30% of the weight comes from four tiny metal parts that are there for the sole purpose of adding weight\n",
            "88\n",
            "pred: Tt million million millions of the\n",
            "spoiler: giant piece of cardboard\n",
            "89\n",
            "pred: Ct million million millions of the\n",
            "spoiler: Solomon’s recent research shows that people who are thinking about death are more likely to say they support him\n",
            "90\n",
            "pred: Kin million million million of of a a\n",
            "spoiler: Mark and Jacoba Tromp and their adult children Ella, Riana and Mitchell left their home in Silvan, east of Melbourne, taking cash but leaving behind bank cards and mobile phones. Hundreds of kangaroos were found dead in far west New South Wales this year from what was described as a \"mystery disease\". When a magnitude 7.8 earthquake struck near Christchurch in November, videos emerged that appeared to show the New Zealand sky lighting up in blue and green. This year it was reported that doctors were at a loss to explain the mysterious illness making a four-year-old Bangladeshi boy look like an old man. The FBI announced this year that one of America's most baffling crimes — that of hijacker Dan \"DB Cooper\" who jumped out of a plane with a parachute and ransom money 45 years ago — looks set to remain an enigma.\n",
            "91\n",
            "pred: Tt million million million of of a\n",
            "spoiler: working on a cattle ranch as a utility farmer, picking up trash at the city dump in 100-plus degree weather telemarketing place\n",
            "92\n",
            "pred: K million million millions of a a\n",
            "spoiler: net neutrality\n",
            "93\n",
            "pred: KK million million millions of of the\n",
            "spoiler: sang his very own special rendition of Lukas Graham’s hit song \"7 Years.\"\n",
            "94\n",
            "pred: Lin million million million the of of a\n",
            "spoiler: Fall of the Resistance.\n",
            "95\n",
            "pred: As the a\n",
            "spoiler: 4.47 billion years ago\n",
            "96\n",
            "pred: BBin million million millions of the\n",
            "spoiler: Switzerland\n",
            "97\n",
            "pred: $ the\n",
            "spoiler: They are soaked in a bath of chlorine\n",
            "98\n",
            "pred: T million million millions of a a a\n",
            "spoiler: preorder figure was 2 million\n",
            "99\n",
            "pred: $ the\n",
            "spoiler: make-shift tap shoes\n",
            "100\n",
            "pred: Kin million million million Trump million of of a a a\n",
            "spoiler: Gov. Rick Snyder (R-Michigan)\n",
            "101\n",
            "pred: Kin million million million Trump million of of a\n",
            "spoiler: Because he believes he can monitor the situation as well — or better — from where he is\n",
            "102\n",
            "pred: KKKGttt Trump million million million Trump millioner million million of of of\n",
            "spoiler: Mr Thiel, who previously said New Zealand was a \"utopia\" and has invested heavily there, is just one of several US migrants who have realised what the country has to offer.\n",
            "103\n",
            "pred: Ltmanmanman million millioner million million million the a\n",
            "spoiler: two foundations\n",
            "104\n",
            "pred: K million million million the of a a\n",
            "spoiler: Boxed\n",
            "105\n",
            "pred: Cer Trump million million million Trump million of a\n",
            "spoiler: key objectives of MILO’s tour was the promotion of free speech on typically censorious American college campuses\n",
            "106\n",
            "pred: C million million millioner million million of of of to\n",
            "spoiler: toxic metal arsenic\n",
            "107\n",
            "pred: A million million of the a a\n",
            "spoiler: $900 million\n",
            "108\n",
            "pred: KKCtt million million millions the a\n",
            "spoiler: 1. Mylan has patent protection that lasts through 2025 2. There’s no room for error when you’re treating anaphylaxis 3. It doesn’t take an auto-injector to get epinephrine into the body — but it sure helps 4. The regulatory process is slow and expensive 5. The public hasn’t spoken (loud enough)\n",
            "109\n",
            "pred: t million million million of of a\n",
            "spoiler: he would keep the nation \"in suspense\" about whether he will accept the results\n",
            "110\n",
            "pred: LLererer million million millioner millionerers to a\n",
            "spoiler: \"I like to do a cleanse once a year. But it's not just juices. And I don't do it for weight loss,\" \"I'll probably get kicked out of our school for admitting this, but I let Apple stay home yesterday. I just needed to be with her. We went out to lunch, we went to the beauty salon, we were together,\" \"If I make my kids something delicious and we sit down to eat it, and I put my phone away and I really listen, that is such money in the bank,\" \"I was very strict for a while. I was macrobiotic for a couple of years, then I got pregnant and just ate ice cream. What I've learned is I want to enjoy my life, and food is a big part of it. I love to cook and feed people. I cook every day,\" \"Anything that can live on a shelf for years and you can open out of a bag and eat: no,\"\n",
            "111\n",
            "pred: KCin million million millions of the\n",
            "spoiler: 1. You buy hemp milk not because it’s ironic, but because your digestion isn't what it used to be. 2. You no longer think spending a Friday night at home is lame 3. But binge-watching isn't limited to just sitcoms and dramas. 4. When your phone rings past 9 p.m. you automatically think it’s an emergency, because really, who calls someone past 9? 5. Your Halloween costumes have become significantly less revealing over the years\n",
            "112\n",
            "pred: KKPCCKCCCttt million million millions of the\n",
            "spoiler: All you need to do is applying pressure on your sinuses between your eyes and nose and on the back of your head for 60 seconds.\n",
            "113\n",
            "pred: KBBBCtt million million millions of a\n",
            "spoiler: 15 Kristen Stewart 14 Aubrey Plaza 13 Mara Wilson 12 Colton Haynes 11 Reid Ewing\n",
            "114\n",
            "pred: Lin million million millioner million million to of a a\n",
            "spoiler: his sixth studio album, \"X,\" will hit stores on May 5\n",
            "115\n",
            "pred: A million millions the a\n",
            "spoiler: wants to achieve her own success\n",
            "116\n",
            "pred: KBKKKBBKBKttts of,\n",
            "spoiler: Chantelle major plastic surgery to repair her injuries\n",
            "117\n",
            "pred: LLererer million million millioner million Trump million the a a\n",
            "spoiler: Savannah Guthrie got married four months pregnant\n",
            "118\n",
            "pred: Tts the a\n",
            "spoiler: Jason Brian Rosenthal \n",
            "119\n",
            "pred: L million million million the of the the the\n",
            "spoiler: EasyAuto123.com\n",
            "120\n",
            "pred: Lt million million millioner million million the a a\n",
            "spoiler: On September 10, flash sale site Joss & Main will be selling actual props from the sets\n",
            "121\n",
            "pred: TrumpKTATTtt million million millions of the\n",
            "spoiler: Guyana\n",
            "122\n",
            "pred: Kt million million million Trump million millions a a a\n",
            "spoiler: Gilbert Arenas\n",
            "123\n",
            "pred: KLererer million million million Trump million millions to of a a a\n",
            "spoiler: Joe McGrath For Nathan’s birthday\n",
            "124\n",
            "pred: KKt million million million Trump million million of of a a a\n",
            "spoiler: 1. \"Never-Before-Seen Photos Behind the Scenes at the Women’s March\" 2. \"Honest Photos of Motherhood Challenge What We Think of as Natural\" 3. \"This Is What International Women’s Day Looked Like Around The World\" 4. \"A Crude Awakening\" 5. \"8 Portraits of Refugees That Will Help You Understand the Refugee Experience\"\n",
            "125\n",
            "pred: KKKKBKKL million million millions of the\n",
            "spoiler: parent who was an immigrant or an educator parents were involved in political activism of some kind conflict-heavy family life, but that conflict was rarely between the parents strong awareness of mortality as children they grew up with much more freedom than their friends\n",
            "126\n",
            "pred: C Trump million million million of to\n",
            "spoiler: Alex Castellanos\n",
            "127\n",
            "pred: KKBCtt million million millions of a\n",
            "spoiler: bring the work to them\n",
            "128\n",
            "pred: KKAAKBKtt million million millions of the\n",
            "spoiler: heroin local news reports in Denver, Chicago, Atlanta, Seattle, Orange County, Syracuse, and a growing number of other areas told the tales of teens dying from heroin\n",
            "129\n",
            "pred: t million million of the\n",
            "spoiler: \"Gravity\"\n",
            "130\n",
            "pred: Ktt million million millions of a\n",
            "spoiler: Naturally Remove Yucky Corns And Calluses\n",
            "131\n",
            "pred: C million million million Trump million million to of a a a\n",
            "spoiler: For some drivers, their app-inspired shortcut became a permanent route.\n",
            "132\n",
            "pred: KTt million million millions of a\n",
            "spoiler: Wisconsin\n",
            "133\n",
            "pred: BBttss of a\n",
            "spoiler: pieces of rubber material embedded in the meat\n",
            "134\n",
            "pred: VirginiaKKKGKKLttts of the\n",
            "spoiler: full moon on the same day as the summer solstice\n",
            "135\n",
            "pred: KKCman million million million Trump million million of of a a\n",
            "spoiler: Isla Fisher\n",
            "136\n",
            "pred: K million million millions of of of to to a a\n",
            "spoiler: the greatest potential health drawback of consuming bottled sparkling water is missing out on the benefit of fluoride when you drink it instead of fluoridated tap water\n",
            "137\n",
            "pred: KLererer million million millioner Trump million millions a a\n",
            "spoiler: never received a dime\n",
            "138\n",
            "pred: As the a\n",
            "spoiler: 170\n",
            "139\n",
            "pred: Kin million million millioner million million to a a\n",
            "spoiler: Her ticket out of debt and into financial freedom has been her blog, Making Sense of Cents, where she offers tips on saving and making money   and publishes income reports.\n",
            "140\n",
            "pred: VirginiaKKKACCTttt million million millions the,\n",
            "spoiler: taking a banned or illegal substance in order to improve performance\n",
            "141\n",
            "pred: GLtt million million millions the a\n",
            "spoiler: move all the perishable older goods to the front and place the newer groceries behind them.\n",
            "142\n",
            "pred: KKin million million millions the the\n",
            "spoiler: Atlanta\n",
            "143\n",
            "pred: BBBCtt million million millions of the\n",
            "spoiler: The woman strongly believed that PM Narendra Modi is responsible for poverty and other terrible conditions in our country. Whereas the husband felt otherwise. He strongly believed that NaMo is doing his best to fix everything, gradually.\n",
            "144\n",
            "pred: Liner million million millioner million of the a\n",
            "spoiler: Dahlesque Golden ticket Human bean Oompa Loompa Scrumdiddlyumptious\n",
            "145\n",
            "pred: Kin million million million Trump million of of of the\n",
            "spoiler: arrested in a 2012 marijuana trafficking bust\n",
            "146\n",
            "pred: KKKKBKBBtt million million millions of of of the\n",
            "spoiler: Peloponnese\n",
            "147\n",
            "pred: KCt million million millions of of of a a\n",
            "spoiler: If you've booked a holiday, but are yet to exchange your pounds for a foreign currency, you're going to be a victim of Britain leaving the EU I'm afraid. And your flights look like they're going to go up in price, too.\n",
            "148\n",
            "pred: KKKLinin million million millions of the\n",
            "spoiler: Israel\n",
            "149\n",
            "pred: TCin Trump Trump million million million of of a a\n",
            "spoiler: Chiwetel Ejiofor\n",
            "150\n",
            "pred: KKKCttt million million millions of the\n",
            "spoiler: Lviv, Ukraine\n",
            "151\n",
            "pred: Kin million million million Trump million of of a a a\n",
            "spoiler: is still wearing her updo braids TWO DAYS after debuting the glamorous look at the Met Gala\n",
            "152\n",
            "pred: KBBCt million million millions the\n",
            "spoiler: Every rupee spent in hotels and restaurants, shops, taxis or temples, is helping Kathmandu back on to its feet\n",
            "153\n",
            "pred: KKmaner million million million Trump million of a a\n",
            "spoiler: \"The Wolf of Wall Street\" features a lot of swear words. According to Vulture, there are 569 variations on the f-word alone\n",
            "154\n",
            "pred: KBBCCttts the\n",
            "spoiler: she joins in\n",
            "155\n",
            "pred: KTttt million millions of a\n",
            "spoiler: Mobile web (m.reddit.com) is undergoing a major overhaul new frontpage algorithm we tested adding affiliate tags to e-commerce links, which we ended up turning off we announced Promoted User Posts sponsored headlines\n",
            "156\n",
            "pred: KKC million million millions a a a\n",
            "spoiler: Guinness World Record: the most peanut butter and jelly sandwiches made in an hour 49100\n",
            "157\n",
            "pred: KBCin million million millions of the\n",
            "spoiler: La Junta\n",
            "158\n",
            "pred: ts the a\n",
            "spoiler: June 12 at 5 PM ET\n",
            "159\n",
            "pred: Kin million million million Trump million million the of a a\n",
            "spoiler: Daisy Ridley endometriosis\n",
            "160\n",
            "pred: KLiner million million millions a a\n",
            "spoiler: on \"Do What U Want\"\n",
            "161\n",
            "pred: KBACttt million millions of of the\n",
            "spoiler: 1. Starbucks prevents you from losing it on your kids. 2. You definitely have no energy to clean a mess without it. 3. And the more you have, the happier you get. 4. If you don’t have caffeine, you look like your kids do every morning.\n",
            "162\n",
            "pred: t million million million of of a\n",
            "spoiler: vote Clinton.\n",
            "163\n",
            "pred: Kt million million millions the a a\n",
            "spoiler: $1,000 per haircut\n",
            "164\n",
            "pred: T million million million of a a\n",
            "spoiler: If we're being honest, this probably won't be the most exciting iPhone release we've seen\n",
            "165\n",
            "pred: BLLterermanerer million million millioner millioner Trump million million the a a\n",
            "spoiler: Wiz Khalifa\n",
            "166\n",
            "pred: Lterer millioner million million millions a\n",
            "spoiler: Long And Healthy\n",
            "167\n",
            "pred: KKCCter million million million of and\n",
            "spoiler: The school created two \"all-gender,\" single-stall restrooms in February. Students can still choose what works best for them.\n",
            "168\n",
            "pred: KKKCt million million millions a a a\n",
            "spoiler: fastest video to break 1 billion views\n",
            "169\n",
            "pred: LLererer million million millioner millions a a\n",
            "spoiler: Fashion Advice\n",
            "170\n",
            "pred: A million million million of of a a a\n",
            "spoiler: The Fed is expected to hike rates The Netherlands is going to the polls Oil is making a comeback An early Twitter investor hates the stock\n",
            "171\n",
            "pred: Trump Trump million million million Trump million of the the\n",
            "spoiler: The Overton Window\n",
            "172\n",
            "pred: C million million millions of the\n",
            "spoiler: Massachusetts\n",
            "173\n",
            "pred: C million million million the a a\n",
            "spoiler: VLC\n",
            "174\n",
            "pred: Kin Trump Trump Trump million million million of to\n",
            "spoiler: Shevchenko’s coach, Pavel Fedotov, was armed with a gun and decided to take matters into his own hands. Firing at the group of men, Fedotov allegedly killed one of the men. He was then shot himself in the stomach\n",
            "175\n",
            "pred: CC million million million Trump million million of of of a a a\n",
            "spoiler: Liechtenstein\n",
            "176\n",
            "pred: Ltt million million millions of the\n",
            "spoiler: concealer and foundation, should be replaced every six months eyeliner and mascara, should be replaced every three months shampoo and conditioner are generally good for a year\n",
            "177\n",
            "pred: Ktss of of a\n",
            "spoiler: marriage proposals vulgar gags about marriage being asked for money\n",
            "178\n",
            "pred: KKKKTKCKCCCinininererer million million millioner millioner Trump million million Trump millionerer Trumper million Trump Trump million\n",
            "spoiler: Sultan Moulay Ismaïl of Morocco have sex an average of 0.83 to 1.43 times per day in order to father 1,171 children in 32 years\n",
            "179\n",
            "pred: KBCCCBCinininerinin million millions a\n",
            "spoiler: restaurant’s manager and co-owner\n",
            "180\n",
            "pred: Lt million million million the of\n",
            "spoiler: sweaters\n",
            "181\n",
            "pred: in million million million to the the\n",
            "spoiler: 1. Dr. Lakisha Jenkins, Master Herbalist 2. Rachel K. Gillette, Attorney 3. Lynne Lyman, California State Director for the Drug Policy Alliance 4. Charlo Greene, Founder of Alaska Cannabis Club 5. Jessica Peters, President of Moxie Meds\n",
            "182\n",
            "pred: C million million millions of a a\n",
            "spoiler: the reactor is a region of natural uranium within the Earth's crust, found in Okla, Gabon. Uranium is naturally radioactive, and the conditions in this rocky area happened to be just right to cook up some nuclear reactions.\n",
            "183\n",
            "pred: t million million million to the\n",
            "spoiler: These services offer a way for us to build monuments to ourselves. They let us combine our curated memories and experiences with all our messy data — the texts and tweets and \"Likes\" and photos — and organize it into something that we hope will let our loved ones and future generations actually understand something about who we were.\n",
            "184\n",
            "pred: TrumpCCCin million million million Trump million million of of of\n",
            "spoiler: Black Women\n",
            "185\n",
            "pred: GTCCCtts of a\n",
            "spoiler: Marriott Rewards has offered to sponsor Johnson to go to Europe, Japan and Australia to catch the regionally based Pokémon and finish his collection\n",
            "186\n",
            "pred: KKKBBKKTin millions a\n",
            "spoiler: Abraj Kudai Mecca, Saudi Arabia\n",
            "187\n",
            "pred: t million million million to the the\n",
            "spoiler: Not only is the weed bad, but Sisley claims it was moldy as well\n",
            "188\n",
            "pred: Kin million million million Trump million of a a a\n",
            "spoiler: Holt is a registered Republican Holt is the first black presidential debate moderator since 1992 He's not really into the Twitter He moderated a Democratic primary debate Holt has two honorary doctorates, but no bachelor's degree\n",
            "189\n",
            "pred: $ to,\n",
            "spoiler: \"OMG same! I keep my closet perfectly color-coded!\"\n",
            "190\n",
            "pred: At million millions the of of a a\n",
            "spoiler: inFlow Carta Rightcontrol Lite Odoo ABC Inventory\n",
            "191\n",
            "pred: L million million million of of of a a\n",
            "spoiler: Tomb Raider\n",
            "192\n",
            "pred: $ the\n",
            "spoiler: swipe left on Tinder and choose the old-fashioned way\n",
            "193\n",
            "pred: Kt Trump million million million Trump million of a\n",
            "spoiler: Take Walks on Beaches\n",
            "194\n",
            "pred: KBCt million million millions of the\n",
            "spoiler: ma'am\n",
            "195\n",
            "pred: LLererer million million millioner millioner Trump million to a the\n",
            "spoiler: \"I think she's fabulous,\"\n",
            "196\n",
            "pred: BCts million millions of a\n",
            "spoiler: its next Mars rover, dubbed Mars 2020, has cleared a massive hurdle that could see it speeding toward the Red Planet in just a few years.\n",
            "197\n",
            "pred: At million millions the a\n",
            "spoiler: largely due to three interrelated factors: European style trends, a government-mandated push for fuel economy, and new technologies that allowed manufacturers to more easily design and create curved shapes.\n",
            "198\n",
            "pred: Kin million million millions of of the a a\n",
            "spoiler: Avoid becoming a busy fool: It is very easy to become very busy doing things that are a huge drain on time but have little or no end value Invest wisely: Don’t invest in anything unless you are prepared to put at least 10 per cent of your net worth into it Save, save, save Know the value of your time\n",
            "199\n",
            "pred: Tt million million million the of a\n",
            "spoiler: pretending that they are men\n",
            "200\n",
            "pred: VirginiaATTttts of a\n",
            "spoiler: 1. Cleopatra lived closer in time to the invention of Snapchat than to the construction of the Great Pyramid at Giza. 2. Also, the Great Pyramid was older to the Romans than the Romans are to us. 3. Oh, and the Stegosaurus was older to the Tyrannosaurus rex than the T. rex is to us. 4. The oldest living person was born closer to the signing of the US Constitution than to today. 5. The 10th president of the United States has two grandchildren who are alive today.\n",
            "201\n",
            "pred: Tt million million millioner million million the the\n",
            "spoiler: altruism\n",
            "202\n",
            "pred: VirginiaKTCCCtt million million million Trump million millions a\n",
            "spoiler: preserving ecosystems to be a just cause, then killing could be justified\n",
            "203\n",
            "pred: KCt million million millions the the the\n",
            "spoiler: misses 'hooking up with strangers' treats her 'like a sex toy' she's 'in love with another man' his wife is simply 'taking advantage of her' spouse 'no longer shows any love'\n",
            "204\n",
            "pred: KLin million million million the of the a a\n",
            "spoiler: dying his hair sky blue\n",
            "205\n",
            "pred: TTt million million million of of a a\n",
            "spoiler: And all of this has got Vincent Kompany very excited. VERY excited. The Manchester City defender said that the standard of his performance was so good it puts him up there with Ronaldo and Messi.\n",
            "206\n",
            "pred: t million million million to the a a\n",
            "spoiler: MasterCard’s new Decision Intelligence software pulls in data, sometimes hundreds of pieces of data, about a specific transaction at the moment a customer swipes his or her credit card. The system then combines all of that information to yield a score indicating how likely the transaction is to be fraudulent.\n",
            "207\n",
            "pred: Cts the a\n",
            "spoiler: when we sleep, our blinking stops\n",
            "208\n",
            "pred: KBBBCttts the\n",
            "spoiler: embracing her new look and wants to use her new platform to help others\n",
            "209\n",
            "pred: t million million million of the\n",
            "spoiler: 1. Cold-Weather Clothing 2. Back-to-School Supplies 3. Theme Park Tickets 4. Patio Furniture 5. Televisions\n",
            "210\n",
            "pred: 1The..\n",
            "spoiler: men are so much more likely than women to work those long hours\n",
            "211\n",
            "pred: Tt million million millions of a\n",
            "spoiler: underarm deodorant\n",
            "212\n",
            "pred: VirginiaTATTttts the\n",
            "spoiler: Toronto resident, Hayley Greenberg Jingee's previous owners had taught him how to use the toilet instead of a litter box\n",
            "213\n",
            "pred: Ler million million Trump million million of of to\n",
            "spoiler: Hispanic voters\n",
            "214\n",
            "pred: Btt million million millioner million million of of a a\n",
            "spoiler: acted in self-defense and buried and burned the bodies in a state of panic\n",
            "215\n",
            "pred: ts the a\n",
            "spoiler: The photograph in question was taken on US President Donald Trump's first day in the Oval Office on Monday, when he signed a ban on federal money going to international groups that perform or provide information on abortions\n",
            "216\n",
            "pred: BTTtts of the\n",
            "spoiler: You can get infections in your fingers from the bacteria in your saliva, not to mention the wounds around your nails. In extreme situations, you can even get irreversible nerve damage.\n",
            "217\n",
            "pred: Cin million million millions of of to\n",
            "spoiler: botulinum toxin, \"type H\"\n",
            "218\n",
            "pred: LLerer Trump million million millioner million million the a a\n",
            "spoiler: \"When you meet someone they don't have a number on their forehead.\"\n",
            "219\n",
            "pred: L million million million the of the a a a\n",
            "spoiler: She says that summer is better because the heat makes it feel that much more steamy, summer bodies are great, and you can get fully nude without having to keep any layers on because of the cold.\n",
            "220\n",
            "pred: C Trump million million million Trump million of a a\n",
            "spoiler: Arthur Brooks\n",
            "221\n",
            "pred: KKBTTttt million millions the,\n",
            "spoiler: 1. Get better at a skill that you can share with others. 2. If you don’t want to invest a lot of time in coordinating contrasting colors in your home or closet, just stick with one. 3. Take care of your skin because if it glows, people will pay attention to you. 4. Find something you love about yourself and show it off like every atom in the universe needs to see it. 5. If you’re having people over, stock up on coffee table books that actually interest you.\n",
            "222\n",
            "pred: A million million of a a\n",
            "spoiler: Now Goel is forming a business to bring the chatbot to the wider world of education.\n",
            "223\n",
            "pred: Lerer million million millioner million Trump million the the\n",
            "spoiler: in a flash, the ring was gone, falling into the cold waters below\n",
            "224\n",
            "pred: Tin million million millions of a\n",
            "spoiler: Antenna Bands\n",
            "225\n",
            "pred: BTBTTttt milliont millions of a\n",
            "spoiler: Martin says this is perfect for modeling agencies, high-profile clients who don't want to give away personal information as they book appointments, or people who find excuses not to do things they don't want to do (the rest of the world).\n",
            "226\n",
            "pred: Ct million million million of of of a\n",
            "spoiler: 1. Jessica 2. Amy 3. Jade 4. Rebecca 5. Charlotte\n",
            "227\n",
            "pred: Kt Trump million million million Trump million the a a a\n",
            "spoiler: grey tee and blue jeans\n",
            "228\n",
            "pred: Lss the a\n",
            "spoiler: Kit Harington admitted during filming he may have got a bit too in character accidentally hitting Iwan not once but twice while miming his punches\n",
            "229\n",
            "pred: A million the,\n",
            "spoiler: £12.99\n",
            "230\n",
            "pred: KKKKCKCinin million million millioner million million Trump million million the a of\n",
            "spoiler: Rob Mrowka, a scientist with the Center for Biological diversity, said the Bundy cattle are essentially \"feral,\" roaming at will over hundreds of thousands of acres — including far outside the range where long ago they were permitted. (Bundy stopped paying his grazing fees in 1993, meaning the cattle were technically trespassing anywhere on public land after that point.)\n",
            "231\n",
            "pred: Cin million million million of a a\n",
            "spoiler: US stocks are at all-time highs. The ECB meets on Thursday. China's economy is bouncing back. Japan's Final Q3 GDP was revised sharply lower. Banks are moving to Paris.\n",
            "232\n",
            "pred: KLmaner million million million Trump million million the of of a a a\n",
            "spoiler: Andy Mizrahi being handsome, charismatic and funny\n",
            "233\n",
            "pred: L million million millions and a a\n",
            "spoiler: Ripcord\n",
            "234\n",
            "pred: At million million million of of a a\n",
            "spoiler: Guangyuan, Shanghai\n",
            "235\n",
            "pred: A million million million of of of to of to\n",
            "spoiler: Resistance to antibiotics is growing at such an alarming rate that they risk losing effectiveness entirely\n",
            "236\n",
            "pred: Cer million million million Trump million of a a a\n",
            "spoiler: pancakes\n",
            "237\n",
            "pred: Ktss the a\n",
            "spoiler: $1,230\n",
            "238\n",
            "pred: BBttss the,\n",
            "spoiler: Salman\n",
            "239\n",
            "pred: L million million million Trump million million of of a a\n",
            "spoiler: Senator Daniel Inouye of Hawaii, a World War II veteran, repeatedly introduced resolutions to return Memorial Day to its traditional date—and, by proxy, its original intent\n",
            "240\n",
            "pred: C million million millions of of a\n",
            "spoiler: Men are paid more partly because they’re much more likely to ask for it. makes offers based on what a job is worth Make Work Easier for Mothers Build More Flexible Workplaces Change the Law\n",
            "241\n",
            "pred: L million million millioner million million to the to a\n",
            "spoiler: It’s clear there is no one-size-fits-all approach for publishers, even if they are in the same category — optimal times vary depending on the location of the audience. One explanation for this could be that daily routines and behaviors vary between countries and cultures, which therefore impacts media consumption patterns.\n",
            "242\n",
            "pred: Ttt million million millions of the\n",
            "spoiler: Pantone 448 C\n",
            "243\n",
            "pred: Tt million million million Trump million million of of a a\n",
            "spoiler: Ellis Rabb\n",
            "244\n",
            "pred: Kin million million million Trump million million of of a a a\n",
            "spoiler: For that we could compare deaths per film to standardize among the actors. If we do that, Sean Bean is still at the top of the list with 0.32 deaths/film, tied with Bela Lugosi\n",
            "245\n",
            "pred: At million million million the of the a a\n",
            "spoiler: Ivory & Deene\n",
            "246\n",
            "pred: KG million million millioner million million the the a the\n",
            "spoiler: Day-trippers or tourists at less than Insta-worthy lodgings can head to nearby luxury hotels and indulge in their amenities while saving on the eyebrow-raising nightly rates\n",
            "247\n",
            "pred: At million million million the a a\n",
            "spoiler: Derrick Taylor he was presented the car\n",
            "248\n",
            "pred: L million million million to the a a a the\n",
            "spoiler: ballet studio\n",
            "249\n",
            "pred: A million million million to the a the\n",
            "spoiler: 1. Emphasize productivity. 2. Evaluate potential ROI. 3. Investigate key features.\n",
            "250\n",
            "pred: KCt million million millions the a a\n",
            "spoiler: one house has been encased in ice\n",
            "251\n",
            "pred: Lin million million millions of of the\n",
            "spoiler: Sarah Brown did not expect to get pregnant the year before\n",
            "252\n",
            "pred: At million million the a the\n",
            "spoiler: One user noted the similarity to a famous drawing that appears to be of both a duck and a rabbit\n",
            "253\n",
            "pred: Kiner million million million Trump million million the a a\n",
            "spoiler: Ryan Tannehill\n",
            "254\n",
            "pred: Ler million million Trump million million million of of a a a\n",
            "spoiler: \"Yeah, I mean, I’d do it,\"\n",
            "255\n",
            "pred: A million million million of the a a a\n",
            "spoiler: the scientific miracle that allowed Glenn to become the first American to orbit the earth\n",
            "256\n",
            "pred: KKC million million millions the a a\n",
            "spoiler: $564\n",
            "257\n",
            "pred: Kt million million million the of a a the\n",
            "spoiler: living a life of goodness and peace and joy in the Holy Spirit\n",
            "258\n",
            "pred: Ttt million million millions of the\n",
            "spoiler: realistic optimists\n",
            "259\n",
            "pred: Kiner million million millioner million of a a\n",
            "spoiler: Rep. Kevin Elmer\n",
            "260\n",
            "pred: KLin million million million the of of a\n",
            "spoiler: Dolores Umbridge was arrested, interrogated and imprisoned for crimes against Muggleborns. After his death, Voldemort is forced to exist in the stunted infant-like form that Harry sees in the Kings Cross-like Limbo. The Resurrection Stone is now buried in the Forbidden Forest after being pressed into the ground by a centaurs hoof. It has not been re-discovered. Harry lost the ability to speak Parseltongue when Voldemorts soul fragment was destroyed. Viktor Krum fell in love with a woman back home in Bulgaria and presumably lived happily ever after.\n",
            "261\n",
            "pred: ts the a\n",
            "spoiler: Hydraulic Presses Drilling Hammers Angle Grinders Russian Men with Chairs\n",
            "262\n",
            "pred: Tt million million millions of a\n",
            "spoiler: spray-on nail polishes\n",
            "263\n",
            "pred: C million million million of of of a the\n",
            "spoiler: The big pause The eye dart The lost breath Overcompensating The poker face\n",
            "264\n",
            "pred: CC Trump million million million Trump millions of the\n",
            "spoiler: 6) Mike Tyson 5) Loretta Lynn 4) Joe Arpaio 3) Wayne Newton AKA Mr Las Vegas 2) Phil Ruffin 1 + Bonus) Sarah Palin\n",
            "265\n",
            "pred: BCCttt million million Trump million million million of of of the\n",
            "spoiler: direct result of the city using its own river as a water source\n",
            "266\n",
            "pred: Cin Trump million million million Trump million of of a a\n",
            "spoiler: his passion\n",
            "267\n",
            "pred: KBLinin Trump Trumps the a\n",
            "spoiler: 1. SRK Teases Fans With A Glimpse Of Raees' Song 'O Zalima', Recites Its Lyrics In His Own Voice 2. Swami Om Threatens The Makers Of Violence & Shutting Down Bigg Boss If They Don't Make Him Win! 3. Aamir Khan Aims To Make Maharashtra Drought Free In The Next Five Years 4. Farhan Akhtar Gives The Perfect Response To Abu Azmi's Remarks On Bengaluru Mass Molestation 5. Salim Khan-Aamir Khan Condemn The Shameful Bengaluru Incident, Call For Strengthening Of Laws\n",
            "268\n",
            "pred: Gt million million million of the a a a\n",
            "spoiler: 9 a.m.\n",
            "269\n",
            "pred: A million million million of of a a\n",
            "spoiler: Investigators say the malware the thieves used shared similarities to the code used by a hacking group known as Lazarus.\n",
            "270\n",
            "pred: KKCttt million million Trump million million million of of a a\n",
            "spoiler: The National Vietnam Veterans Foundation less than 2 percent for actual veterans and veterans' charitable causes\n",
            "271\n",
            "pred: KKK Trump Trump million Trump million million millions of a a\n",
            "spoiler: Sgt. Jose Luis Sanchez\n",
            "272\n",
            "pred: Kerer millioner million million million Trump million to a\n",
            "spoiler: for driving with a suspended license and without a vehicle tag and insurance\n",
            "273\n",
            "pred: KLininer million million millioner millions the a\n",
            "spoiler: Kamya Punjabi is all set to release her short film 'Hum Kuchh Kah Na Sakey'\n",
            "274\n",
            "pred: Kiner million million millioner million of to a\n",
            "spoiler: reduced the risk of flu-related hospitalization\n",
            "275\n",
            "pred: KCt million million millions of a a\n",
            "spoiler: \"If I had no discipline in sex before marriage, I will have no discipline in sex after marriage,\"\n",
            "276\n",
            "pred: Kt million millions the a\n",
            "spoiler: link between workaholism and reduced physical and mental well-being\n",
            "277\n",
            "pred: Lss the a\n",
            "spoiler: Sombra\n",
            "278\n",
            "pred: Kt million million millions of a\n",
            "spoiler: 1 North Dakota\n",
            "279\n",
            "pred: KL Trump Trump million million million Trump million of of a a a\n",
            "spoiler: third \"Riddick\" movie\n",
            "280\n",
            "pred: As the a\n",
            "spoiler: NASA tracks more than 500,000 pieces of defunct satellites and old rocket bits orbiting Earth, some no larger than marbles, so that the space station can be maneuvered away from close encounters — but many small pieces, like the one that hit the space station, are too difficult to keep track of\n",
            "281\n",
            "pred: Kin million million million Trump million of of a a\n",
            "spoiler: zone-read specialist\n",
            "282\n",
            "pred: Lt million million millioner million million the of of a a\n",
            "spoiler: one million dollars\n",
            "283\n",
            "pred: KKKCtererer million million millions of a\n",
            "spoiler: to cut down on his perceived duty to wave to other motorcyclists\n",
            "284\n",
            "pred: T million million million Trump million million of of of a\n",
            "spoiler: Target’s stock had been dented since the American Family Association’s boycott began amassing signatures in the millions, but we obviously had no way of knowing if people were actually boycotting the store.\n",
            "285\n",
            "pred: KKKCtt Trump million million millions a\n",
            "spoiler: Markwayne Mullin welcoming two twin girls into his home\n",
            "286\n",
            "pred: A million million million of of a a a\n",
            "spoiler: Braconid\n",
            "287\n",
            "pred: KLter Trump million million million Trump millions a a a\n",
            "spoiler: $420,000\n",
            "288\n",
            "pred: T million million million Trump million of of a a\n",
            "spoiler: begin laying off a large part of his staff starting Wednesday\n",
            "289\n",
            "pred: Lt million million millions the a a\n",
            "spoiler: Vintage Home Finds\n",
            "290\n",
            "pred: KKKAKKCCtt million milliont the,\n",
            "spoiler: 5. Al Jubail, Saudi Arabia 4. Riyadh, Saudi Arabia 3. Allahabad, India 2. Gwalior, India 1. Zabol, Iran\n",
            "291\n",
            "pred: Kt million million millioner million million the of of\n",
            "spoiler: retro Coca Cola advertisement\n",
            "292\n",
            "pred: L million million million of of of the\n",
            "spoiler: Toilet paper, unlike toilet seats, are not designed to ward off bacteria. In fact, they’re sponges for it.\n",
            "293\n",
            "pred: At million million million the of of a a\n",
            "spoiler: Lions Die First\n",
            "294\n",
            "pred: ts the a\n",
            "spoiler: as a sex toy\n",
            "295\n",
            "pred: Kt million million million Trump million million of of of\n",
            "spoiler: The 'Sledgehammer' visual is premiering in IMAX, making it the first music video ever to do that.\n",
            "296\n",
            "pred: t million million million of the\n",
            "spoiler: a national catastrophe. a recession. A terrorist attack a divided Democratic Party. \n",
            "297\n",
            "pred: BTCCtt million million millioner million millions of of of the\n",
            "spoiler: second Sumatran rhino\n",
            "298\n",
            "pred: TrumpC million Trump million million million of of a a\n",
            "spoiler: Trump has a theory. He thinks that Hispanics know that he will create jobs and that many of these jobs will go to Hispanics because his years in business have taught him that Hispanics are especially hard workers.\n",
            "299\n",
            "pred: K million million million Trump million million the of a a a\n",
            "spoiler: broadened eligibility in feature writing and investigative reporting to include online and print magazines\n",
            "300\n",
            "pred: ts the a\n",
            "spoiler: Ted Cruz\n",
            "301\n",
            "pred: Ain million million million of of of the\n",
            "spoiler: asteroid\n",
            "302\n",
            "pred: KBtts of,\n",
            "spoiler: The mom, Dorothy, plays the role of the photographer, junk food supplier and a super fan.\n",
            "303\n",
            "pred: Tin million million millions of a a\n",
            "spoiler: 2010 study by scientists from Copenhagen University Hospital reduce a man’s sperm count by as much as thirty percent\n",
            "304\n",
            "pred: Lererer million million millioner millions a a\n",
            "spoiler: pizza, chicken wings, and a Mountain Dew\n",
            "305\n",
            "pred: C million million million of of of a\n",
            "spoiler: An inward turn for the U.S.? Angry voters China - opportunity or risk? Russian disruption\n",
            "306\n",
            "pred: TLter millioner million million million Trump million to a the\n",
            "spoiler: original Tweets\n",
            "307\n",
            "pred: CCCt million millions of the\n",
            "spoiler: A recent study proves that cheating is not a one-time thing, and that person will probably cheat on you again\n",
            "308\n",
            "pred: KL million million millioner million million of of of a a a\n",
            "spoiler: September 16\n",
            "309\n",
            "pred: Ler million million million of of of a a\n",
            "spoiler: Petit\n",
            "310\n",
            "pred: BBttt million million millions of the\n",
            "spoiler: ottoman\n",
            "311\n",
            "pred: C million million millioner million of of a a\n",
            "spoiler: \"It's both sides playing equally bad football. We are not capable of beating teams if we play like that\"\n",
            "312\n",
            "pred: KC million million millions of a a a\n",
            "spoiler: strict diet, as well as a strict workout regime\n",
            "313\n",
            "pred: KKt million millions the the\n",
            "spoiler: He converted it into a boat's cabin.\n",
            "314\n",
            "pred: KATtt million million million of the a a\n",
            "spoiler: 1. You cannot go out tonight because you are practicing for the big recital. 2. You cannot go out tonight because you are making cross-stitches of every Kanye West tweet. 3. You cannot go out tonight because you have decided to wash your hair every other day instead of every day, but you will not go out with dirty hair. 4. You cannot go out tonight because it is not Halloween. 5. You cannot go out tonight because you do not want to miss a fire tweet from J.K. Rowling.\n",
            "315\n",
            "pred: Lt million million millioner million million of a a\n",
            "spoiler: I have no reason to tell you, and others, something that is not of interest to you.\n",
            "316\n",
            "pred: Lt million million million of the\n",
            "spoiler: While still abiding by the new dress code, Rivas is now showing up to work in head-to-toe costumes.\n",
            "317\n",
            "pred: ts the a\n",
            "spoiler: Digit\n",
            "318\n",
            "pred: KKKCKCCtt million million million Trump million million of a of\n",
            "spoiler: He would come into my classroom, grab a Post-It, write something and stick it to my desk on his way out. One read, ‘I love you so much, my empress’. He would always call me ‘my empress’.\n",
            "319\n",
            "pred: A million million of the a the\n",
            "spoiler: brave gentlemen perforates his phone's battery using the tip of a knife, causing a reaction that makes it explode into flames!\n",
            "320\n",
            "pred: C million million million of of the\n",
            "spoiler: The bill would allocate $19.5 billion in funds to NASA in 2017, but it has a critical mission for the space agency: send men to Mars.\n",
            "321\n",
            "pred: Kin million million million Trump million million to the the a a a\n",
            "spoiler: Guns N’ Roses\n",
            "322\n",
            "pred: C million million million Trump million million to of a a\n",
            "spoiler: He said: \"Google is directly engaged with Hillary Clinton’s campaign\" and claimed the technology giant used the US State Department on a quid pro quo\" basis\n",
            "323\n",
            "pred: T million million million Trump million million of the a a\n",
            "spoiler: Clinton\n",
            "324\n",
            "pred: C million million millions of of of a a\n",
            "spoiler: She sits down in her shabby overcoat and... starts playing the piano. However, it isn’t just a bit of ‘Chopsticks’! No, indeed. This musical piece was composed by Natalie when she was just 13-years-old!\n",
            "325\n",
            "pred: KLin million million million Trump millions the a\n",
            "spoiler: El Chavo\n",
            "326\n",
            "pred: KKBBBCCtt million million million Trump million millions of the\n",
            "spoiler: transitioning my portfolio into more income-based opportunities that can provide steady money to cover my expenses each year\n",
            "327\n",
            "pred: Kin million million million of of a a a\n",
            "spoiler: $15 million\n",
            "328\n",
            "pred: Ltt million millions of a\n",
            "spoiler: you’re startling it while it’s in its comfort zone. In worst case scenarios, it can react by hurting itself, breaking a household object, or remaining anxious for days\n",
            "329\n",
            "pred: Kinin million million millions the the the\n",
            "spoiler: \"Learning to deal with death and grief.\"\n",
            "330\n",
            "pred: Kin million million million Trump million million of of a a\n",
            "spoiler: Hypnosis\n",
            "331\n",
            "pred: Lter million million millioner millionerer million to a\n",
            "spoiler: Originally written by Mr. Mellencamp as a valentine to his hometown of Seymour, Ind.,\n",
            "332\n",
            "pred: Kin million million Trump million million million the the a a a\n",
            "spoiler: Elmore Leonard\n",
            "333\n",
            "pred: TTBBTTTttt millions of a\n",
            "spoiler: island of Monserrat\n",
            "334\n",
            "pred: Kin million million millions of a a\n",
            "spoiler: someone in Miami, someone he respected, telling the superstar that going to the Cavaliers was the biggest mistake of his career\n",
            "335\n",
            "pred: Tin million million million Trump million of of a a\n",
            "spoiler: Plum Creek Timber Rick Holley\n",
            "336\n",
            "pred: LLererer millioner million million millioner Trump million million to a to\n",
            "spoiler: Hala Kamil the Oscar-nominated dir. wants to make Syria's crisis matter again\n",
            "337\n",
            "pred: CorKAACACCCtttint million million million Trump million millioner million million of of\n",
            "spoiler: Kathryn Smith Two inmates took justice into their own hands and brutally attacked the woman in a stairwell that was absent of video surveillance. While ten other inmates watched, the two women went to working beating Smith, using a homemade knife to stab her and punch her numerous times. While Smith howled in pain, the others watched for four minutes as the attack continued, resulting in a cut across her cheek that was an inch in length.\n",
            "338\n",
            "pred: Tin million million million of of a\n",
            "spoiler: $290 million\n",
            "339\n",
            "pred: L million million millions of a a\n",
            "spoiler: 15 percent of the new units it creates will go to low- or middle-income families\n",
            "340\n",
            "pred: TBATTtt million million millions of the\n",
            "spoiler: A 35-year-old woman in Melbourne, Australia collapsed after her pair of skinny jeans stopped the blood flow in her calf muscles!\n",
            "341\n",
            "pred: A millions the a a\n",
            "spoiler: Black Mirror's 'San Junipero': 2016's most life-affirming piece of television\n",
            "342\n",
            "pred: KBt million million million Trump million million the a a a\n",
            "spoiler: The statue, titled \"Sleepwalker\" Wellesley College\n",
            "343\n",
            "pred: Ct million million million of of of the the\n",
            "spoiler: Type 2 diabetes\n",
            "344\n",
            "pred: L million million millions of of to to the\n",
            "spoiler: Janine Hall\n",
            "345\n",
            "pred: KKKKTKKCttinttt million million Trump million million millions and\n",
            "spoiler: \"[I told her] ‘that’s fine’, with a smile, and she moved aside, waiting for a white person. I told my staff that she is racist and that we won’t be serving her, while I continued serving other customers with a smile.\"\n",
            "346\n",
            "pred: Tt million million million Trump million million of of the\n",
            "spoiler: the little girl says something so profound that it changes the mother’s mind\n",
            "347\n",
            "pred: BTCtts of a\n",
            "spoiler: trudged through 32.5km of heavy Alpine snow\n",
            "348\n",
            "pred: Att million million million the a a\n",
            "spoiler: wedding dress\n",
            "349\n",
            "pred: Kt million million million of the a a\n",
            "spoiler: retract and disappear if tampered\n",
            "350\n",
            "pred: KKCttt millioner million million millions of a\n",
            "spoiler: six or a seven\n",
            "351\n",
            "pred: $ to,\n",
            "spoiler: There will be fights. There will be anger. There will be score-keeping. there will be hard times. It’s OK that marriage is hard.\n",
            "352\n",
            "pred: Kin million million million of of of a\n",
            "spoiler: Pounded by the Pound: Turned Gay by the Socioeconomic Implications of Britain Leaving the European Union\n",
            "353\n",
            "pred: T million million million Trump million million the of a a a\n",
            "spoiler: saving them to use in 2017 plan on taking advantage of the fact that their company lets them roll over unused days. many U.S. workers feel bound to their jobs\n",
            "354\n",
            "pred: Ltt million million Trump million million millions a a\n",
            "spoiler: Trey Songz\n",
            "355\n",
            "pred: Tt million million million Trump million million of of a a\n",
            "spoiler: pilot made sure the plane didn’t crash into an apartment building\n",
            "356\n",
            "pred: KL million million millions of the the a the\n",
            "spoiler: mediation gardens prayer rooms chapels\n",
            "357\n",
            "pred: Cin million million million the of of a\n",
            "spoiler: the S&P 500 rose in each of the first three quarters\n",
            "358\n",
            "pred: C million million millions the the the to to a\n",
            "spoiler: BUY CLOTHES OUT OF SEASON STOCKPILE WHEN THE PRICE IS RIGHT ASK FOR DISCOUNTS DON'T BE AFRAID TO PRAISE AND COMPLAIN CHECK FOR A CASHBACK DEAL\n",
            "359\n",
            "pred: t million million million the the\n",
            "spoiler: far too early to declare Rio any sort of ratings disaster\n",
            "360\n",
            "pred: Kt million million millions of a\n",
            "spoiler: newspaper pulp\n",
            "361\n",
            "pred: VirginiaTrumpCt million million million of a a\n",
            "spoiler: The poll found that 48 percent of Americans approve of Obama speaking on the phone with Iranian President Hassan Rouhani, while 31 percent said they disapprove (another 21 percent weren't sure).\n",
            "362\n",
            "pred: KCin million million millions of a a\n",
            "spoiler: Oregano\n",
            "363\n",
            "pred: K million million millioner million million of of of a of a a a\n",
            "spoiler: This means that the day of St. Patrick's death — March 17 in the Gregorian calendar — falls on March 30 for the Russian Orthodox Church.\n",
            "364\n",
            "pred: TrumpBCCCtt Trump million million million Trump million Trump Trump million of of\n",
            "spoiler: Californian state law defines rape as penetration by the penis.\n",
            "365\n",
            "pred: Kinin million million million Trump million million the the a a the\n",
            "spoiler: great tomatoes\n",
            "366\n",
            "pred: C Trump million million million Trump million of a a\n",
            "spoiler: California's economy grew by 4.1 percent in 2015, according to new numbers from the Bureau of Economic Analysis, tying it with Oregon for the fastest state growth The Kansas economy, on the other hand, grew 0.2 percent in 2015. That's down from 1.2 percent in 2014\n",
            "367\n",
            "pred: KKBBKtts the\n",
            "spoiler: \"BLACK PPL\" typed at the top of their receipt\n",
            "368\n",
            "pred: Lt million million millions of the\n",
            "spoiler: many women we spotted wearing the exact same outfit\n",
            "369\n",
            "pred: KC million million millions of a a a\n",
            "spoiler: you can only live there for ten months out of the year.\n",
            "370\n",
            "pred: KCer million million million Trump million millioner million of to of of of\n",
            "spoiler: Ruben Guerrero\n",
            "371\n",
            "pred: KC million million millions of a a a\n",
            "spoiler: Ramen\n",
            "372\n",
            "pred: Lerer million million millioner millions to a\n",
            "spoiler: Benjamin Deeds Comes Out As Gay\n",
            "373\n",
            "pred: LLer Trump million million million Trump million of a a\n",
            "spoiler: they were real! Or at least, were based on real stories\n",
            "374\n",
            "pred: KCiner Trump Trump million million million Trump million of to\n",
            "spoiler: Tawny Kitaen\n",
            "375\n",
            "pred: KCin million million millions of the\n",
            "spoiler: Maryland\n",
            "376\n",
            "pred: T million million million of of a a\n",
            "spoiler: going for a walk\n",
            "377\n",
            "pred: ATCtt million million millions the a\n",
            "spoiler: 'The Choking Game,'\n",
            "378\n",
            "pred: Ttt million millioner million million million the\n",
            "spoiler: What determines your success isn’t \"What do you want to enjoy?\" The question is, \"What pain do you want to sustain?\" The quality of your life is not determined by the quality of your positive experiences but the quality of your negative experiences.\n",
            "379\n",
            "pred: KKABAAAttts of,\n",
            "spoiler: double-checking that the high-quality option is checked browser could be causing your streaming issues While watching a movie or TV show, press Shift+Opt or Shift+Alt (for Windows) and left click to reveal the evasive Stream Manager menu. The options on this menu can help fix buffering.\n",
            "380\n",
            "pred: Lss the a a\n",
            "spoiler: Melissa McCarthy is Abby Yates, Kristen Wiig is Erin Gilbert, Kate McKinnon is Jillian Holtzmann and Leslie Jones is Patty Tolan.\n",
            "381\n",
            "pred: Kt million million millions of a\n",
            "spoiler: body image issues\n",
            "382\n",
            "pred: t million million million the a a a\n",
            "spoiler: lucky first guess while playing the clock game\n",
            "383\n",
            "pred: TLt Trump Trump million million million Trump million of of\n",
            "spoiler: Katy Perry kept joking on set, calling herself ‘No. 1’ on the call sheet\n",
            "384\n",
            "pred: Ktiner Trump million million million Trump million of of a\n",
            "spoiler: Bizzle\n",
            "385\n",
            "pred: Kt million million millions of a\n",
            "spoiler: \"When I make my first entrance. I’d like to come out of the door carrying a cane and then walk toward the crowd with a limp. After the crowd sees Willy Wonka is a cripple, they all whisper to themselves and then become deathly quiet. As I walk toward them, my cane sinks into one of the cobblestones I’m walking on and stands straight up, by itself; but I keep on walking, until I realize that I no longer have my cane. I start to fall forward, and just before I hit the ground, I do a beautiful forward somersault and bounce back up, to great applause.\"\n",
            "386\n",
            "pred: KKKTttts the\n",
            "spoiler: Coln River in Gloucestershire\n",
            "387\n",
            "pred: t million million million of the the\n",
            "spoiler: slash\n",
            "388\n",
            "pred: BCC Trump Trump million million Trump million of a a\n",
            "spoiler: Vince Gill\n",
            "389\n",
            "pred: KC million million millions of a a a\n",
            "spoiler: A photo posted by Paris-Michael K. Jackson (@parisjackson) on OK, so she's there and there's possibly someone in the other front seat of the car. But who the fuck is that in the hat in the back seat lurking in the shadows? It's quite obviously MJ. No one else wears a fedora and a mask, right?\n",
            "390\n",
            "pred: 1The.\n",
            "spoiler: An ongoing study out of Sweden seems to indicate that a shorter work day may actually result in more productivity.\n",
            "391\n",
            "pred: t million million million of a\n",
            "spoiler: Uber let us get behind the wheel and experience what it's like to monitor a giant robot on wheels, but not everything went according to plan.\n",
            "392\n",
            "pred: T million million million of of a a\n",
            "spoiler: They claim to have lost faith in the Croatian Football Federation and have accused it of widespread corruption.\n",
            "393\n",
            "pred: A million million million of a a\n",
            "spoiler: Virtual reality will get more physical VR is an industry-wide effort A wireless Rift or Vive won't happen soon\n",
            "394\n",
            "pred: KKKLter million million millions of a\n",
            "spoiler: \"Mandela: Long Walk to Freedom\" in 2017\n",
            "395\n",
            "pred: KBCtin Trump Trump Trump million million million of,\n",
            "spoiler: congestive heart failure, along with a liver mass\n",
            "396\n",
            "pred: T million million million of a a\n",
            "spoiler: the effect of jet lag increased the air pressure inside the cabin\n",
            "397\n",
            "pred: Lin million million millions of a\n",
            "spoiler: cheat programs\n",
            "398\n",
            "pred: GGGt million million millions of a\n",
            "spoiler: Self-crucifixion: Philippines Bunny killing: New Zealand Clay pot throwing: Corfu Inter-church rocket war: Chios Hill burning: Texas\n",
            "399\n",
            "pred: KC million million million Trump million million of of the a a\n",
            "spoiler: People who work in schools, higher ed, public institutions of education — they are government employees. Most of the not-for-profits we work with would be 50 to 90 percent government funded \"Take the mortgage deduction,\" he continued. \"This is to stimulate homeownership amongst people who are already going to own homes. That is worth, to a middle-income family, a hundred bucks a year. There is data for the number of firearms manufactured, licenses, inspections\n",
            "400\n",
            "pred: KTTCtt million million millions the\n",
            "spoiler: So, when his buddy Danny Kolzow, a 23-year-old nurse at Baylor All Saints in Fort Worth, TX, found himself in need of a kidney, McMillan didn’t hesitate to find out if he could donate one of his.\n",
            "401\n",
            "pred: A million million million of of of the\n",
            "spoiler: They believe that selflessness at work leads to exhaustion, and ironically often hurts the people you intended to help\n",
            "402\n",
            "pred: C million million millions of of a a\n",
            "spoiler: Have a Bunch of Money\n",
            "403\n",
            "pred: A million million million of of a a a\n",
            "spoiler: Braconid\n",
            "404\n",
            "pred: KCt million million millions of a a\n",
            "spoiler: Cured egg yolks\n",
            "405\n",
            "pred: Kt million million millions of a a\n",
            "spoiler: iconic John Williams music\n",
            "406\n",
            "pred: BTTttts of a\n",
            "spoiler: 1. Take a long, warm shower with sweet-smelling essential oils. 2. Create the perfect ambience before you head over to sleep. 3. Let your bed exclusively be for sleeping. 4. Read or listen to something soothing. 5. Tell yourself a story.\n",
            "407\n",
            "pred: KKBBBTttts of a\n",
            "spoiler: distraction\n",
            "408\n",
            "pred: KKBBCCCmanmanermanererer Trump million million millioner million million Trump million Trump Trump million of of\n",
            "spoiler: Lord Ivar Mountbatten\n",
            "409\n",
            "pred: Kt million million million Trump million million of the a a\n",
            "spoiler: \"Don't sacrifice happiness for that final number in the bank.\"\n",
            "410\n",
            "pred: Lt million million millions of of a\n",
            "spoiler: \"Who is watching your kids?\"\n",
            "411\n",
            "pred: t million million million to the\n",
            "spoiler: To ensure you're building a good rapport with users, don't be disingenuous or misrepresent your brand's identity on social media.\n",
            "412\n",
            "pred: L million million millioner million million the of of a a a\n",
            "spoiler: Celia Cruz\n",
            "413\n",
            "pred: C million million millions of of a a a\n",
            "spoiler: \"I've got three more years on my contract with United and the desire to win more titles here.\"\n",
            "414\n",
            "pred: TLt million million millioner million million the the a of a a a\n",
            "spoiler: \"Malibu,\" \"The Beach Boy,\" Homesick for Another World \n",
            "415\n",
            "pred: Cin million million million the the a a a\n",
            "spoiler: $117 billion\n",
            "416\n",
            "pred: Ttt million million million the of of a\n",
            "spoiler: She wanted to know why the Chihiro's parents turned into pigs, what was in their food, and how she managed to pass her final exam. In the letter Studio Ghibli explained that the swine transformation represented the greed that took place during Japan's bubble economy of the 80s.\n",
            "417\n",
            "pred: KKman million million million Trump million of a a\n",
            "spoiler: The title, Al-Maria says, is in part a reference to the American \"national holiday of shopping,\" but also a nod to the holy nature of Fridays in Islam.\n",
            "418\n",
            "pred: T million million million the of of a a\n",
            "spoiler: 1. Get an education 2. Say hello to risk 3. Take that risk through index funds 4. Put a lid on fees 5. Use a Roth IRA or a Roth 401(k)\n",
            "419\n",
            "pred: K million million million of of of a\n",
            "spoiler: New York\n",
            "420\n",
            "pred: KKBBBCmanmanerer Trump Trump million million million Trump million of,\n",
            "spoiler: Michael Sam\n",
            "421\n",
            "pred: As the a a\n",
            "spoiler: Police in western Sweden initially thought a man whose car was stuck in the snow was Danish, but after a while they realized he was slurring his words because he was drunk.\n",
            "422\n",
            "pred: T million million millions the a a a\n",
            "spoiler: Swoggi actually delivers its promises\n",
            "423\n",
            "pred: LLerer millioner million million millions a a\n",
            "spoiler: yellow\n",
            "424\n",
            "pred: TCt million million millioner million million the the a of\n",
            "spoiler: The Polished Man Project which aims to raise awareness about the number of children who face sexual abuse\n",
            "425\n",
            "pred: Lt million million millions a a a\n",
            "spoiler: Gender matters not when you're rocking the Force like you are, baby!\n",
            "426\n",
            "pred: KKinererer million million million Trump million of a a\n",
            "spoiler: men had just left Officer Smith’s funeral\n",
            "427\n",
            "pred: Ct million million million Trump million of a a\n",
            "spoiler: climate change\n",
            "428\n",
            "pred: C million Trump million million million of to a a a\n",
            "spoiler: Trump ran up huge margins with white Florida voters -- who comprise 64 percent of the state’s voting rolls -- and have always been the most reliable voters in terms of turnout.\n",
            "429\n",
            "pred: Tt million million million of the\n",
            "spoiler: Y Combinator to test a scheme called Universal Basic Income\n",
            "430\n",
            "pred: K million million million of the\n",
            "spoiler: 1. Iceland\n",
            "431\n",
            "pred: Tt million million million to the a a\n",
            "spoiler: In short: he is giving his owner some seriously relentless side-eye\n",
            "432\n",
            "pred: KC million million millions of a a a\n",
            "spoiler: supermarkets\n",
            "433\n",
            "pred: T million million million of of a a\n",
            "spoiler: not available for pre-order\n",
            "434\n",
            "pred: t million million million to the\n",
            "spoiler: textbooks\n",
            "435\n",
            "pred: KBBTTttt of,\n",
            "spoiler: he belts out a few of his favorite Broadway tunes\n",
            "436\n",
            "pred: Tt million million millions of a a\n",
            "spoiler: People waste years of their lives not being willing to waste hours of their lives. If you mistake busyness for importance, which we do a lot, you're not able to see what really is important\n",
            "437\n",
            "pred: VirginiaKGKGGGKttts of a\n",
            "spoiler: Austin, Texas\n",
            "438\n",
            "pred: Kin million million million of the a a\n",
            "spoiler: juvenile-onset disorder\n",
            "439\n",
            "pred: C million million millioner million million to of a a\n",
            "spoiler: \"YInMn blue.\"\n",
            "440\n",
            "pred: t million million million of of a\n",
            "spoiler: blue\n",
            "441\n",
            "pred: KBCCin million million millions the a\n",
            "spoiler: It is a form of apophenia, when people see patterns in random, unconnected data\n",
            "442\n",
            "pred: Kin million million million of of a a\n",
            "spoiler: \"Parenthood,\"\n",
            "443\n",
            "pred: KKt million million millioner million million the of of a a\n",
            "spoiler: strong winds can still carry the stench back over to the city on a hot day\n",
            "444\n",
            "pred: As the a a a\n",
            "spoiler: albino raccoon\n",
            "445\n",
            "pred: KLter million million million Trump million million of a a\n",
            "spoiler: Jason Aldean\n",
            "446\n",
            "pred: KKTttts the\n",
            "spoiler: he called up JK and asked if he could say ‘Harry put it in his pocket’ instead. She thought for a moment, then said ‘no’, and hung up.\n",
            "447\n",
            "pred: L million million millions of of the the the\n",
            "spoiler: 1,448.7 days\n",
            "448\n",
            "pred: Tt million million millions of a\n",
            "spoiler: Before a game, I’m always listening to gospel music.\n",
            "449\n",
            "pred: $ million million millions of the\n",
            "spoiler: can afford more than that\n",
            "450\n",
            "pred: Kiner million million million Trump million the a a\n",
            "spoiler: Rodgers earned the role on his talent alone\n",
            "451\n",
            "pred: C million million millions of of a a\n",
            "spoiler: Thea: The Awakening added multiplayer\n",
            "452\n",
            "pred: Lt million million millions of the the\n",
            "spoiler: 1) Readers wanted to read about it 2) Mad Men debuted right after The Sopranos ended and filled the void the show left 3) There just wasn't as much good TV on 4) Mad Men was actually pretty popular, when all was said and done 5) Never underestimate passion\n",
            "453\n",
            "pred: $ the\n",
            "spoiler: 1. Eat legumes 2. Eat plant sterols, margarines and spreads 3. Eat nuts 4. Use olive oil 5. Avoid junk food\n",
            "454\n",
            "pred: K million million million the of a the\n",
            "spoiler: Pennsylvania\n",
            "455\n",
            "pred: CCin million million millions of the\n",
            "spoiler: plastic debris\n",
            "456\n",
            "pred: K million million millioner million million the of the a a a\n",
            "spoiler: According to new findings from the Pew Research center, 40 percent of marriages involve one spouse who's been married before.\n",
            "457\n",
            "pred: Kt million million millions of of the\n",
            "spoiler: Amber Galloway Gallego is not your everyday American Sign Language (ASL) interpreter — she specialises in the interpretation of music, in real time and in front of thousands of people. Taylor Anderton and Michael Cox, who were both born with Down syndrome, have known each other for six years and been a serious couple since last year. When Michael partnered Taylor at the 16th Gold Coast Debutante Ball for Disability, they had already decided they would spend the rest of their lives together — and our video of their love story has had more than 13 million views on social media. Louise the infant koala — a squeaking, wet, grey bundle of fur found on the ground after the east coast storms — became a symbol of hope after wild weather hit Australia's east coast in June. The storm cell brought tragedy and tears to many who lost loved ones, livelihoods, and homes, but when the iconic little Australian was rescued — and named after rescuer Louise Haynes — her fate was soon being followed by thousands of fans on social media.\n",
            "458\n",
            "pred: TTtt million millions of a\n",
            "spoiler: Kili the Senegal parrot has somehow managed to become a master at \"playing dead.\"\n",
            "459\n",
            "pred: Ktt million millioner million million million the of of a of\n",
            "spoiler: Miley Cyrus\n",
            "460\n",
            "pred: $ the\n",
            "spoiler: The items on the lengthy checklist include childhood favorites, such as going on a bike or scooter ride, eating ice cream or candy and watching a favorite TV program\n",
            "461\n",
            "pred: Kin million million millions of a a a\n",
            "spoiler: roll back the time in the device by around 25 minutes.\n",
            "462\n",
            "pred: C million million millioner million million of of the a a\n",
            "spoiler: handwashing habits\n",
            "463\n",
            "pred: KLman Trump Trump Trump million million Trump million of a\n",
            "spoiler: Jussie Smollett\n",
            "464\n",
            "pred: ts the a\n",
            "spoiler: \"Many Republican voters have an unfavorable impression of him — worse still, many don’t even think he has the knowledge, temperament or qualifications to be president,\" Cohn wrote. \"If Mr. Trump can’t reunify Republican-leaning voters, Mrs. Clinton really could win in a landslide.\"\n",
            "465\n",
            "pred: Ats the a\n",
            "spoiler: post pictures of your boarding pass on social media\n",
            "466\n",
            "pred: Kin million million million Trump million million to of of\n",
            "spoiler: the \"Delaware loophole\" — the unassuming building at 1209 North Orange Street — has become, as the Guardian described Reportedly dozens of Fortune 500 companies — Coca-Cola, Walmart, American Airlines, and Apple, to name a few — use Delaware’s strict corporate secrecy laws and legal tax loopholes by registering the North Orange Street address for official business.\n",
            "467\n",
            "pred: Lt million million million Trump million million the a a a\n",
            "spoiler: Kevin Hart\n",
            "468\n",
            "pred: L million million millioner million million the the a a a\n",
            "spoiler: HBO Go is also coming to Amazon's new $99 streaming media player, Amazon Fire, by the end of the year\n",
            "469\n",
            "pred: KKKBKKtts of,\n",
            "spoiler: $25,000 per month\n",
            "470\n",
            "pred: C million million million to of a a a\n",
            "spoiler: #1: Goodbye Sapphire? #2: New Home Button, New Limitations\n",
            "471\n",
            "pred: As the a\n",
            "spoiler: University of Vermont\n",
            "472\n",
            "pred: BTt million million million Trump million million of of the\n",
            "spoiler: Doors open based on who you know As you build your network of contacts, remember to follow up In a new situation, aim to assimilate Dress for the job you want Be humble and willing to learn from any task\n",
            "473\n",
            "pred: KKKCtt million millions the,\n",
            "spoiler: 1. Museum of Modern Art 2. Metropolitan Museum of Art 3. Guggenheim 4. The Whitney Museum of American Art 5. Musée du Louvre\n",
            "474\n",
            "pred: KBBBttt million million millions a\n",
            "spoiler: a vegan athlete can compete effectively at a high level\n",
            "475\n",
            "pred: KKCCt million million millions the a\n",
            "spoiler: Bolivia's chaotic San Pedro Prison became a tourist attraction, a place where backpackers stayed for weeks and partied with inmates\n",
            "476\n",
            "pred: Cin million million million of a a\n",
            "spoiler: a charcoal-black moon\n",
            "477\n",
            "pred: Ats the a a\n",
            "spoiler: Tokyo Chicago London\n",
            "478\n",
            "pred: Kter millioner million million millions the a\n",
            "spoiler: they'll be teasing their collaboration on Snapchat\n",
            "479\n",
            "pred: Cin million million million Trump million million to of a a a\n",
            "spoiler: Mexico City\n",
            "480\n",
            "pred: T million million million of of a\n",
            "spoiler: MS Dhoni\n",
            "481\n",
            "pred: KBBBttt million millions of the\n",
            "spoiler: 4 mm tumor in her brain\n",
            "482\n",
            "pred: Lt million million millioner million million the the the\n",
            "spoiler: basically the 'undercut,'\n",
            "483\n",
            "pred: Lter million million million Trump million million of to a\n",
            "spoiler: she had plastic surgery years ago to make her eyes look \"bigger\"\n",
            "484\n",
            "pred: LLerer Trump Trump Trumper Trump million of a\n",
            "spoiler: Shape Magazine - Meryl Streep and Robert Redford shock the world with plans of getting married after years of keeping their relationship a secret.\n",
            "485\n",
            "pred: K million million millions of a a a\n",
            "spoiler: tons of benefits to metcon workouts, including improved fat loss, increased calorie burn compared to steady-state workouts, muscular strength gains, and improved cardiovascular function\n",
            "486\n",
            "pred: Ler millioner million million million Trump million the,\n",
            "spoiler: Katy Perry\n",
            "487\n",
            "pred: Cin million million million of a a\n",
            "spoiler: Netherlands\n",
            "488\n",
            "pred: KKKCinininererer millioner million million million Trump million million of,\n",
            "spoiler: continued posting photos to her account, along with perfect messages for her critics\n",
            "489\n",
            "pred: in millions the a\n",
            "spoiler: DICE showed him a small demo of what it had in mind\n",
            "490\n",
            "pred: A million million million the a the a a\n",
            "spoiler: photograph of a doormat\n",
            "491\n",
            "pred: BTttt million million millions of a\n",
            "spoiler: 1. A SINGLE TABLESPOON HAS MORE SUGAR THAN A CHOCOLATE CHIP COOKIE. 2. IT OVERPOWERS EVERYTHING IT TOUCHES. 3. THAT GLOPPY TEXTURE IS FOUL. 4. THE SMELL. 5. I CAN NEVER ESCAPE IT.\n",
            "492\n",
            "pred: A million million million the the a the\n",
            "spoiler: Kat Cole\n",
            "493\n",
            "pred: Kts million million the,\n",
            "spoiler: 1937 Bugatti Type 57S amid piles of medical machinery 1,500 beer steins thousands of receipts World War II spy drone\n",
            "494\n",
            "pred: Kin million million million Trump million of a a\n",
            "spoiler: \"I feel like a Republican now because they don’t stand for same-sex marriage, and that appeals to me,\" he said.\n",
            "495\n",
            "pred: LLererer million million millioner millionerer P the a a a\n",
            "spoiler: Peter Pilotto\n",
            "496\n",
            "pred: GBBCtt million million millioner million million the of of of the the the a of the\n",
            "spoiler: irregular working hours and work stress in the IT, media industry and BPOs\n",
            "497\n",
            "pred: Ct Trump million million million Trump million of the\n",
            "spoiler: 'Hey Trump, f*** you!'\n",
            "498\n",
            "pred: BCCt Trump Trump million million million of,\n",
            "spoiler: Nebraska allocates its electoral college votes not in the usual winner-take-all method, but by congressional district.\n",
            "499\n",
            "pred: C million million million Trump million million to of of\n",
            "spoiler: Introducing what he billed as an \"energy plan,\" Trump promised to \"cancel the Paris Climate Plan.\"\n",
            "500\n",
            "pred: K million million million the the the to the the\n",
            "spoiler: 1. Adventurous Christmas: Chile 2. Premium Christmas: Switzerland 3. Christmas in Santa's home: Finland 4. Christmas-at-sea: Genting 5. Break the bank Christmas: The Maldives\n",
            "501\n",
            "pred: in million million million of a\n",
            "spoiler: SECOND WIND NEW REALITY FRESH FACES LOCATION, LOCATION, LOCATION GENERATION X\n",
            "502\n",
            "pred: TrumpKCCCt million million millions of the\n",
            "spoiler: \"Despite being awake, they are not physically active. They spend their day sitting inside cars, at their desk or in meeting rooms. They usually lie down while watching TV or checking their smartphones,\"\n",
            "503\n",
            "pred: t million million the a\n",
            "spoiler: online multiplayer unplayable\n",
            "504\n",
            "pred: KCCtss of a\n",
            "spoiler: raw apple and raw lettuce\n",
            "505\n",
            "pred: FloridaTTTCCTttts of the\n",
            "spoiler: allergic reaction from a wasp like John was stung by\n",
            "506\n",
            "pred: KKCCtt million millions of a\n",
            "spoiler: fettuccine Alfredo\n",
            "507\n",
            "pred: Ct million million millions of of a a\n",
            "spoiler: serious heart condition, and also attributes her two miscarriages\n",
            "508\n",
            "pred: BATTTtts the\n",
            "spoiler: remove sugar from my diet, stop late-night eating and start using the gym membership\n",
            "509\n",
            "pred: KKKCininin million million million Trump million of,\n",
            "spoiler: Fire authorities in Western Australia say no properties were lost during a bushfire around the town of Gwindinup south-east of Bunbury, which has now been contained.\n",
            "510\n",
            "pred: KBCCCttt million million millions of a\n",
            "spoiler: 1.) Housing 2.) School Fees 3.) Food medicines for chronic illness (Cancer, HIV, etc), livestock for income, and clothing & shoes. no evidence that vice consumption is different from others in the community who did not receive transfers.\n",
            "511\n",
            "pred: A million million million the of of a a the\n",
            "spoiler: The logic of the drugs war only leads one way: the police get smarter, so the criminals get nastier\n",
            "512\n",
            "pred: BTCtts of a\n",
            "spoiler: Gulls are what's known as \"opportunistic kleptoparasites,\" meaning they don't derive 100 percent of their diets from heist but rather just steal when the opportunity arises. And a big vat of tikka masala is an opportunity no gull can afford to miss.\n",
            "513\n",
            "pred: Kin Trump million million million of of the the\n",
            "spoiler: the title page\n",
            "514\n",
            "pred: Kin Trump Trump million million million of a a a\n",
            "spoiler: Shamus Beaglehole\n",
            "515\n",
            "pred: Kin million million million Trump million million to a of\n",
            "spoiler: When you trade your DNA for information you are also giving it away -- and the database you give it to, owns it. What if that company goes out of business and sells the database? Do the protections you have been promised still stand?\n",
            "516\n",
            "pred: BTCtt million million millions the a\n",
            "spoiler: Wolverampton\n",
            "517\n",
            "pred: KLinin million million millions of a\n",
            "spoiler: colourful igloo\n",
            "518\n",
            "pred: LLerer million million million Trump million millions a a\n",
            "spoiler: none can claim to rival the atmosphere and immersion of Unbreakable\n",
            "519\n",
            "pred: KKCinin million million millions of a\n",
            "spoiler: free burger (up to $10 value) with the purchase of an adult entree on Sunday at Ruby Tuesday BurgerFi is selling $5 cheeseburgers on Sunday Delta (DAL) will serve Shake Shake (SHAK) burgers aboard select flights from JFK to LAX on Sunday. In addition, customers on the flights will receive a voucher for a free ShackBurger at the LA West Hollywood Shake Shack. Hurricane Grill & Wings is offering $5.99 beef, turkey or veggie burgers with cheese, lettuce, tomato, onion and pickles on Sunday\n",
            "520\n",
            "pred: T million million million of of a a\n",
            "spoiler: Investors are now expected to pay for the privilege of lending money to the Swiss for a half-century at a time.\n",
            "521\n",
            "pred: Tt million million millions of a\n",
            "spoiler: towel\n",
            "522\n",
            "pred: KKKBtt million million millions the a\n",
            "spoiler: J. Crew\n",
            "523\n",
            "pred: Kt million million millions of a a\n",
            "spoiler: Andy Stern \"tectonic shifts\" in the labor market that will see more and more workers replaced by robots\n",
            "524\n",
            "pred: KKKKTKTtts of,\n",
            "spoiler: might have been a result of something that happened to Walt and his family. Something tragic that involved his mother\n",
            "525\n",
            "pred: Kin million million million Trump million million of of a a a\n",
            "spoiler: him breaking a bottle of champagne on a Playboy-branded limo while several of the playmates are visiting New York City\n",
            "526\n",
            "pred: At million million million the the a the\n",
            "spoiler: a watch\n",
            "527\n",
            "pred: KBCtt million millions of a a\n",
            "spoiler: dead whale\n",
            "528\n",
            "pred: Ler million million million Trump million million to of to\n",
            "spoiler: shooter game \"Destiny\"\n",
            "529\n",
            "pred: K million million million of of a a\n",
            "spoiler: Xbox head Phil Spencer told Gamespot he doesn’t believe in the piece by piece upgrading of systems, and you won’t be seeing that with the Xbox One.\n",
            "530\n",
            "pred: K million million millioner million million of of of a a a\n",
            "spoiler: Costco AmEx cards should switch over to the new Costco Visa Anywhere card\n",
            "531\n",
            "pred: BBBCCtt Trump Trump million million million Trump million of of\n",
            "spoiler: online gaming\n",
            "532\n",
            "pred: KBLttt million million millions a\n",
            "spoiler: professional Kim Jong Un impersonator\n",
            "533\n",
            "pred: Ct million million million Trump million million to of a a\n",
            "spoiler: Only 1 participant out of 85 correctly recalled the Apple logo, and fewer than half of all participants correctly identified the logo\n",
            "534\n",
            "pred: KKKKBKKCKKininin million millions of a\n",
            "spoiler: requested Internet access\n",
            "535\n",
            "pred: Kt million million million Trump million million of of a a\n",
            "spoiler: frostbite\n",
            "536\n",
            "pred: At million millions the a\n",
            "spoiler: The rumor was released by a fake (FAKE, PEOPLE!) @Unreel_News\n",
            "537\n",
            "pred: KCiner million million millions a a\n",
            "spoiler: Christine \"Tink\" Newman four continuous hours of CPR\n",
            "538\n",
            "pred: Ktt million million millions of the the\n",
            "spoiler: used condoms being discarded under beds women flashing the porter instead of giving a tip parents forgetting to collect their children from the kids club because they were drunk call staff to the room, and then make them wait outside while they get intimate answer the door naked\n",
            "539\n",
            "pred: Lin million million millions of a\n",
            "spoiler: Bad language...ill thoughts...rude ideas...negative radical views...I don’t need.\n",
            "540\n",
            "pred: Lerer million million million Trump million million the a a a\n",
            "spoiler: José José\n",
            "541\n",
            "pred: GBTCCCt million million millions the a\n",
            "spoiler: Worldwide, developed nations would have to pay more than $1.1 trillion\n",
            "542\n",
            "pred: Kts of a\n",
            "spoiler: The unemployment rate in November fell to 4.6 percent, the lowest in nine years With 14 total world championship medals, 19-year-old Simon Biles arrived in Rio already a star. It was her near-perfect performance at the Olympics, however — which netted four golds and a bronze it was hard not celebrate alongside the Chicago Cubs after they won their first World Series in 108 years. Several women of color made political history last month when they were elected to prominent leadership positions within their respective states. Few albums in history have managed to be as simultaneously vulnerable and powerful as \"Lemonade\" — Beyonce's sixth solo record that laid bare an important slice of black womanhood.\n",
            "543\n",
            "pred: A million million of a a\n",
            "spoiler: Who can guess what evolution has in store for us ten thousand years hence?\n",
            "544\n",
            "pred: BBBCBCmanmant million million millions of the\n",
            "spoiler: foetus was detected with anencephaly, a serious defect where parts of the skull are not adequately developed\n",
            "545\n",
            "pred: C Trump million million million of of a\n",
            "spoiler: it’s been time to panic for a while\n",
            "546\n",
            "pred: Ct million million millions of a\n",
            "spoiler: Wax inside a whale’s ear stores all sorts of useful information on the animal’s exposure to pollutants and stress levels throughout life\n",
            "547\n",
            "pred: KBBBCttt million millions of a a\n",
            "spoiler: replace all the larger vessels with those of smaller sizes\n",
            "548\n",
            "pred: Lter million million millions of of a\n",
            "spoiler: \"No,\" she apparently replied, before pointing at the policeman and adding: \"But he has.\"\n",
            "549\n",
            "pred: Lin million million millioner million of of a a\n",
            "spoiler: May 8\n",
            "550\n",
            "pred: Lt million million million Trump million million of the a a\n",
            "spoiler: \"The Woman in Black\" it was different enough that people realized it was a different reason to go and see it. And the fact that it did so well and it was a really good movie\n",
            "551\n",
            "pred: KKKCCttts of a\n",
            "spoiler: The Rock\n",
            "552\n",
            "pred: KBBBCBCtt Trump Trumps of a\n",
            "spoiler: Jackson Vroman\n",
            "553\n",
            "pred: t million million of the\n",
            "spoiler: 1. Ideas Have Consequences 2. The Road to Serfdom 3. The Closing of the American Mind 4. A Choice Not an Echo 5. Capitalism and Freedom\n",
            "554\n",
            "pred: t million million million of a a\n",
            "spoiler: Researchers from the Massachusetts Institute of Technology (MIT) have developed a prototype imaging system that’s able to read pages of a book without opening it.\n",
            "555\n",
            "pred: Lt million million million Trump million million the a a\n",
            "spoiler: difficult behavior\n",
            "556\n",
            "pred: BTTttt million millions of the\n",
            "spoiler: Kiwi Tart Cherries and Tart Cherry Juice Malted Milk and Nighttime Milk Fatty Fish Nuts\n",
            "557\n",
            "pred: LLer Trump million million million Trump million of of a a a\n",
            "spoiler: potatoes\n",
            "558\n",
            "pred: Kt million million millions of a a\n",
            "spoiler: According to the top beauty gurus in the business, applying your foundation, blusher and concealer incorrectly can add decades to your looks.\n",
            "559\n",
            "pred: Kin Trump million million million Trump million of a a\n",
            "spoiler: Jay Pharoah and Taran Killam\n",
            "560\n",
            "pred: $ of.\n",
            "spoiler: Idaho\n",
            "561\n",
            "pred: ATLtt million million millions the a a\n",
            "spoiler: more empathetic towards others\n",
            "562\n",
            "pred: Ats the the a\n",
            "spoiler: Katrina Henry Admit it and go get help\n",
            "563\n",
            "pred: tt million million the,\n",
            "spoiler: Hip hop fans don’t need to feel pressured to only listen to music that is considered smart. Smart isn’t everything, and just because a song is intellectually smart, doesn’t necessarily mean it is enjoyable to listen to.\n",
            "564\n",
            "pred: Kt million million million Trump million million of of a\n",
            "spoiler: 77\n",
            "565\n",
            "pred: At million million of of the\n",
            "spoiler: Your brain needs to prune a lot of those connections away and build more streamlined, efficient pathways. It does that when we sleep.\n",
            "566\n",
            "pred: t million million the the\n",
            "spoiler: enough medals to throw any Olympic predictions seriously out of whack. And many of the medals that might have otherwise gone to Russian athletes would likely go to competitors from the US\n",
            "567\n",
            "pred: Ats the a a\n",
            "spoiler: penis\n",
            "568\n",
            "pred: Ttt million million million of,\n",
            "spoiler: 1. Don't touch her bump 2. Don't ask if she has chosen a name 3. Don't look shocked when you hear she's pregnant 4. Don't tell her she's getting bigger (and bigger) 5. Don't tell her she's not big enough\n",
            "569\n",
            "pred: KKKLtss the\n",
            "spoiler: years as a single mother\n",
            "570\n",
            "pred: ts to the the the\n",
            "spoiler: $765,759\n",
            "571\n",
            "pred: C million million million of of a a\n",
            "spoiler: save lives on the road, and protect young people from other hazards of drinking\n",
            "572\n",
            "pred: Kin million million million of a a\n",
            "spoiler: Oregon\n",
            "573\n",
            "pred: Kt Trump Trump Trump million million million Trump million of to\n",
            "spoiler: incredible eye for design\n",
            "574\n",
            "pred: KKBmanmanman million million millions the a\n",
            "spoiler: in an ambassadorial capacity\n",
            "575\n",
            "pred: KLman Trump million million million Trump million of of a a a\n",
            "spoiler: 'Dumb And Dumber' Sequel\n",
            "576\n",
            "pred: At million million the,\n",
            "spoiler: cancer\n",
            "577\n",
            "pred: t million million of the\n",
            "spoiler: fell an average of 13.7%\n",
            "578\n",
            "pred: KKCtt million million millions of a\n",
            "spoiler: tahini-walnut spread\n",
            "579\n",
            "pred: Tt million million million Trump million of of of\n",
            "spoiler: singing the set's opening number \"Prancer\" sitting on a couch, reading newspaper and sipping tea.\n",
            "580\n",
            "pred: BBBBTttts of a\n",
            "spoiler: strawberry rhubarb\n",
            "581\n",
            "pred: Kin million million million the a a\n",
            "spoiler: paper bag.\n",
            "582\n",
            "pred: $ the\n",
            "spoiler: And instead of boiling them, we should be steaming, stir-frying or even microwaving Brussels to get that perfectly crunchy consistency.\n",
            "583\n",
            "pred: BTttt million millions of the\n",
            "spoiler: Moisturizer\n",
            "584\n",
            "pred: Tin million million millions of the\n",
            "spoiler: Jimmy Smits\n",
            "585\n",
            "pred: T million million million of of a a\n",
            "spoiler: There’s a clear winner here, and unless Linux rectifies its performance disparity, lack of natively supported control options and impoverished game library, the OS to beat for PC gaming will remain Windows 10.\n",
            "586\n",
            "pred: t million million million of the\n",
            "spoiler: 1. Stand in front of her 2. Have a relaxed, easy-going smile. 3. Is she hasn’t already looked up at you, simply get her attention with a wave of your hand. Wave your hand in her direct line of vision so she can’t ignore it. 4. When she looks at you, smile and point to her headphones and say, \"Take off your headphones for a minute\" and pretend to be taking headphones off your head, so she fully understands. 5. Then, do what we call \"Acknowledging the Awkwardness\" by quickly mentioning something about the awkwardness of the moment\n",
            "587\n",
            "pred: Lt million million millioner million million to of the\n",
            "spoiler: rejecting a fans’s gift and calling it ‘shit’\n",
            "588\n",
            "pred: T million million million the a a\n",
            "spoiler: 1. Half-Life 3 2. The Last Guardian 3. Final Fantasy XV 4. Kingdom Hearts 3 5. Beyond Good & Evil 2\n",
            "589\n",
            "pred: Ats of of the\n",
            "spoiler: the guard screamed \"Step back from the Queen's Guard!\" and thrust his rather-deadly gun complete with rather-pointy bayonet in his face\n",
            "590\n",
            "pred: VirginiaAAttts the\n",
            "spoiler: playing hide-and-seek\n",
            "591\n",
            "pred: Ct million million million Trump million of of a a\n",
            "spoiler: Laura Bush\n",
            "592\n",
            "pred: Kin million million million Trump million million to of a a a\n",
            "spoiler: A man dubbed the ‘cash card Casanova’\n",
            "593\n",
            "pred: KKCttss of a\n",
            "spoiler: pizza\n",
            "594\n",
            "pred: KCt Trump Trump million million million of a a\n",
            "spoiler: JT managed to swat away the woman -- while still keeping rhythm.\n",
            "595\n",
            "pred: K million million million the of the\n",
            "spoiler: Depending on your priorities, it might be\n",
            "596\n",
            "pred: GKGBCKKKtts of,\n",
            "spoiler: Colorado\n",
            "597\n",
            "pred: TrumpC million million million of of a a\n",
            "spoiler: Claims, which count the number of people who applied for unemployment insurance for the first time since the past week, fell to 234,000. This marks the 101st week straight of claims being below 300,000\n",
            "598\n",
            "pred: C million million millions of of a a\n",
            "spoiler: couldn't make enough toys to satiate demand in North America, and needed a break while it boosted capacity at its factories and increased its workforce by nearly a quarter.\n",
            "599\n",
            "pred: Kin Trump million million million of of a a\n",
            "spoiler: not killing off Harrison [Ford] at the end\n",
            "600\n",
            "pred: Kin million million million the a a\n",
            "spoiler: New generation e-passport featuring enhanced security features such as bio-metric details may soon be rolled out by the government.\n",
            "601\n",
            "pred: L million million millions of of a a\n",
            "spoiler: 1. The action’s going to pick back up in Fall of 1984. 2.\"I think we talked like a larger time jump where the kids are older now and it’s a different decade,\" 3. Expect four brand new characters. 4.Bringing the show’s tally of cool guys the Duffer Brothers admire up to 6 million 5. Good old monster-infested Hawkins won’t be the only location.\n",
            "602\n",
            "pred: Lt Trump Trump Trump million million million of to\n",
            "spoiler: John Slattery 'God's Pocket'\n",
            "603\n",
            "pred: ts the a\n",
            "spoiler: money\n",
            "604\n",
            "pred: KBK million million millions of of the the\n",
            "spoiler: he slowed his vehicle in a tree-covered section of the road. As the Honda Accord kept moving, he climbed almost imperceptibly out of the driver’s window\n",
            "605\n",
            "pred: KBBCttin million million millions a\n",
            "spoiler: Tulsiram Manere\n",
            "606\n",
            "pred: Kin million million million of of a a\n",
            "spoiler: she fell\n",
            "607\n",
            "pred: PTTTtts of a\n",
            "spoiler: counting each small win we make\n",
            "608\n",
            "pred: K million million millions of of a a the\n",
            "spoiler: Chicago\n",
            "609\n",
            "pred: Kin million million million of a a a\n",
            "spoiler: being a sex object is empowering\n",
            "610\n",
            "pred: KKACCCttts of,\n",
            "spoiler: Access to prisons has been vastly curtailed. There is no way to know what truly happens inside but to go there.\n",
            "611\n",
            "pred: CorTrumpACCCtt million million milliont of a\n",
            "spoiler: french fry\n",
            "612\n",
            "pred: KKBKBCKKCininererer million million millioner million Trump million million of of of a a\n",
            "spoiler: a vast chamber where several stalagmites had been deliberately broken. Most of the 400 pieces had been arranged into two rings—a large one between 4 and 7 metres across, and a smaller one just 2 metres wide. Others had been propped up against these donuts. Yet others had been stacked into four piles. Traces of fire were everywhere, and there was a mass of burnt bones. These weren’t natural formations, and they weren’t the work of bears. They were built by people.\n",
            "613\n",
            "pred: Cin million million millioner million million to of of the\n",
            "spoiler: got distracted by her own ‘erotic thoughts.’\n",
            "614\n",
            "pred: Kin million million million of the a a\n",
            "spoiler: My ex raised the thing to eye level, looked at me, and pushed the button. And the thing in his hand sprang open. My relative's foot shavings ERUPTED from within the Ped Egg like an explosion of flesh confetti\n",
            "615\n",
            "pred: A million million of the a a a\n",
            "spoiler: they cannot take 2s and expect to beat 3s\n",
            "616\n",
            "pred: Kt million million millions of a\n",
            "spoiler: robots\n",
            "617\n",
            "pred: KLin million million millions of the\n",
            "spoiler: $10,527,843,932\n",
            "618\n",
            "pred: A million million million the a a a\n",
            "spoiler: By dialing the secret phone number 1-999-367-3767, you’re automatically connected to \"Black Cellphones\" and soon after an explosion is triggered (for reasons best known to the creators).\n",
            "619\n",
            "pred: Kin million million million the of of a\n",
            "spoiler: colleges\n",
            "620\n",
            "pred: KKCin million million millions of a\n",
            "spoiler: TEL AVIV – The sister of a Palestinian terrorist who plowed his truck into a group of soldiers in Jerusalem on Sunday, killing four and wounding 16, said the family was \"thankful\" for his \"most beautiful martyrdom.\"\n",
            "621\n",
            "pred: Ler million million million Trump million million the a a\n",
            "spoiler: Neither Ben nor Jen will confirm or deny the divorce allegations.\n",
            "622\n",
            "pred: C million million million Trump million million of of of to\n",
            "spoiler: On 5 May last year, Ms Chien took her own life, using the same drug she used to put down animals. She said she wanted to help people understand what happens to strays in Taiwan\n",
            "623\n",
            "pred: TC million million million Trump Trump million of of a\n",
            "spoiler: High Country Crime Stoppers for their invaluable service\n",
            "624\n",
            "pred: VirginiaKKKKAKKBKKCKttt million million milliont million Trump million millions of of the\n",
            "spoiler: a wave and sleeping face emoji\n",
            "625\n",
            "pred: KBCtt million million million Trump million millions a a\n",
            "spoiler: Menelik Watson\n",
            "626\n",
            "pred: Lt million million millions of a\n",
            "spoiler: It was definitely \"Feeling This.\" Unless it was actually \"I Miss You.\"\n",
            "627\n",
            "pred: Lmanererer million million millioner million Trump million the a to\n",
            "spoiler: Kim Kardashian stepped out in leggings Tuesday for a Pilates class in Los Angeles\n",
            "628\n",
            "pred: L million million millions of of the a\n",
            "spoiler: Ana, the kickass heal-sniper\n",
            "629\n",
            "pred: Ct Trump Trump Trump million Trump Trumps a\n",
            "spoiler: Last week, Barkley said James was being \"whiny\" for publicly saying the Cavs need to bring in another playmaker.\n",
            "630\n",
            "pred: KBCCDCt Trump million million million Trump millions of of the\n",
            "spoiler: The shooter’s autopsy was also conducted in another building, away from the victims.\n",
            "631\n",
            "pred: Kin million million million the of of a\n",
            "spoiler: Korean pear juice\n",
            "632\n",
            "pred: KLer million million Trump million million million of of a a a\n",
            "spoiler: Blake McIver\n",
            "633\n",
            "pred: Kin million million million Trump million million of the the\n",
            "spoiler: As of Sunday in Illinois, it is now legal to hunt for catfish with pitchforks or spears Farther west in Oregon, the state legislature banned the use of 'sky lanterns' In Pennsylvania, beer customers are now able to purchase six-packs at local grocery stores, and are able to purchase beverages in any quantity from a beer distributor.\n",
            "634\n",
            "pred: KKin million million millioner million million to the the a a a\n",
            "spoiler: \"Son, you go back to study. You’re not going to be a football player. You are Chinese.\"\n",
            "635\n",
            "pred: K million million million Trump million million of of a a a\n",
            "spoiler: most-searched people of 2013\n",
            "636\n",
            "pred: Tt million million millioner million of of a\n",
            "spoiler: stopped the band and laughed off her mistake declaring: \"S***, wrong words!\"\n",
            "637\n",
            "pred: Kmaner million million million Trump million of a a\n",
            "spoiler: The quote reads: \"I need feminism because I intend on marrying rich and I can’t do that if my wife and I are making .75 cent for every dollar a man makes.\"\n",
            "638\n",
            "pred: KBBLLttts the\n",
            "spoiler: a great big belly rub\n",
            "639\n",
            "pred: L million million million Trump million million the of a a\n",
            "spoiler: parody on tonight’s episode of \"Inside Amy Schumer.\"\n",
            "640\n",
            "pred: C million million million of of a\n",
            "spoiler: For the last question, the interviewer asked: \"A right triangle has a hypotenuse equal to 10 and an altitude to the hypotenuse equal to 6. Find the area of the triangle.\"\n",
            "641\n",
            "pred: A million million million of of of a a a\n",
            "spoiler: U.K. Prime Minister has said that negotiations with the European Union over the country’s exit will require the engagement of the Scottish, Welsh and Northern Ireland governments.\n",
            "642\n",
            "pred: A million million million of of the\n",
            "spoiler: it’s a calling because you find a deep sense of purpose and positive impact in your role\n",
            "643\n",
            "pred: KKKCtt million millions of a\n",
            "spoiler: 60-year-old woman sitting alone in her car reading a book\n",
            "644\n",
            "pred: Lss the a a\n",
            "spoiler: that Rey is in fact Emperor Palpatine's granddaughter\n",
            "645\n",
            "pred: BLmanmanmanererer million million millioner million Trump million of a\n",
            "spoiler: Lisa Brown\n",
            "646\n",
            "pred: Tt million million million the of a a a\n",
            "spoiler: With the reported $115 million raised from the Ice Bucket Challenge, the ALS Association funded six research projects,\n",
            "647\n",
            "pred: Ctt million million millioner million million the a\n",
            "spoiler: These small circles are called \"Venus Holes\" and are formed on the lower back of women. However, some men can also have them and in that case they are called \"Apollo Holes\"\n",
            "648\n",
            "pred: Kin million million million Trump million of of a a\n",
            "spoiler: cupping\n",
            "649\n",
            "pred: Tt million million million of a a\n",
            "spoiler: This week security forces of the Palestinian Islamist movement Hamas have arrested dozens of Gaza residents for participating in protests against cuts to the electricity supply\n",
            "650\n",
            "pred: t million million million of the\n",
            "spoiler: It depends on how you define \"sleep,\" but trees do relax their branches at night, which might be a sign of snoozing, scientists said.\n",
            "651\n",
            "pred: TrumpC million Trump million million million of of to\n",
            "spoiler: \"there will always be exceptions.\"\n",
            "652\n",
            "pred: Tt million millions the a\n",
            "spoiler: Berlin\n",
            "653\n",
            "pred: KTCt million million millioner million million of of the\n",
            "spoiler: (SPOT.ph) For those of us who don’t work there, attending a conference or a meeting at the Asian Development Bank (ADB) in Ortigas requires an appointment in the system, a photo ID taken on the spot, and a bout with two X-ray machines. But the bank’s real treasures aren’t in some vault, they’re in plain sight—the ADB canteen offers scrumptious meals from several Asian cuisines at affordable prices, as well as a bakery that serves up all kinds of breads, pies, and pastries.\n",
            "654\n",
            "pred: Tt million million million the,\n",
            "spoiler: breathe in for four, hold for seven, and breathe out for eight. You must inhale through your nose and exhale through your mouth.\n",
            "655\n",
            "pred: KKBBKBCtt Trumps of,\n",
            "spoiler: college graduation\n",
            "656\n",
            "pred: KCt million million millions the a the\n",
            "spoiler: chopsticks holder\n",
            "657\n",
            "pred: Kin million million millions of the the\n",
            "spoiler: great organizational tools\n",
            "658\n",
            "pred: KTrumpCCinin million million million of,\n",
            "spoiler: While members of sports teams — and other male-dominated groups like fraternities and the military — are indeed more likely to commit gang rapes than the average person, there are few conclusive studies to show that athletes are more likely to commit sex crimes.\n",
            "659\n",
            "pred: T million million million of of the\n",
            "spoiler: losing weight. Trim your pubic hair. make sure your boner is at maximum capacity there are some pills you can take.\n",
            "660\n",
            "pred: Cin million million million of the the\n",
            "spoiler: nothing morally wrong with voting for a flawed candidate if you think he will do more good for the nation than his opponent\n",
            "661\n",
            "pred: BTt million million millions of the\n",
            "spoiler: Watermelon seeds are packed with nutrients that are beneficial and healthy for you.\n",
            "662\n",
            "pred: Tin million million millions of a\n",
            "spoiler: Ben Bernanke\n",
            "663\n",
            "pred: KL million million million Trump million millions the a a a\n",
            "spoiler: Daily Show companion series will air Monday-Thursday at 11:30 p.m. ET/PT, beginning this fall\n",
            "664\n",
            "pred: t million million million of a a\n",
            "spoiler: your activities could be monitored and private information collected\n",
            "665\n",
            "pred: KCt million million millions of the\n",
            "spoiler: sleep deprivation\n",
            "666\n",
            "pred: L million million millions of a a a\n",
            "spoiler: Feb. 24\n",
            "667\n",
            "pred: $ the\n",
            "spoiler: In very small amounts (say, a few teaspoons), and if the blood is free from pathogens (such as the many blood-borne diseases), blood might not harm you. Beyond that, watch out.\n",
            "668\n",
            "pred: C million million million of of the\n",
            "spoiler: talk about their money\n",
            "669\n",
            "pred: Tt million million millions of a\n",
            "spoiler: polymyalgia rheumatica\n",
            "670\n",
            "pred: KBACtts the\n",
            "spoiler: the Cubs poured it on in the 10th inning, scoring 2 runs to provide the cushion they needed. World Series M.V.P. Ben Zobrist hit a go-ahead double, and Miguel Montero followed up with an R.B.I. single to give the Cubs the insurance run they needed.\n",
            "671\n",
            "pred: C million million millions of a a a\n",
            "spoiler: Don’t Don’t! Just don’t. Scope Out Your Next Pokéstop Before Going There Leave Your Phone On, But On Silent And Hidden Get Out And Walk Around Anyway\n",
            "672\n",
            "pred: C million million million of of the\n",
            "spoiler: drink coaster\n",
            "673\n",
            "pred: t million million million to the a the\n",
            "spoiler: a pet\n",
            "674\n",
            "pred: KLLtt Trump million million millions the a\n",
            "spoiler: \"I live inside my own heart, Matt Damon.\"\n",
            "675\n",
            "pred: Aer million million million of a a a\n",
            "spoiler: She creates a complex, full-bodied character without any body at all\n",
            "676\n",
            "pred: TTTCttt million millions of a\n",
            "spoiler: descended from a single European\n",
            "677\n",
            "pred: KKKKBKKintererer Trump Trump million Trump million million millions of of the\n",
            "spoiler: Dawn Grace lost her son four years ago, and now she meets the man who received his heart in a transplant.\n",
            "678\n",
            "pred: Ttt million million millions of a\n",
            "spoiler: It will change your life because you will be received so much differently\n",
            "679\n",
            "pred: T million million million of of a\n",
            "spoiler: THE MUSIC THE MYSTERIOUS ORIGINS THE CAMARADERIE THE TRADITION THE RELIEF\n",
            "680\n",
            "pred: HillaryAKCCts of a\n",
            "spoiler: adding a teaspoon of coconut oil to boiling water with a half-cup of non-fortified white rice, letting it simmer for 20 to 40 minutes, then refrigerating it for 12 hours may reduce the number of calories your body takes in by 50 to 60 percent\n",
            "681\n",
            "pred: TTtss of a\n",
            "spoiler: 1. There are many theories about why cats knead their paws into you, but many researchers agree that they’re trying to return the affection that you’re showing them. They just don’t know that it actually hurts. 2. Sure, it’s kind of gross when cats bring dead animals into the house, but they do it because they consider you family – and they’re trying to teach you how to catch your own prey. 3. When cats rub their head against you, they’re marking you as one of their own with the concentrated scent glands in their cheeks and head. Congratulations, you’re family. 4. A cat’s unending stare can be a little bit disconcerting, but don’t fret. First of all, they’re not really staring. Their eyes are evolved to blink less frequently than ours. But more than that, a steady, soft gaze means that they feel safe and comfortable with you. 5. Like most creatures, cats don’t like to make themselves vulnerable. So if they roll over and present their belly to you, it’s a sign of unwavering trust.\n",
            "682\n",
            "pred: G million million million the of of a\n",
            "spoiler: spy camera\n",
            "683\n",
            "pred: LKererer million million millioner million Trump million the a of a\n",
            "spoiler: Watch Hill, Rhode Island Hamilton Island on Australia's Great Barrier Reef Cuba southern Italy town of Puglia Kenya\n",
            "684\n",
            "pred: KKKLmanmanman Trumps a\n",
            "spoiler: Louisville, Kentucky Where Opportunity Knox\n",
            "685\n",
            "pred: C Trump Trump million million million Trump million of the\n",
            "spoiler: racial\n",
            "686\n",
            "pred: Tt million million million Trump million million of of a a\n",
            "spoiler: \"Men’s shirts have more room, so it’s easier to slip them off over your head; women don’t have that option,\"\n",
            "687\n",
            "pred: C million million million Trump million of of a a\n",
            "spoiler: the Stark sigil was back in its rightful place\n",
            "688\n",
            "pred: Lererer millioner million million millioner Trump million the to\n",
            "spoiler: lime green\n",
            "689\n",
            "pred: VirginiaATTtts the\n",
            "spoiler: Aaron Judge\n",
            "690\n",
            "pred: Kt million million millions of a\n",
            "spoiler: 460. Coimbatore, India 461. Bhubaneswar, India 462. Cairo, Egypt 463. Mangalore, India 464. Thiruvananthapuram, India\n",
            "691\n",
            "pred: C million million millions of to a to\n",
            "spoiler: pizza\n",
            "692\n",
            "pred: Kt million million millions of the the\n",
            "spoiler: 1. Ecuador 2. Nicaragua 3. Thailand 4. Belize 5. Panama\n",
            "693\n",
            "pred: T million million million of the the\n",
            "spoiler: This new study is one of the first to suggest that not getting enough sleep is one way of making our gut bacteria unhealthy\n",
            "694\n",
            "pred: L million million millions of a a a\n",
            "spoiler: In the remake, Belle (Emma Watson) is more than a bold, beautiful bookworm and devoted daughter -- she’s also a teacher and inventor. It was never clear who Belle’s mother was or what happened to her. But thanks to the remake, those questions have finally been answered. the audience assumes the Beast was born like that, his attitude simply accompanying the crown. In the remake, we found out that’s not entirely the case. Lyrics to one of the movie’s most popular songs, \"Gaston,\" sounded a little different than the original. the new Beast is highly educated\n",
            "695\n",
            "pred: Lterer million million millioner million of to\n",
            "spoiler: Acoustic Take On 'Adore You'\n",
            "696\n",
            "pred: in million million million of the the\n",
            "spoiler: moral\n",
            "697\n",
            "pred: Kin million million million Trump million of of a a\n",
            "spoiler: a show whose tone shifted rapidly between horror, melodrama, action, and pure comedy. The name tells you all of that\n",
            "698\n",
            "pred: BAKBBDt million million millions of a a\n",
            "spoiler: The company wants you to change your last name to \"burger.\"\n",
            "699\n",
            "pred: KKCCt million million millions of a\n",
            "spoiler: Hot dogs\n",
            "700\n",
            "pred: tt million million million the the\n",
            "spoiler: Peter Facinelli 'Breaking Bad' Obsession\n",
            "701\n",
            "pred: Ln Trump Trump million million millions the a\n",
            "spoiler: Chris Pratt\n",
            "702\n",
            "pred: L million million millioner million million the of the the\n",
            "spoiler: Diagnostic medical sonographer compliance officer Operations research analyst\n",
            "703\n",
            "pred: T million million million of the the a the\n",
            "spoiler: CEO Lawyer Media Salesperson Surgeon\n",
            "704\n",
            "pred: KBBCCttt million millions of the\n",
            "spoiler: rosemary\n",
            "705\n",
            "pred: Lmanman Trump Trump million million million of a a a\n",
            "spoiler: 1,080\n",
            "706\n",
            "pred: T million million million of a a\n",
            "spoiler: Google\n",
            "707\n",
            "pred: TrumpTTCtt million million millions of of a a\n",
            "spoiler: 1. CHILI PEPPERS 2. WHOLE GRAINS AND VEGETABLES 3. BLACK PEPPER 4. GINGER 5. GARLIC\n",
            "708\n",
            "pred: C Trump Trump Trump million million million Trump million to a to\n",
            "spoiler: Intrepid\n",
            "709\n",
            "pred: Kt million million millioner million million to of a a a\n",
            "spoiler: in the MORNINGS\n",
            "710\n",
            "pred: TrumpBBCCC million million million Trump million million of of\n",
            "spoiler: University of California, Los Angeles\n",
            "711\n",
            "pred: BBATTttts the\n",
            "spoiler: busybodies\n",
            "712\n",
            "pred: ts the a\n",
            "spoiler: ‘Pigeon game mode’\n",
            "713\n",
            "pred: KLss the a\n",
            "spoiler: Chal Na Katrina\n",
            "714\n",
            "pred: LLerer million million million Trump million million the the a a a\n",
            "spoiler: February 24 at 9 p.m.\n",
            "715\n",
            "pred: KTCt million million millions of the\n",
            "spoiler: seven hours\n",
            "716\n",
            "pred: Lererer Trump million millioner million million million the a\n",
            "spoiler: By watching the trailer for \"G.B.F.\"\n",
            "717\n",
            "pred: KKCCCin million million millions the\n",
            "spoiler: Then get in touch with Muncie Animal Shelter because they want you to take their dogs for a walk while you’re out capturing Meowths and Rattatas.\n",
            "718\n",
            "pred: C Trump million million million Trump million of a a\n",
            "spoiler: pussy\n",
            "719\n",
            "pred: Cs of a a\n",
            "spoiler: Donald Trump already has the authorization to build a wall on the U.S.-Mexico border\n",
            "720\n",
            "pred: KKin million million million Trump million million of of of a a a the\n",
            "spoiler: new wars in the Middle East\n",
            "721\n",
            "pred: KBCCCt Trump Trump Trump million million million Trump million of a the\n",
            "spoiler: Lawrence Phillips\n",
            "722\n",
            "pred: Kin million million million Trump million the of a a a\n",
            "spoiler: Carrer Avinyo\n",
            "723\n",
            "pred: Tin million million million the of of a a\n",
            "spoiler: International travel firm Marriott Rewards has committed to sponsor Johnson’s mission to catch the remaining Pokémon in Europe, Japan and Australia.\n",
            "724\n",
            "pred: KBBTttts of a\n",
            "spoiler: Tawny Nelson the older man wanted was for me to never give up and keep being an amazing mom\n",
            "725\n",
            "pred: Kin million million million Trump million million to a of\n",
            "spoiler: I mean localized, renewable energy creates backup support systems on the grid\n",
            "726\n",
            "pred: KBKKBKCttt milliont millions of the\n",
            "spoiler: The technical term for this is superfetation. In humans, it's possible, but it's very uncommon.\n",
            "727\n",
            "pred: KC Trump Trump Trump million million million Trump million of a a\n",
            "spoiler: Colin Quinn\n",
            "728\n",
            "pred: ts the a\n",
            "spoiler: you can imagine an AI-based chatbot that truly comprehends intention behind language, a bot that you can have long-term discussions with about the news the way you would with a well-informed friend\n",
            "729\n",
            "pred: BBBCCCttts of a\n",
            "spoiler: fossil\n",
            "730\n",
            "pred: $ the\n",
            "spoiler: $1,427\n",
            "731\n",
            "pred: $ the\n",
            "spoiler: 1. Lunch Boxes all of a sudden, some things started coming out of the woodwork,\" Dixey added. \"You go back to the old adage of supply and demand. All of a sudden there's a glut of them 2. Beanie Babies the company that developed Beanie Babies abruptly announced that it would stop producing the toys 3. Baseball Cards America simply doesn't have the love affair with its pastime that we once did. 4. Commemorative Plates there was never any market 5. Comic Books many people have of lost riches in the paperbacks of their youth, the reality is far more mundane\n",
            "732\n",
            "pred: T million million million of of to\n",
            "spoiler: The evacuation of civilians and opposition fighters from eastern Aleppo have been suspended after rebels opened fire on a convoy at one of the crossing points of the rebel-held enclave, according to Syrian state TV.\n",
            "733\n",
            "pred: BBCCttts the\n",
            "spoiler: lost and hidden city of Skara Brae\n",
            "734\n",
            "pred: Lt million million millions the a a\n",
            "spoiler: The Jump Sir Bradley Wiggins Jade Jones Kadeena Cox Gareth Thomas Jason Robinson\n",
            "735\n",
            "pred: Kin million million million the of of a\n",
            "spoiler: firing a gun\n",
            "736\n",
            "pred: Cer Trump Trump Trump million million million of a\n",
            "spoiler: If Elliott can give this offense what DeMarco Murray provided in 2014, I can see Romo playing all 16 games.\n",
            "737\n",
            "pred: $ the\n",
            "spoiler: Pikaqiu\n",
            "738\n",
            "pred: KLermanererer million million millioner millions a a\n",
            "spoiler: Whitney Thompson Panache Lingerie's new Sculptresse line\n",
            "739\n",
            "pred: TCCt million million millions of the\n",
            "spoiler: greenhouse gases\n",
            "740\n",
            "pred: As the a a\n",
            "spoiler: an exiled Russian prince sued MGM in 1933 over the studio’s Rasputin biopic, claiming that the American production did not accurately depict Rasputin’s murder. And the prince ought to have known, having murdered him.\n",
            "741\n",
            "pred: GTTTttts of a\n",
            "spoiler: \"Cats have to be suspicious of the unknown: It could represent the danger of a snake or another predator,\"\n",
            "742\n",
            "pred: C million million millions of the\n",
            "spoiler: water\n",
            "743\n",
            "pred: Kin Trump Trump million million million of a a a\n",
            "spoiler: \"Captain Phillips\"\n",
            "744\n",
            "pred: KBTttt million million millions of of a\n",
            "spoiler: It looks like it's all going to end in complete disaster but just as the big cat goes to pounce, the man spins around and starts laughing and playing with him like they're bezzie mates.\n",
            "745\n",
            "pred: $ the\n",
            "spoiler: a humble rag\n",
            "746\n",
            "pred: KCt million million million Trump million millioner million million of of of to\n",
            "spoiler: Blue Lives Matter is a movement that counters BLM by encouraging citizens to support police officers that put their lives on the line every single day for citizens across the country\n",
            "747\n",
            "pred: KKin million million million of of the\n",
            "spoiler: Houston\n",
            "748\n",
            "pred: Kin million million million of the a a\n",
            "spoiler: In May, after almost a decade and three moves, they finally decided to open the box, and found two hand-written notes wrapped around some cash, as well as wine glasses and bath products. The note to Kathy told her to buy pizza (\"or something you both like\") and prepare a bath, and the note to Brandon said, \"Go get flowers and a bottle of wine.\"In the end, Kathy wrote that they had been enjoying this gift for almost a decade, even though the box – which she calls \"the greatest wedding gift of all\" – hadn't been opened.\"I realized that the tools for creating a strong, healthy marriage were never within that box – they were within us.\n",
            "749\n",
            "pred: $ the\n",
            "spoiler: You see, if you swipe a chip card instead of inserting it into slot, the merchant is responsible for covering any fraudulent charges — not the bank.\n",
            "750\n",
            "pred: Lter Trump Trump million million million Trump million the a a\n",
            "spoiler: Elle\n",
            "751\n",
            "pred: Tt million million millions of a\n",
            "spoiler: A few years ago, one of McQueen's students wore his jeans for 15 months straight without a single wash and then tested the level of bacteria on them. The student-teacher team was surprised to find that the unwashed jeans carried nearly the same amount of bacteria as those same pants after they had been washed and then worn for another 13 days.\n",
            "752\n",
            "pred: ts of a\n",
            "spoiler: Many tweeters noticed some strange behavior on Trump’s part; in addition to his seeming inability to control the volume of his speech, the man couldn’t stop sniffling (see the supercut below).\n",
            "753\n",
            "pred: KBCCttts of a\n",
            "spoiler: coconut oil pinch of turmeric\n",
            "754\n",
            "pred: Bt million million millions the a a a\n",
            "spoiler: It’s chock-full of nutrients and electrolytes like potassium, which not only keep you hydrated but also take longer for your body to process so you buy some time between bathroom breaks.\n",
            "755\n",
            "pred: At million million of the\n",
            "spoiler: laughing face emoji\n",
            "756\n",
            "pred: Kt million million million Trump million million to a a\n",
            "spoiler: Chris Crocker\n",
            "757\n",
            "pred: KCCtt million millions of a\n",
            "spoiler: Talk to a doctor before shoveling if you have a history of heart disease Stop immediately if you feel dizzy or tightness in the chest\n",
            "758\n",
            "pred: Kt million million millions of a\n",
            "spoiler: Newborn Smiling In Her Recently Deceased Father’s Gloves\n",
            "759\n",
            "pred: Kiner million million millions of a\n",
            "spoiler: number 7 with medium fries and a Coke\n",
            "760\n",
            "pred: CaliforniaC million million millions of a a a\n",
            "spoiler: Minnesota\n",
            "761\n",
            "pred: KCt million million million Trump million million of to of of\n",
            "spoiler: \"If you are dropping off your son’s forgotten lunch, books, homework, equipment, etc., please TURN AROUND and exit the building,\" the sign read. \"Your son will learn to problem-solve in your absence.\"\n",
            "762\n",
            "pred: C million million millions of a a a\n",
            "spoiler: 2 human players and 2 CPUs resulted in no lag. gets better if you plug in the charger while playing\n",
            "763\n",
            "pred: Lt million million million of of a\n",
            "spoiler: stealing her jewelry\n",
            "764\n",
            "pred: KCerer Trump Trump Trump million Trump Trump of a\n",
            "spoiler: Cecily Strong\n",
            "765\n",
            "pred: TBTTCttts the\n",
            "spoiler: 1. When it comes to food, you won’t just eat any old thing. 2. Because you’re partial to fancy hipster foods, like expensive artisanal bread. 3. However, you’ll happily skimp on other items to afford your luxurious foodie lifestyle. 4. You know exactly what you like to drink and won’t order anything else, even if it’s cheaper. 5. But no glasses? No problem!\n",
            "766\n",
            "pred: T million million million the of the\n",
            "spoiler: Arizona\n",
            "767\n",
            "pred: Kt million million millions the a a\n",
            "spoiler: airlines rarely follow those recommendations\n",
            "768\n",
            "pred: BCtt million millions of the\n",
            "spoiler: Springfield, Oregon\n",
            "769\n",
            "pred: BBttt million millions the a\n",
            "spoiler: Norah picked her head up\n",
            "770\n",
            "pred: t million million million of a a\n",
            "spoiler: Sony confirmed that the company was not against cross-console support and open to the idea of allowing PlayStation 4 players to play online against other consoles\n",
            "771\n",
            "pred: Kin Trump Trump million million millions of a\n",
            "spoiler: the canisters Bruce Wayne/Batman used have \"Pb\"\n",
            "772\n",
            "pred: KTt million million millions of the\n",
            "spoiler: \"post-micturition convulsion syndrome.\"\n",
            "773\n",
            "pred: TrumpC million million millions of a a a\n",
            "spoiler: The end is not near, as a ‘near Earth pass’ isn’t actually that near at all – in fact, the asteroid is going to go past the Earth 7.3 million miles away (that’s 30 times further away than the moon), reports the Daily Star.\n",
            "774\n",
            "pred: t million million million of of of a the\n",
            "spoiler: \"Where do you see yourself in five years?\"\n",
            "775\n",
            "pred: A million million of of of to the a a\n",
            "spoiler: Dumb British blond fucks 15 million people at once.\n",
            "776\n",
            "pred: LLL Trump Trump Trump million million millions a a\n",
            "spoiler: Pat Patterson\n",
            "777\n",
            "pred: CC Trump Trump Trump million million million Trump million of a a a\n",
            "spoiler: bullshitting\n",
            "778\n",
            "pred: KL Trump Trump Trump million million million Trump million of of a a a\n",
            "spoiler: Yvette Nicole Brown\n",
            "779\n",
            "pred: VirginiaKKKGKKCint millionin million million millions the a the a of\n",
            "spoiler: Florida\n",
            "780\n",
            "pred: Lin million million millions of of a a\n",
            "spoiler: not have enough happy memories to produce a Patronus\n",
            "781\n",
            "pred: C million Trump million million million of of a a\n",
            "spoiler: Democrat Jason Kander looks to be running very strongly in the exit polls.\n",
            "782\n",
            "pred: Lt million million million of a a\n",
            "spoiler: from 33 seconds to 44 minutes\n",
            "783\n",
            "pred: T million million million Trump million million of of a a a\n",
            "spoiler: he could not locate the guy sitting right in front of him.\n",
            "784\n",
            "pred: A million million the a of a a\n",
            "spoiler: monoclonal antibodies as scientific tools and for the treatment of diseases\n",
            "785\n",
            "pred: t million million million of the\n",
            "spoiler: BOOM – Portugal Meadows in the Mountains – Bulgaria Burning Man - Black Rock Desert Electric Forest - Rothbury, Michigan AfrikaBurn - Tankwa Karoo\n",
            "786\n",
            "pred: CCin million million millions a a a\n",
            "spoiler: white pepper\n",
            "787\n",
            "pred: Ler million million millioner million of the a a\n",
            "spoiler: anal sex\n",
            "788\n",
            "pred: Tt million million millions of the\n",
            "spoiler: earned much of their fortune making unintentionally laughable direct-to-video movies Mary-Kate was somehow linked with actor Heath Ledger's death successful fashion designers people began noticing just how different Mary-Kate looked compared to her younger self and her fraternal twin sister Fuller House, the Olsen twins were the only ones to decline Elizabeth is outgoing, vivacious, and comes off like an extremely well-adjusted regular person\n",
            "789\n",
            "pred: TTTCTtt million million millions of of the\n",
            "spoiler: Klefki Rotom Luvdisk Unknown (Unown) Vanilluxe\n",
            "790\n",
            "pred: Attss of a a\n",
            "spoiler: I don’t pretend to be an ordinary housewife People always assume you’re going to carry a grudge, but I don’t do that there are plenty of fish in the sea Disagreements are an inevitable part of any relationship I’ve always admitted that I’m ruled by my passions\n",
            "791\n",
            "pred: T Trump Trump Trump million Trump Trump of of a\n",
            "spoiler: Hillary Clinton\n",
            "792\n",
            "pred: KBTttt million million millions of the\n",
            "spoiler: This was clearly a crack house, and he was standing there with a dealers stash. People have gotten killed for less. Well guess what? The dealer comes back!\n",
            "793\n",
            "pred: A million million million the a a a\n",
            "spoiler: China is too quick to rebalance its services sector\n",
            "794\n",
            "pred: KKABBTtts the\n",
            "spoiler: the plant the dog was chewing on was deadly water hemlock\n",
            "795\n",
            "pred: BBttt million million millions a a\n",
            "spoiler: The video below shows the stunned cleaner initially refusing to accept the tip, before another hotel worker reassured her by saying, \"You deserve it.\"\n",
            "796\n",
            "pred: TrumpC Trump Trump million million million of a a\n",
            "spoiler: Christopher Suprun\n",
            "797\n",
            "pred: KKBtts of a\n",
            "spoiler: Rachel Crawley High fat vegan plant based diet (nothing processed) Real whole foods\n",
            "798\n",
            "pred: t million million of the\n",
            "spoiler: Julian Assange’s internet link has been intentionally severed by a state party\n",
            "799\n",
            "pred: KKel Trump Trump Trump million Trump Trumps the a\n",
            "spoiler: Richard Belzer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}"
      ],
      "metadata": {
        "id": "CiYl0ZYrh5ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, pred in enumerate(pred_str):\n",
        "  tmp = {}\n",
        "  tmp[\"roberta\"] = pred\n",
        "  tmp[\"spoiler\"] = label_str[i]\n",
        "  results[i] = tmp"
      ],
      "metadata": {
        "id": "su88AiPkh8kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmm2BbKfiK87",
        "outputId": "eb291f13-4941-48f5-9516-c57cb81a91bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {'roberta': 'Kin million million millions of the',\n",
              "  'spoiler': 'some of the plot elements are so disturbing that they are making him feel sick'},\n",
              " 1: {'roberta': 'KKin million million million Trump million of of a a',\n",
              "  'spoiler': '\"intentionally\" could transform a court case against Phoenix-area Sheriff Joe Arpaio from civil charges to a criminal prosecution'},\n",
              " 2: {'roberta': 't million million million to a the', 'spoiler': '20%'},\n",
              " 3: {'roberta': 'Lin million million million Trump million the a a a',\n",
              "  'spoiler': 'Alan Rickman & Rupert Grint CBGB'},\n",
              " 4: {'roberta': 'T million million million of the',\n",
              "  'spoiler': 'a man who swallowed a 64GB microSD card and then pooped it into a strainer'},\n",
              " 5: {'roberta': 'Kin million million million to the the the',\n",
              "  'spoiler': 'Sprite'},\n",
              " 6: {'roberta': 'BBCCttt million millions the',\n",
              "  'spoiler': 'Smoky Paprika-Baked Garbanzo Beans'},\n",
              " 7: {'roberta': 'KLLtt millions of a',\n",
              "  'spoiler': 'McGonagall was appointed as Dumbledore’s assistant in 1956, not as his replacement.'},\n",
              " 8: {'roberta': 'L million million millions of of a a a',\n",
              "  'spoiler': 'All the scenes are actually in the movie'},\n",
              " 9: {'roberta': 'Aerer million million millions a a',\n",
              "  'spoiler': '\"I had fake relationships, fake fights. I don\\'t care anymore, I can tell you.'},\n",
              " 10: {'roberta': 'Lt million million millions of the',\n",
              "  'spoiler': 'Elettra Wiedemann extra strength work, so weights, and quite a few planks for my core. My diet stayed pretty much the same, except I cut out sugar for the week of the shoot'},\n",
              " 11: {'roberta': 'Ct Trump Trump million million million Trump million of a a',\n",
              "  'spoiler': 'Anthony Bourdain'},\n",
              " 12: {'roberta': 'BCinererer million million millioner million of of the',\n",
              "  'spoiler': \"he'd eaten a peanut butter sandwich and wasn't aware of her peanut allergy\"},\n",
              " 13: {'roberta': 'LLererer Trump Trumper million million million Trump million millioner million of a a',\n",
              "  'spoiler': 'Marcia Clark Does she think Simpson really did it?'},\n",
              " 14: {'roberta': 'KKKererer million million million Trump million million of of a the',\n",
              "  'spoiler': 'Mitzi Gaynor Beverly Hills, California'},\n",
              " 15: {'roberta': 'Tt million million million Trump million million to of of of',\n",
              "  'spoiler': 'Dibrom'},\n",
              " 16: {'roberta': 'KKBACCts of a', 'spoiler': 'They don’t fart'},\n",
              " 17: {'roberta': 'Aterer million million million of a a',\n",
              "  'spoiler': 'kicked her and got into a fight with her current boyfriend'},\n",
              " 18: {'roberta': 'KBABTtts of a',\n",
              "  'spoiler': 'Dunham picked the boy up and took him to a Subway to get something to eat. He then took him to the Franklin Police Department.'},\n",
              " 19: {'roberta': 'KKATTtt million million millions of the',\n",
              "  'spoiler': 'Not only does Aubrey have cerebral palsy, but she was neglected and abused by her biological mother.'},\n",
              " 20: {'roberta': 't million million million of of of a',\n",
              "  'spoiler': 'starts later'},\n",
              " 21: {'roberta': 'KBC million million millions the a a a',\n",
              "  'spoiler': \"The bottom line: Unfortunately, there's not enough solid research out there on whether melatonin supplements are truly an effective and safe way to get to sleep.\"},\n",
              " 22: {'roberta': 'KLLerer Trump Trump Trump million million million Trump million of a',\n",
              "  'spoiler': \"Kevin Williamson said he didn't want to write it\"},\n",
              " 23: {'roberta': 'KTt million million millions of a',\n",
              "  'spoiler': 'Sophie and Riley were considered \"micro-preemies\" and suffered a slew of health issues, like chronic lung disease and holes in their hearts.'},\n",
              " 24: {'roberta': 'ts the', 'spoiler': 'bricking iPad Pros'},\n",
              " 25: {'roberta': 'TrumpC Trump Trump Trump million million Trump million of a a a',\n",
              "  'spoiler': 'Stace Nelson'},\n",
              " 26: {'roberta': '$ of.',\n",
              "  'spoiler': 'Paige Hathaway – £3.8million Chantel Zales – £3.6 million Ana Cheri – £2.4 million Abigail Ratchford – £2.3 million Claudia Alende – £2.1 million'},\n",
              " 27: {'roberta': 'Ct million million millioner million of of the',\n",
              "  'spoiler': 'Buss and Jackson announced that they were mutually ending their four-year engagement'},\n",
              " 28: {'roberta': 'Ler million million millioner million of a a',\n",
              "  'spoiler': 'In the video, an abstract, wooden sculpture clock appeared not to be working, so a male tourist decided to take matters into his own hands, pulling on weights and levers for more than 30 seconds before the clock came flying off the wall and into pieces on the floor.'},\n",
              " 29: {'roberta': 'KCttt million million millions of a',\n",
              "  'spoiler': 'reduced fat sour cream'},\n",
              " 30: {'roberta': 'KCin million million millions of of a',\n",
              "  'spoiler': 'Brooks would eventually return to a role at News Corp, few expected her to land at Storyful'},\n",
              " 31: {'roberta': 'Ktt million million million Trump million millions the a a a',\n",
              "  'spoiler': 'Edward Gorey'},\n",
              " 32: {'roberta': 'Tt million million million Trump million million of of a',\n",
              "  'spoiler': 'Rag & Bone'},\n",
              " 33: {'roberta': 'BBttt Trump Trump million million million the,',\n",
              "  'spoiler': 'pixie cut'},\n",
              " 34: {'roberta': 'K million million millions the a a a',\n",
              "  'spoiler': 'homemade oven cleaner place the shelves in a resealable plastic bag, spray with oven cleaner, seal the bag, then leave to soak old toothbrush is an essential oven-cleaning tool glass scraper is ideal for removing tough stains remove greasy build-up on the hood of your oven with oil'},\n",
              " 35: {'roberta': 'Kin million million million Trump million million to a a',\n",
              "  'spoiler': 'In February, when Rep. David Jolly introduced his quixotic plan to ban members of Congress from soliciting campaign contributions, the Florida Republican had only six co-sponsors.'},\n",
              " 36: {'roberta': 'BBBTttts the', 'spoiler': 'southern flying squirrel'},\n",
              " 37: {'roberta': 'Lt million million millions of a a',\n",
              "  'spoiler': '1. \"You can\\'t live your life according to maybes.\" 2. \"I wish there was a way to know you’re in the good old days before you’ve actually left them.\" 3. \"And what exactly do you think fairy tales are? They are a reminder that our lives will get better if we just hold on to hope. Your happy ending may not be what you expect, but that is what will make it so special.\" 4. \"To exist is to survive unfair choices.\" 5. \"I don’t want normal, and easy, and simple. I want painful, difficult, devastating, life-changing, extraordinary love. Don’t you want that, too?\"'},\n",
              " 38: {'roberta': 'At million million million the of of a a',\n",
              "  'spoiler': 'At this point, a large dog -- presumably Stella -- bounds into view, runs to one end of the fence and jumps right over it, almost without effort.'},\n",
              " 39: {'roberta': 'Kiner million million million Trump million million the a of a a',\n",
              "  'spoiler': \"Hope's antique cabinet\"},\n",
              " 40: {'roberta': 'Lin million million million of of the',\n",
              "  'spoiler': '1. Some alt-right guy will invent the make-up gun. 2. There will be a referendum on whether or not to deport illegal immigrants. 3. Greedy, corrupt energy firms will cause an environmental catastrophe, and a dome will be built over the contaminated site. 4. The chandelier in Elton John’s private jet will malfunction. 5. It will be made illegal to teach evolution in schools.'},\n",
              " 41: {'roberta': 'Tt million million million of a',\n",
              "  'spoiler': 'Some people are just better at sports than others'},\n",
              " 42: {'roberta': 'KC million million million Trump million million of of a a a',\n",
              "  'spoiler': 'August 6th'},\n",
              " 43: {'roberta': 'BBBttt million million millions the a',\n",
              "  'spoiler': 'perfectly average'},\n",
              " 44: {'roberta': 'Ats the,', 'spoiler': '\"but\"'},\n",
              " 45: {'roberta': 't million million the a',\n",
              "  'spoiler': 'On Tuesday morning, NASA will broadcast its first-ever rocket launch livestream in 360-degree video'},\n",
              " 46: {'roberta': 'Lt million million millioner million million the the a a',\n",
              "  'spoiler': 'Vince Carter Paul Pierce Dirk Nowitzki'},\n",
              " 47: {'roberta': 'BTttt million million millions of the a',\n",
              "  'spoiler': 'Baking soda bee stings'},\n",
              " 48: {'roberta': 'TrumpC Trump Trump Trump million million million Trump million to a to',\n",
              "  'spoiler': 'The Arizona Republic'},\n",
              " 49: {'roberta': 'Tt million million million of the',\n",
              "  'spoiler': 'I don’t think we should be starting to panic'},\n",
              " 50: {'roberta': 'KACCt million million millions of of of the a the the',\n",
              "  'spoiler': 'apocalyptic omen'},\n",
              " 51: {'roberta': 'Kin million million million the a a',\n",
              "  'spoiler': 'Rudner and her husband rented ATVs. After hitting a speed bump and falling off of the vehicle, her husband shattered his shoulder and Rudner rotated her hip'},\n",
              " 52: {'roberta': 'Ct million million millions of of a a',\n",
              "  'spoiler': 'rapamycin'},\n",
              " 53: {'roberta': 'Kin million million million of of a',\n",
              "  'spoiler': 'Santa Clara University'},\n",
              " 54: {'roberta': 'KCin million million million Trump million of to',\n",
              "  'spoiler': 'Oklahoma Wesleyan University'},\n",
              " 55: {'roberta': 'T million million million of the',\n",
              "  'spoiler': 'The sadness some men feel at this point may be due to the contrast between the joy of arousal and feeling like a superhero and the sensation of the feel-good hormones wearing off.'},\n",
              " 56: {'roberta': 'KTtt million million millions of a',\n",
              "  'spoiler': 'Edan Lepucki'},\n",
              " 57: {'roberta': 'Tt million million millioner million million the a a',\n",
              "  'spoiler': 'Tim Masthay'},\n",
              " 58: {'roberta': 'Lin million million million the a the',\n",
              "  'spoiler': 'But psychedelics may not be as dangerous and addictive as our society thinks.'},\n",
              " 59: {'roberta': 'KBBCtts of,', 'spoiler': \"World's Largest Pumpkin\"},\n",
              " 60: {'roberta': 'KKtsss the a', 'spoiler': '$25K'},\n",
              " 61: {'roberta': 'KTC million million millions of the',\n",
              "  'spoiler': 'If you find yourself so moved to store ice cream in your back pocket in Alabama, you’ll pay the price, and we don’t just mean a cold rear end.'},\n",
              " 62: {'roberta': '$ the',\n",
              "  'spoiler': 'close your eyes or simply stare at a fixed point Next, turn all focus to pulling in your breath Then, simply sigh Then start counting the breath in and out through the nose repeat this eight times'},\n",
              " 63: {'roberta': 'KC million million million Trump million million of of of a a a',\n",
              "  'spoiler': 'University of Tennessee'},\n",
              " 64: {'roberta': 'KKCerer million million million Trump million million of a a a',\n",
              "  'spoiler': 'Almario alleged physical abuse and uploading their sex video online'},\n",
              " 65: {'roberta': 'TLt million million million Trump million million of the of of of a a',\n",
              "  'spoiler': 'Crazy Frog'},\n",
              " 66: {'roberta': 'C million million million Trump million million to of the a a',\n",
              "  'spoiler': '\"Although astrologers seek to explain the natural world, they don\\'t usually attempt to critically evaluate whether those explanations are valid — and this is a key part of science.\"'},\n",
              " 67: {'roberta': 'BBTCttts the', 'spoiler': 'face'},\n",
              " 68: {'roberta': 'KLmanmanerer million million millioner millioner Trump million million of of a to',\n",
              "  'spoiler': 'Rachel Zoe Gives Birth To Her Second Baby Boy!'},\n",
              " 69: {'roberta': 'Kin million millions the a a a', 'spoiler': 'Amazon'},\n",
              " 70: {'roberta': 'BCt million million millions of the a a',\n",
              "  'spoiler': '1. Washing Your Face 2. Not Washing Your Feet 3. Not Washing or Replacing Your Loofah Regularly 4. Using a Soap Dish 5. Using Scented Soaps'},\n",
              " 71: {'roberta': 'Kinin million million million of of a a', 'spoiler': 'kuru'},\n",
              " 72: {'roberta': 'BC million million millions of the the a the',\n",
              "  'spoiler': 'Dieffenbachia'},\n",
              " 73: {'roberta': 'BBtt million million millions the the',\n",
              "  'spoiler': 'The answer to that is a definitive ... maybe.'},\n",
              " 74: {'roberta': 'At million million million of of the',\n",
              "  'spoiler': 'baby food'},\n",
              " 75: {'roberta': 'C million million millions the a a',\n",
              "  'spoiler': 'Michael and Kirk'},\n",
              " 76: {'roberta': 'Tt million million millions the a a',\n",
              "  'spoiler': '1,4-dioxane'},\n",
              " 77: {'roberta': 'TACCCtin million million millions the',\n",
              "  'spoiler': 'gut bacteria'},\n",
              " 78: {'roberta': 'At million million million of of of the',\n",
              "  'spoiler': 'from the last two weeks until the last breath, somewhere in that interval, people become too sick, or too drowsy, or too unconscious, to tell us what they’re experiencing Pre-death dreams were frequently so intense that the dream carried into wakefulness. First hunger and then thirst are lost. Speech is lost next, followed by vision. The last senses to go are usually hearing and touch. There are some kinds of conditions where pain is inevitable We generally believe that if your brain is really in a comatose kind of situation, or you’re not really responsive, that your perception—how you feel about things—may also be significantly decreased,'},\n",
              " 79: {'roberta': 'KCCin million million millions of the',\n",
              "  'spoiler': 'Wyoming'},\n",
              " 80: {'roberta': 'L million million million of of the', 'spoiler': 'Nvidia'},\n",
              " 81: {'roberta': 'Kin million million million of of of the',\n",
              "  'spoiler': \"it's all part of a massive effort to encourage people around the globe to donate blood\"},\n",
              " 82: {'roberta': 'LLer million million Trump million million millioner million Trump Trump million to a of a a',\n",
              "  'spoiler': 'Republican voters would enthusiastically welcome a black candidate, a Donell Trump — so long as he, too, championed nationalist, politically incorrect, anti-immigrant populism.'},\n",
              " 83: {'roberta': 'KKTKCCttt million millions of a',\n",
              "  'spoiler': '2.1 coffee drinks'},\n",
              " 84: {'roberta': 'KKBCTttts the',\n",
              "  'spoiler': 'Yuliana Avalos Match.com’s parent company, IAC (InterActiveCorp)'},\n",
              " 85: {'roberta': 'C million million million of of a a',\n",
              "  'spoiler': 'you’d still have a hard time arguing they were even the worst band on this stage'},\n",
              " 86: {'roberta': 'Lin million million million Trump million the a a',\n",
              "  'spoiler': 'Female staffers adopted a meeting strategy they called \"amplification\": When a woman made a key point, other women would repeat it, giving credit to its author.'},\n",
              " 87: {'roberta': 'C million million millions of of a',\n",
              "  'spoiler': 'In these headphones, 30% of the weight comes from four tiny metal parts that are there for the sole purpose of adding weight'},\n",
              " 88: {'roberta': 'Tt million million millions of the',\n",
              "  'spoiler': 'giant piece of cardboard'},\n",
              " 89: {'roberta': 'Ct million million millions of the',\n",
              "  'spoiler': 'Solomon’s recent research shows that people who are thinking about death are more likely to say they support him'},\n",
              " 90: {'roberta': 'Kin million million million of of a a',\n",
              "  'spoiler': 'Mark and Jacoba Tromp and their adult children Ella, Riana and Mitchell left their home in Silvan, east of Melbourne, taking cash but leaving behind bank cards and mobile phones. Hundreds of kangaroos were found dead in far west New South Wales this year from what was described as a \"mystery disease\". When a magnitude 7.8 earthquake struck near Christchurch in November, videos emerged that appeared to show the New Zealand sky lighting up in blue and green. This year it was reported that doctors were at a loss to explain the mysterious illness making a four-year-old Bangladeshi boy look like an old man. The FBI announced this year that one of America\\'s most baffling crimes — that of hijacker Dan \"DB Cooper\" who jumped out of a plane with a parachute and ransom money 45 years ago — looks set to remain an enigma.'},\n",
              " 91: {'roberta': 'Tt million million million of of a',\n",
              "  'spoiler': 'working on a cattle ranch as a utility farmer, picking up trash at the city dump in 100-plus degree weather telemarketing place'},\n",
              " 92: {'roberta': 'K million million millions of a a',\n",
              "  'spoiler': 'net neutrality'},\n",
              " 93: {'roberta': 'KK million million millions of of the',\n",
              "  'spoiler': 'sang his very own special rendition of Lukas Graham’s hit song \"7 Years.\"'},\n",
              " 94: {'roberta': 'Lin million million million the of of a',\n",
              "  'spoiler': 'Fall of the Resistance.'},\n",
              " 95: {'roberta': 'As the a', 'spoiler': '4.47 billion years ago'},\n",
              " 96: {'roberta': 'BBin million million millions of the',\n",
              "  'spoiler': 'Switzerland'},\n",
              " 97: {'roberta': '$ the', 'spoiler': 'They are soaked in a bath of chlorine'},\n",
              " 98: {'roberta': 'T million million millions of a a a',\n",
              "  'spoiler': 'preorder figure was 2 million'},\n",
              " 99: {'roberta': '$ the', 'spoiler': 'make-shift tap shoes'},\n",
              " 100: {'roberta': 'Kin million million million Trump million of of a a a',\n",
              "  'spoiler': 'Gov. Rick Snyder (R-Michigan)'},\n",
              " 101: {'roberta': 'Kin million million million Trump million of of a',\n",
              "  'spoiler': 'Because he believes he can monitor the situation as well — or better — from where he is'},\n",
              " 102: {'roberta': 'KKKGttt Trump million million million Trump millioner million million of of of',\n",
              "  'spoiler': 'Mr Thiel, who previously said New Zealand was a \"utopia\" and has invested heavily there, is just one of several US migrants who have realised what the country has to offer.'},\n",
              " 103: {'roberta': 'Ltmanmanman million millioner million million million the a',\n",
              "  'spoiler': 'two foundations'},\n",
              " 104: {'roberta': 'K million million million the of a a', 'spoiler': 'Boxed'},\n",
              " 105: {'roberta': 'Cer Trump million million million Trump million of a',\n",
              "  'spoiler': 'key objectives of MILO’s tour was the promotion of free speech on typically censorious American college campuses'},\n",
              " 106: {'roberta': 'C million million millioner million million of of of to',\n",
              "  'spoiler': 'toxic metal arsenic'},\n",
              " 107: {'roberta': 'A million million of the a a', 'spoiler': '$900 million'},\n",
              " 108: {'roberta': 'KKCtt million million millions the a',\n",
              "  'spoiler': '1. Mylan has patent protection that lasts through 2025 2. There’s no room for error when you’re treating anaphylaxis 3. It doesn’t take an auto-injector to get epinephrine into the body — but it sure helps 4. The regulatory process is slow and expensive 5. The public hasn’t spoken (loud enough)'},\n",
              " 109: {'roberta': 't million million million of of a',\n",
              "  'spoiler': 'he would keep the nation \"in suspense\" about whether he will accept the results'},\n",
              " 110: {'roberta': 'LLererer million million millioner millionerers to a',\n",
              "  'spoiler': '\"I like to do a cleanse once a year. But it\\'s not just juices. And I don\\'t do it for weight loss,\" \"I\\'ll probably get kicked out of our school for admitting this, but I let Apple stay home yesterday. I just needed to be with her. We went out to lunch, we went to the beauty salon, we were together,\" \"If I make my kids something delicious and we sit down to eat it, and I put my phone away and I really listen, that is such money in the bank,\" \"I was very strict for a while. I was macrobiotic for a couple of years, then I got pregnant and just ate ice cream. What I\\'ve learned is I want to enjoy my life, and food is a big part of it. I love to cook and feed people. I cook every day,\" \"Anything that can live on a shelf for years and you can open out of a bag and eat: no,\"'},\n",
              " 111: {'roberta': 'KCin million million millions of the',\n",
              "  'spoiler': \"1. You buy hemp milk not because it’s ironic, but because your digestion isn't what it used to be. 2. You no longer think spending a Friday night at home is lame 3. But binge-watching isn't limited to just sitcoms and dramas. 4. When your phone rings past 9 p.m. you automatically think it’s an emergency, because really, who calls someone past 9? 5. Your Halloween costumes have become significantly less revealing over the years\"},\n",
              " 112: {'roberta': 'KKPCCKCCCttt million million millions of the',\n",
              "  'spoiler': 'All you need to do is applying pressure on your sinuses between your eyes and nose and on the back of your head for 60 seconds.'},\n",
              " 113: {'roberta': 'KBBBCtt million million millions of a',\n",
              "  'spoiler': '15 Kristen Stewart 14 Aubrey Plaza 13 Mara Wilson 12 Colton Haynes 11 Reid Ewing'},\n",
              " 114: {'roberta': 'Lin million million millioner million million to of a a',\n",
              "  'spoiler': 'his sixth studio album, \"X,\" will hit stores on May 5'},\n",
              " 115: {'roberta': 'A million millions the a',\n",
              "  'spoiler': 'wants to achieve her own success'},\n",
              " 116: {'roberta': 'KBKKKBBKBKttts of,',\n",
              "  'spoiler': 'Chantelle major plastic surgery to repair her injuries'},\n",
              " 117: {'roberta': 'LLererer million million millioner million Trump million the a a',\n",
              "  'spoiler': 'Savannah Guthrie got married four months pregnant'},\n",
              " 118: {'roberta': 'Tts the a', 'spoiler': 'Jason Brian Rosenthal '},\n",
              " 119: {'roberta': 'L million million million the of the the the',\n",
              "  'spoiler': 'EasyAuto123.com'},\n",
              " 120: {'roberta': 'Lt million million millioner million million the a a',\n",
              "  'spoiler': 'On September 10, flash sale site Joss & Main will be selling actual props from the sets'},\n",
              " 121: {'roberta': 'TrumpKTATTtt million million millions of the',\n",
              "  'spoiler': 'Guyana'},\n",
              " 122: {'roberta': 'Kt million million million Trump million millions a a a',\n",
              "  'spoiler': 'Gilbert Arenas'},\n",
              " 123: {'roberta': 'KLererer million million million Trump million millions to of a a a',\n",
              "  'spoiler': 'Joe McGrath For Nathan’s birthday'},\n",
              " 124: {'roberta': 'KKt million million million Trump million million of of a a a',\n",
              "  'spoiler': '1. \"Never-Before-Seen Photos Behind the Scenes at the Women’s March\" 2. \"Honest Photos of Motherhood Challenge What We Think of as Natural\" 3. \"This Is What International Women’s Day Looked Like Around The World\" 4. \"A Crude Awakening\" 5. \"8 Portraits of Refugees That Will Help You Understand the Refugee Experience\"'},\n",
              " 125: {'roberta': 'KKKKBKKL million million millions of the',\n",
              "  'spoiler': 'parent who was an immigrant or an educator parents were involved in political activism of some kind conflict-heavy family life, but that conflict was rarely between the parents strong awareness of mortality as children they grew up with much more freedom than their friends'},\n",
              " 126: {'roberta': 'C Trump million million million of to',\n",
              "  'spoiler': 'Alex Castellanos'},\n",
              " 127: {'roberta': 'KKBCtt million million millions of a',\n",
              "  'spoiler': 'bring the work to them'},\n",
              " 128: {'roberta': 'KKAAKBKtt million million millions of the',\n",
              "  'spoiler': 'heroin local news reports in Denver, Chicago, Atlanta, Seattle, Orange County, Syracuse, and a growing number of other areas told the tales of teens dying from heroin'},\n",
              " 129: {'roberta': 't million million of the', 'spoiler': '\"Gravity\"'},\n",
              " 130: {'roberta': 'Ktt million million millions of a',\n",
              "  'spoiler': 'Naturally Remove Yucky Corns And Calluses'},\n",
              " 131: {'roberta': 'C million million million Trump million million to of a a a',\n",
              "  'spoiler': 'For some drivers, their app-inspired shortcut became a permanent route.'},\n",
              " 132: {'roberta': 'KTt million million millions of a', 'spoiler': 'Wisconsin'},\n",
              " 133: {'roberta': 'BBttss of a',\n",
              "  'spoiler': 'pieces of rubber material embedded in the meat'},\n",
              " 134: {'roberta': 'VirginiaKKKGKKLttts of the',\n",
              "  'spoiler': 'full moon on the same day as the summer solstice'},\n",
              " 135: {'roberta': 'KKCman million million million Trump million million of of a a',\n",
              "  'spoiler': 'Isla Fisher'},\n",
              " 136: {'roberta': 'K million million millions of of of to to a a',\n",
              "  'spoiler': 'the greatest potential health drawback of consuming bottled sparkling water is missing out on the benefit of fluoride when you drink it instead of fluoridated tap water'},\n",
              " 137: {'roberta': 'KLererer million million millioner Trump million millions a a',\n",
              "  'spoiler': 'never received a dime'},\n",
              " 138: {'roberta': 'As the a', 'spoiler': '170'},\n",
              " 139: {'roberta': 'Kin million million millioner million million to a a',\n",
              "  'spoiler': 'Her ticket out of debt and into financial freedom has been her blog, Making Sense of Cents, where she offers tips on saving and making money   and publishes income reports.'},\n",
              " 140: {'roberta': 'VirginiaKKKACCTttt million million millions the,',\n",
              "  'spoiler': 'taking a banned or illegal substance in order to improve performance'},\n",
              " 141: {'roberta': 'GLtt million million millions the a',\n",
              "  'spoiler': 'move all the perishable older goods to the front and place the newer groceries behind them.'},\n",
              " 142: {'roberta': 'KKin million million millions the the',\n",
              "  'spoiler': 'Atlanta'},\n",
              " 143: {'roberta': 'BBBCtt million million millions of the',\n",
              "  'spoiler': 'The woman strongly believed that PM Narendra Modi is responsible for poverty and other terrible conditions in our country. Whereas the husband felt otherwise. He strongly believed that NaMo is doing his best to fix everything, gradually.'},\n",
              " 144: {'roberta': 'Liner million million millioner million of the a',\n",
              "  'spoiler': 'Dahlesque Golden ticket Human bean Oompa Loompa Scrumdiddlyumptious'},\n",
              " 145: {'roberta': 'Kin million million million Trump million of of of the',\n",
              "  'spoiler': 'arrested in a 2012 marijuana trafficking bust'},\n",
              " 146: {'roberta': 'KKKKBKBBtt million million millions of of of the',\n",
              "  'spoiler': 'Peloponnese'},\n",
              " 147: {'roberta': 'KCt million million millions of of of a a',\n",
              "  'spoiler': \"If you've booked a holiday, but are yet to exchange your pounds for a foreign currency, you're going to be a victim of Britain leaving the EU I'm afraid. And your flights look like they're going to go up in price, too.\"},\n",
              " 148: {'roberta': 'KKKLinin million million millions of the',\n",
              "  'spoiler': 'Israel'},\n",
              " 149: {'roberta': 'TCin Trump Trump million million million of of a a',\n",
              "  'spoiler': 'Chiwetel Ejiofor'},\n",
              " 150: {'roberta': 'KKKCttt million million millions of the',\n",
              "  'spoiler': 'Lviv, Ukraine'},\n",
              " 151: {'roberta': 'Kin million million million Trump million of of a a a',\n",
              "  'spoiler': 'is still wearing her updo braids TWO DAYS after debuting the glamorous look at the Met Gala'},\n",
              " 152: {'roberta': 'KBBCt million million millions the',\n",
              "  'spoiler': 'Every rupee spent in hotels and restaurants, shops, taxis or temples, is helping Kathmandu back on to its feet'},\n",
              " 153: {'roberta': 'KKmaner million million million Trump million of a a',\n",
              "  'spoiler': '\"The Wolf of Wall Street\" features a lot of swear words. According to Vulture, there are 569 variations on the f-word alone'},\n",
              " 154: {'roberta': 'KBBCCttts the', 'spoiler': 'she joins in'},\n",
              " 155: {'roberta': 'KTttt million millions of a',\n",
              "  'spoiler': 'Mobile web (m.reddit.com) is undergoing a major overhaul new frontpage algorithm we tested adding affiliate tags to e-commerce links, which we ended up turning off we announced Promoted User Posts sponsored headlines'},\n",
              " 156: {'roberta': 'KKC million million millions a a a',\n",
              "  'spoiler': 'Guinness World Record: the most peanut butter and jelly sandwiches made in an hour 49100'},\n",
              " 157: {'roberta': 'KBCin million million millions of the',\n",
              "  'spoiler': 'La Junta'},\n",
              " 158: {'roberta': 'ts the a', 'spoiler': 'June 12 at 5 PM ET'},\n",
              " 159: {'roberta': 'Kin million million million Trump million million the of a a',\n",
              "  'spoiler': 'Daisy Ridley endometriosis'},\n",
              " 160: {'roberta': 'KLiner million million millions a a',\n",
              "  'spoiler': 'on \"Do What U Want\"'},\n",
              " 161: {'roberta': 'KBACttt million millions of of the',\n",
              "  'spoiler': '1. Starbucks prevents you from losing it on your kids. 2. You definitely have no energy to clean a mess without it. 3. And the more you have, the happier you get. 4. If you don’t have caffeine, you look like your kids do every morning.'},\n",
              " 162: {'roberta': 't million million million of of a',\n",
              "  'spoiler': 'vote Clinton.'},\n",
              " 163: {'roberta': 'Kt million million millions the a a',\n",
              "  'spoiler': '$1,000 per haircut'},\n",
              " 164: {'roberta': 'T million million million of a a',\n",
              "  'spoiler': \"If we're being honest, this probably won't be the most exciting iPhone release we've seen\"},\n",
              " 165: {'roberta': 'BLLterermanerer million million millioner millioner Trump million million the a a',\n",
              "  'spoiler': 'Wiz Khalifa'},\n",
              " 166: {'roberta': 'Lterer millioner million million millions a',\n",
              "  'spoiler': 'Long And Healthy'},\n",
              " 167: {'roberta': 'KKCCter million million million of and',\n",
              "  'spoiler': 'The school created two \"all-gender,\" single-stall restrooms in February. Students can still choose what works best for them.'},\n",
              " 168: {'roberta': 'KKKCt million million millions a a a',\n",
              "  'spoiler': 'fastest video to break 1 billion views'},\n",
              " 169: {'roberta': 'LLererer million million millioner millions a a',\n",
              "  'spoiler': 'Fashion Advice'},\n",
              " 170: {'roberta': 'A million million million of of a a a',\n",
              "  'spoiler': 'The Fed is expected to hike rates The Netherlands is going to the polls Oil is making a comeback An early Twitter investor hates the stock'},\n",
              " 171: {'roberta': 'Trump Trump million million million Trump million of the the',\n",
              "  'spoiler': 'The Overton Window'},\n",
              " 172: {'roberta': 'C million million millions of the',\n",
              "  'spoiler': 'Massachusetts'},\n",
              " 173: {'roberta': 'C million million million the a a', 'spoiler': 'VLC'},\n",
              " 174: {'roberta': 'Kin Trump Trump Trump million million million of to',\n",
              "  'spoiler': 'Shevchenko’s coach, Pavel Fedotov, was armed with a gun and decided to take matters into his own hands. Firing at the group of men, Fedotov allegedly killed one of the men. He was then shot himself in the stomach'},\n",
              " 175: {'roberta': 'CC million million million Trump million million of of of a a a',\n",
              "  'spoiler': 'Liechtenstein'},\n",
              " 176: {'roberta': 'Ltt million million millions of the',\n",
              "  'spoiler': 'concealer and foundation, should be replaced every six months eyeliner and mascara, should be replaced every three months shampoo and conditioner are generally good for a year'},\n",
              " 177: {'roberta': 'Ktss of of a',\n",
              "  'spoiler': 'marriage proposals vulgar gags about marriage being asked for money'},\n",
              " 178: {'roberta': 'KKKKTKCKCCCinininererer million million millioner millioner Trump million million Trump millionerer Trumper million Trump Trump million',\n",
              "  'spoiler': 'Sultan Moulay Ismaïl of Morocco have sex an average of 0.83 to 1.43 times per day in order to father 1,171 children in 32 years'},\n",
              " 179: {'roberta': 'KBCCCBCinininerinin million millions a',\n",
              "  'spoiler': 'restaurant’s manager and co-owner'},\n",
              " 180: {'roberta': 'Lt million million million the of', 'spoiler': 'sweaters'},\n",
              " 181: {'roberta': 'in million million million to the the',\n",
              "  'spoiler': '1. Dr. Lakisha Jenkins, Master Herbalist 2. Rachel K. Gillette, Attorney 3. Lynne Lyman, California State Director for the Drug Policy Alliance 4. Charlo Greene, Founder of Alaska Cannabis Club 5. Jessica Peters, President of Moxie Meds'},\n",
              " 182: {'roberta': 'C million million millions of a a',\n",
              "  'spoiler': \"the reactor is a region of natural uranium within the Earth's crust, found in Okla, Gabon. Uranium is naturally radioactive, and the conditions in this rocky area happened to be just right to cook up some nuclear reactions.\"},\n",
              " 183: {'roberta': 't million million million to the',\n",
              "  'spoiler': 'These services offer a way for us to build monuments to ourselves. They let us combine our curated memories and experiences with all our messy data — the texts and tweets and \"Likes\" and photos — and organize it into something that we hope will let our loved ones and future generations actually understand something about who we were.'},\n",
              " 184: {'roberta': 'TrumpCCCin million million million Trump million million of of of',\n",
              "  'spoiler': 'Black Women'},\n",
              " 185: {'roberta': 'GTCCCtts of a',\n",
              "  'spoiler': 'Marriott Rewards has offered to sponsor Johnson to go to Europe, Japan and Australia to catch the regionally based Pokémon and finish his collection'},\n",
              " 186: {'roberta': 'KKKBBKKTin millions a',\n",
              "  'spoiler': 'Abraj Kudai Mecca, Saudi Arabia'},\n",
              " 187: {'roberta': 't million million million to the the',\n",
              "  'spoiler': 'Not only is the weed bad, but Sisley claims it was moldy as well'},\n",
              " 188: {'roberta': 'Kin million million million Trump million of a a a',\n",
              "  'spoiler': \"Holt is a registered Republican Holt is the first black presidential debate moderator since 1992 He's not really into the Twitter He moderated a Democratic primary debate Holt has two honorary doctorates, but no bachelor's degree\"},\n",
              " 189: {'roberta': '$ to,',\n",
              "  'spoiler': '\"OMG same! I keep my closet perfectly color-coded!\"'},\n",
              " 190: {'roberta': 'At million millions the of of a a',\n",
              "  'spoiler': 'inFlow Carta Rightcontrol Lite Odoo ABC Inventory'},\n",
              " 191: {'roberta': 'L million million million of of of a a',\n",
              "  'spoiler': 'Tomb Raider'},\n",
              " 192: {'roberta': '$ the',\n",
              "  'spoiler': 'swipe left on Tinder and choose the old-fashioned way'},\n",
              " 193: {'roberta': 'Kt Trump million million million Trump million of a',\n",
              "  'spoiler': 'Take Walks on Beaches'},\n",
              " 194: {'roberta': 'KBCt million million millions of the', 'spoiler': \"ma'am\"},\n",
              " 195: {'roberta': 'LLererer million million millioner millioner Trump million to a the',\n",
              "  'spoiler': '\"I think she\\'s fabulous,\"'},\n",
              " 196: {'roberta': 'BCts million millions of a',\n",
              "  'spoiler': 'its next Mars rover, dubbed Mars 2020, has cleared a massive hurdle that could see it speeding toward the Red Planet in just a few years.'},\n",
              " 197: {'roberta': 'At million millions the a',\n",
              "  'spoiler': 'largely due to three interrelated factors: European style trends, a government-mandated push for fuel economy, and new technologies that allowed manufacturers to more easily design and create curved shapes.'},\n",
              " 198: {'roberta': 'Kin million million millions of of the a a',\n",
              "  'spoiler': 'Avoid becoming a busy fool: It is very easy to become very busy doing things that are a huge drain on time but have little or no end value Invest wisely: Don’t invest in anything unless you are prepared to put at least 10 per cent of your net worth into it Save, save, save Know the value of your time'},\n",
              " 199: {'roberta': 'Tt million million million the of a',\n",
              "  'spoiler': 'pretending that they are men'},\n",
              " 200: {'roberta': 'VirginiaATTttts of a',\n",
              "  'spoiler': '1. Cleopatra lived closer in time to the invention of Snapchat than to the construction of the Great Pyramid at Giza. 2. Also, the Great Pyramid was older to the Romans than the Romans are to us. 3. Oh, and the Stegosaurus was older to the Tyrannosaurus rex than the T. rex is to us. 4. The oldest living person was born closer to the signing of the US Constitution than to today. 5. The 10th president of the United States has two grandchildren who are alive today.'},\n",
              " 201: {'roberta': 'Tt million million millioner million million the the',\n",
              "  'spoiler': 'altruism'},\n",
              " 202: {'roberta': 'VirginiaKTCCCtt million million million Trump million millions a',\n",
              "  'spoiler': 'preserving ecosystems to be a just cause, then killing could be justified'},\n",
              " 203: {'roberta': 'KCt million million millions the the the',\n",
              "  'spoiler': \"misses 'hooking up with strangers' treats her 'like a sex toy' she's 'in love with another man' his wife is simply 'taking advantage of her' spouse 'no longer shows any love'\"},\n",
              " 204: {'roberta': 'KLin million million million the of the a a',\n",
              "  'spoiler': 'dying his hair sky blue'},\n",
              " 205: {'roberta': 'TTt million million million of of a a',\n",
              "  'spoiler': 'And all of this has got Vincent Kompany very excited. VERY excited. The Manchester City defender said that the standard of his performance was so good it puts him up there with Ronaldo and Messi.'},\n",
              " 206: {'roberta': 't million million million to the a a',\n",
              "  'spoiler': 'MasterCard’s new Decision Intelligence software pulls in data, sometimes hundreds of pieces of data, about a specific transaction at the moment a customer swipes his or her credit card. The system then combines all of that information to yield a score indicating how likely the transaction is to be fraudulent.'},\n",
              " 207: {'roberta': 'Cts the a', 'spoiler': 'when we sleep, our blinking stops'},\n",
              " 208: {'roberta': 'KBBBCttts the',\n",
              "  'spoiler': 'embracing her new look and wants to use her new platform to help others'},\n",
              " 209: {'roberta': 't million million million of the',\n",
              "  'spoiler': '1. Cold-Weather Clothing 2. Back-to-School Supplies 3. Theme Park Tickets 4. Patio Furniture 5. Televisions'},\n",
              " 210: {'roberta': '1The..',\n",
              "  'spoiler': 'men are so much more likely than women to work those long hours'},\n",
              " 211: {'roberta': 'Tt million million millions of a',\n",
              "  'spoiler': 'underarm deodorant'},\n",
              " 212: {'roberta': 'VirginiaTATTttts the',\n",
              "  'spoiler': \"Toronto resident, Hayley Greenberg Jingee's previous owners had taught him how to use the toilet instead of a litter box\"},\n",
              " 213: {'roberta': 'Ler million million Trump million million of of to',\n",
              "  'spoiler': 'Hispanic voters'},\n",
              " 214: {'roberta': 'Btt million million millioner million million of of a a',\n",
              "  'spoiler': 'acted in self-defense and buried and burned the bodies in a state of panic'},\n",
              " 215: {'roberta': 'ts the a',\n",
              "  'spoiler': \"The photograph in question was taken on US President Donald Trump's first day in the Oval Office on Monday, when he signed a ban on federal money going to international groups that perform or provide information on abortions\"},\n",
              " 216: {'roberta': 'BTTtts of the',\n",
              "  'spoiler': 'You can get infections in your fingers from the bacteria in your saliva, not to mention the wounds around your nails. In extreme situations, you can even get irreversible nerve damage.'},\n",
              " 217: {'roberta': 'Cin million million millions of of to',\n",
              "  'spoiler': 'botulinum toxin, \"type H\"'},\n",
              " 218: {'roberta': 'LLerer Trump million million millioner million million the a a',\n",
              "  'spoiler': '\"When you meet someone they don\\'t have a number on their forehead.\"'},\n",
              " 219: {'roberta': 'L million million million the of the a a a',\n",
              "  'spoiler': 'She says that summer is better because the heat makes it feel that much more steamy, summer bodies are great, and you can get fully nude without having to keep any layers on because of the cold.'},\n",
              " 220: {'roberta': 'C Trump million million million Trump million of a a',\n",
              "  'spoiler': 'Arthur Brooks'},\n",
              " 221: {'roberta': 'KKBTTttt million millions the,',\n",
              "  'spoiler': '1. Get better at a skill that you can share with others. 2. If you don’t want to invest a lot of time in coordinating contrasting colors in your home or closet, just stick with one. 3. Take care of your skin because if it glows, people will pay attention to you. 4. Find something you love about yourself and show it off like every atom in the universe needs to see it. 5. If you’re having people over, stock up on coffee table books that actually interest you.'},\n",
              " 222: {'roberta': 'A million million of a a',\n",
              "  'spoiler': 'Now Goel is forming a business to bring the chatbot to the wider world of education.'},\n",
              " 223: {'roberta': 'Lerer million million millioner million Trump million the the',\n",
              "  'spoiler': 'in a flash, the ring was gone, falling into the cold waters below'},\n",
              " 224: {'roberta': 'Tin million million millions of a',\n",
              "  'spoiler': 'Antenna Bands'},\n",
              " 225: {'roberta': 'BTBTTttt milliont millions of a',\n",
              "  'spoiler': \"Martin says this is perfect for modeling agencies, high-profile clients who don't want to give away personal information as they book appointments, or people who find excuses not to do things they don't want to do (the rest of the world).\"},\n",
              " 226: {'roberta': 'Ct million million million of of of a',\n",
              "  'spoiler': '1. Jessica 2. Amy 3. Jade 4. Rebecca 5. Charlotte'},\n",
              " 227: {'roberta': 'Kt Trump million million million Trump million the a a a',\n",
              "  'spoiler': 'grey tee and blue jeans'},\n",
              " 228: {'roberta': 'Lss the a',\n",
              "  'spoiler': 'Kit Harington admitted during filming he may have got a bit too in character accidentally hitting Iwan not once but twice while miming his punches'},\n",
              " 229: {'roberta': 'A million the,', 'spoiler': '£12.99'},\n",
              " 230: {'roberta': 'KKKKCKCinin million million millioner million million Trump million million the a of',\n",
              "  'spoiler': 'Rob Mrowka, a scientist with the Center for Biological diversity, said the Bundy cattle are essentially \"feral,\" roaming at will over hundreds of thousands of acres — including far outside the range where long ago they were permitted. (Bundy stopped paying his grazing fees in 1993, meaning the cattle were technically trespassing anywhere on public land after that point.)'},\n",
              " 231: {'roberta': 'Cin million million million of a a',\n",
              "  'spoiler': \"US stocks are at all-time highs. The ECB meets on Thursday. China's economy is bouncing back. Japan's Final Q3 GDP was revised sharply lower. Banks are moving to Paris.\"},\n",
              " 232: {'roberta': 'KLmaner million million million Trump million million the of of a a a',\n",
              "  'spoiler': 'Andy Mizrahi being handsome, charismatic and funny'},\n",
              " 233: {'roberta': 'L million million millions and a a', 'spoiler': 'Ripcord'},\n",
              " 234: {'roberta': 'At million million million of of a a',\n",
              "  'spoiler': 'Guangyuan, Shanghai'},\n",
              " 235: {'roberta': 'A million million million of of of to of to',\n",
              "  'spoiler': 'Resistance to antibiotics is growing at such an alarming rate that they risk losing effectiveness entirely'},\n",
              " 236: {'roberta': 'Cer million million million Trump million of a a a',\n",
              "  'spoiler': 'pancakes'},\n",
              " 237: {'roberta': 'Ktss the a', 'spoiler': '$1,230'},\n",
              " 238: {'roberta': 'BBttss the,', 'spoiler': 'Salman'},\n",
              " 239: {'roberta': 'L million million million Trump million million of of a a',\n",
              "  'spoiler': 'Senator Daniel Inouye of Hawaii, a World War II veteran, repeatedly introduced resolutions to return Memorial Day to its traditional date—and, by proxy, its original intent'},\n",
              " 240: {'roberta': 'C million million millions of of a',\n",
              "  'spoiler': 'Men are paid more partly because they’re much more likely to ask for it. makes offers based on what a job is worth Make Work Easier for Mothers Build More Flexible Workplaces Change the Law'},\n",
              " 241: {'roberta': 'L million million millioner million million to the to a',\n",
              "  'spoiler': 'It’s clear there is no one-size-fits-all approach for publishers, even if they are in the same category — optimal times vary depending on the location of the audience. One explanation for this could be that daily routines and behaviors vary between countries and cultures, which therefore impacts media consumption patterns.'},\n",
              " 242: {'roberta': 'Ttt million million millions of the',\n",
              "  'spoiler': 'Pantone 448 C'},\n",
              " 243: {'roberta': 'Tt million million million Trump million million of of a a',\n",
              "  'spoiler': 'Ellis Rabb'},\n",
              " 244: {'roberta': 'Kin million million million Trump million million of of a a a',\n",
              "  'spoiler': 'For that we could compare deaths per film to standardize among the actors. If we do that, Sean Bean is still at the top of the list with 0.32 deaths/film, tied with Bela Lugosi'},\n",
              " 245: {'roberta': 'At million million million the of the a a',\n",
              "  'spoiler': 'Ivory & Deene'},\n",
              " 246: {'roberta': 'KG million million millioner million million the the a the',\n",
              "  'spoiler': 'Day-trippers or tourists at less than Insta-worthy lodgings can head to nearby luxury hotels and indulge in their amenities while saving on the eyebrow-raising nightly rates'},\n",
              " 247: {'roberta': 'At million million million the a a',\n",
              "  'spoiler': 'Derrick Taylor he was presented the car'},\n",
              " 248: {'roberta': 'L million million million to the a a a the',\n",
              "  'spoiler': 'ballet studio'},\n",
              " 249: {'roberta': 'A million million million to the a the',\n",
              "  'spoiler': '1. Emphasize productivity. 2. Evaluate potential ROI. 3. Investigate key features.'},\n",
              " 250: {'roberta': 'KCt million million millions the a a',\n",
              "  'spoiler': 'one house has been encased in ice'},\n",
              " 251: {'roberta': 'Lin million million millions of of the',\n",
              "  'spoiler': 'Sarah Brown did not expect to get pregnant the year before'},\n",
              " 252: {'roberta': 'At million million the a the',\n",
              "  'spoiler': 'One user noted the similarity to a famous drawing that appears to be of both a duck and a rabbit'},\n",
              " 253: {'roberta': 'Kiner million million million Trump million million the a a',\n",
              "  'spoiler': 'Ryan Tannehill'},\n",
              " 254: {'roberta': 'Ler million million Trump million million million of of a a a',\n",
              "  'spoiler': '\"Yeah, I mean, I’d do it,\"'},\n",
              " 255: {'roberta': 'A million million million of the a a a',\n",
              "  'spoiler': 'the scientific miracle that allowed Glenn to become the first American to orbit the earth'},\n",
              " 256: {'roberta': 'KKC million million millions the a a', 'spoiler': '$564'},\n",
              " 257: {'roberta': 'Kt million million million the of a a the',\n",
              "  'spoiler': 'living a life of goodness and peace and joy in the Holy Spirit'},\n",
              " 258: {'roberta': 'Ttt million million millions of the',\n",
              "  'spoiler': 'realistic optimists'},\n",
              " 259: {'roberta': 'Kiner million million millioner million of a a',\n",
              "  'spoiler': 'Rep. Kevin Elmer'},\n",
              " 260: {'roberta': 'KLin million million million the of of a',\n",
              "  'spoiler': 'Dolores Umbridge was arrested, interrogated and imprisoned for crimes against Muggleborns. After his death, Voldemort is forced to exist in the stunted infant-like form that Harry sees in the Kings Cross-like Limbo. The Resurrection Stone is now buried in the Forbidden Forest after being pressed into the ground by a centaurs hoof. It has not been re-discovered. Harry lost the ability to speak Parseltongue when Voldemorts soul fragment was destroyed. Viktor Krum fell in love with a woman back home in Bulgaria and presumably lived happily ever after.'},\n",
              " 261: {'roberta': 'ts the a',\n",
              "  'spoiler': 'Hydraulic Presses Drilling Hammers Angle Grinders Russian Men with Chairs'},\n",
              " 262: {'roberta': 'Tt million million millions of a',\n",
              "  'spoiler': 'spray-on nail polishes'},\n",
              " 263: {'roberta': 'C million million million of of of a the',\n",
              "  'spoiler': 'The big pause The eye dart The lost breath Overcompensating The poker face'},\n",
              " 264: {'roberta': 'CC Trump million million million Trump millions of the',\n",
              "  'spoiler': '6) Mike Tyson 5) Loretta Lynn 4) Joe Arpaio 3) Wayne Newton AKA Mr Las Vegas 2) Phil Ruffin 1 + Bonus) Sarah Palin'},\n",
              " 265: {'roberta': 'BCCttt million million Trump million million million of of of the',\n",
              "  'spoiler': 'direct result of the city using its own river as a water source'},\n",
              " 266: {'roberta': 'Cin Trump million million million Trump million of of a a',\n",
              "  'spoiler': 'his passion'},\n",
              " 267: {'roberta': 'KBLinin Trump Trumps the a',\n",
              "  'spoiler': \"1. SRK Teases Fans With A Glimpse Of Raees' Song 'O Zalima', Recites Its Lyrics In His Own Voice 2. Swami Om Threatens The Makers Of Violence & Shutting Down Bigg Boss If They Don't Make Him Win! 3. Aamir Khan Aims To Make Maharashtra Drought Free In The Next Five Years 4. Farhan Akhtar Gives The Perfect Response To Abu Azmi's Remarks On Bengaluru Mass Molestation 5. Salim Khan-Aamir Khan Condemn The Shameful Bengaluru Incident, Call For Strengthening Of Laws\"},\n",
              " 268: {'roberta': 'Gt million million million of the a a a',\n",
              "  'spoiler': '9 a.m.'},\n",
              " 269: {'roberta': 'A million million million of of a a',\n",
              "  'spoiler': 'Investigators say the malware the thieves used shared similarities to the code used by a hacking group known as Lazarus.'},\n",
              " 270: {'roberta': 'KKCttt million million Trump million million million of of a a',\n",
              "  'spoiler': \"The National Vietnam Veterans Foundation less than 2 percent for actual veterans and veterans' charitable causes\"},\n",
              " 271: {'roberta': 'KKK Trump Trump million Trump million million millions of a a',\n",
              "  'spoiler': 'Sgt. Jose Luis Sanchez'},\n",
              " 272: {'roberta': 'Kerer millioner million million million Trump million to a',\n",
              "  'spoiler': 'for driving with a suspended license and without a vehicle tag and insurance'},\n",
              " 273: {'roberta': 'KLininer million million millioner millions the a',\n",
              "  'spoiler': \"Kamya Punjabi is all set to release her short film 'Hum Kuchh Kah Na Sakey'\"},\n",
              " 274: {'roberta': 'Kiner million million millioner million of to a',\n",
              "  'spoiler': 'reduced the risk of flu-related hospitalization'},\n",
              " 275: {'roberta': 'KCt million million millions of a a',\n",
              "  'spoiler': '\"If I had no discipline in sex before marriage, I will have no discipline in sex after marriage,\"'},\n",
              " 276: {'roberta': 'Kt million millions the a',\n",
              "  'spoiler': 'link between workaholism and reduced physical and mental well-being'},\n",
              " 277: {'roberta': 'Lss the a', 'spoiler': 'Sombra'},\n",
              " 278: {'roberta': 'Kt million million millions of a',\n",
              "  'spoiler': '1 North Dakota'},\n",
              " 279: {'roberta': 'KL Trump Trump million million million Trump million of of a a a',\n",
              "  'spoiler': 'third \"Riddick\" movie'},\n",
              " 280: {'roberta': 'As the a',\n",
              "  'spoiler': 'NASA tracks more than 500,000 pieces of defunct satellites and old rocket bits orbiting Earth, some no larger than marbles, so that the space station can be maneuvered away from close encounters — but many small pieces, like the one that hit the space station, are too difficult to keep track of'},\n",
              " 281: {'roberta': 'Kin million million million Trump million of of a a',\n",
              "  'spoiler': 'zone-read specialist'},\n",
              " 282: {'roberta': 'Lt million million millioner million million the of of a a',\n",
              "  'spoiler': 'one million dollars'},\n",
              " 283: {'roberta': 'KKKCtererer million million millions of a',\n",
              "  'spoiler': 'to cut down on his perceived duty to wave to other motorcyclists'},\n",
              " 284: {'roberta': 'T million million million Trump million million of of of a',\n",
              "  'spoiler': 'Target’s stock had been dented since the American Family Association’s boycott began amassing signatures in the millions, but we obviously had no way of knowing if people were actually boycotting the store.'},\n",
              " 285: {'roberta': 'KKKCtt Trump million million millions a',\n",
              "  'spoiler': 'Markwayne Mullin welcoming two twin girls into his home'},\n",
              " 286: {'roberta': 'A million million million of of a a a',\n",
              "  'spoiler': 'Braconid'},\n",
              " 287: {'roberta': 'KLter Trump million million million Trump millions a a a',\n",
              "  'spoiler': '$420,000'},\n",
              " 288: {'roberta': 'T million million million Trump million of of a a',\n",
              "  'spoiler': 'begin laying off a large part of his staff starting Wednesday'},\n",
              " 289: {'roberta': 'Lt million million millions the a a',\n",
              "  'spoiler': 'Vintage Home Finds'},\n",
              " 290: {'roberta': 'KKKAKKCCtt million milliont the,',\n",
              "  'spoiler': '5. Al Jubail, Saudi Arabia 4. Riyadh, Saudi Arabia 3. Allahabad, India 2. Gwalior, India 1. Zabol, Iran'},\n",
              " 291: {'roberta': 'Kt million million millioner million million the of of',\n",
              "  'spoiler': 'retro Coca Cola advertisement'},\n",
              " 292: {'roberta': 'L million million million of of of the',\n",
              "  'spoiler': 'Toilet paper, unlike toilet seats, are not designed to ward off bacteria. In fact, they’re sponges for it.'},\n",
              " 293: {'roberta': 'At million million million the of of a a',\n",
              "  'spoiler': 'Lions Die First'},\n",
              " 294: {'roberta': 'ts the a', 'spoiler': 'as a sex toy'},\n",
              " 295: {'roberta': 'Kt million million million Trump million million of of of',\n",
              "  'spoiler': \"The 'Sledgehammer' visual is premiering in IMAX, making it the first music video ever to do that.\"},\n",
              " 296: {'roberta': 't million million million of the',\n",
              "  'spoiler': 'a national catastrophe. a recession. A terrorist attack a divided Democratic Party. '},\n",
              " 297: {'roberta': 'BTCCtt million million millioner million millions of of of the',\n",
              "  'spoiler': 'second Sumatran rhino'},\n",
              " 298: {'roberta': 'TrumpC million Trump million million million of of a a',\n",
              "  'spoiler': 'Trump has a theory. He thinks that Hispanics know that he will create jobs and that many of these jobs will go to Hispanics because his years in business have taught him that Hispanics are especially hard workers.'},\n",
              " 299: {'roberta': 'K million million million Trump million million the of a a a',\n",
              "  'spoiler': 'broadened eligibility in feature writing and investigative reporting to include online and print magazines'},\n",
              " 300: {'roberta': 'ts the a', 'spoiler': 'Ted Cruz'},\n",
              " 301: {'roberta': 'Ain million million million of of of the',\n",
              "  'spoiler': 'asteroid'},\n",
              " 302: {'roberta': 'KBtts of,',\n",
              "  'spoiler': 'The mom, Dorothy, plays the role of the photographer, junk food supplier and a super fan.'},\n",
              " 303: {'roberta': 'Tin million million millions of a a',\n",
              "  'spoiler': '2010 study by scientists from Copenhagen University Hospital reduce a man’s sperm count by as much as thirty percent'},\n",
              " 304: {'roberta': 'Lererer million million millioner millions a a',\n",
              "  'spoiler': 'pizza, chicken wings, and a Mountain Dew'},\n",
              " 305: {'roberta': 'C million million million of of of a',\n",
              "  'spoiler': 'An inward turn for the U.S.? Angry voters China - opportunity or risk? Russian disruption'},\n",
              " 306: {'roberta': 'TLter millioner million million million Trump million to a the',\n",
              "  'spoiler': 'original Tweets'},\n",
              " 307: {'roberta': 'CCCt million millions of the',\n",
              "  'spoiler': 'A recent study proves that cheating is not a one-time thing, and that person will probably cheat on you again'},\n",
              " 308: {'roberta': 'KL million million millioner million million of of of a a a',\n",
              "  'spoiler': 'September 16'},\n",
              " 309: {'roberta': 'Ler million million million of of of a a',\n",
              "  'spoiler': 'Petit'},\n",
              " 310: {'roberta': 'BBttt million million millions of the',\n",
              "  'spoiler': 'ottoman'},\n",
              " 311: {'roberta': 'C million million millioner million of of a a',\n",
              "  'spoiler': '\"It\\'s both sides playing equally bad football. We are not capable of beating teams if we play like that\"'},\n",
              " 312: {'roberta': 'KC million million millions of a a a',\n",
              "  'spoiler': 'strict diet, as well as a strict workout regime'},\n",
              " 313: {'roberta': 'KKt million millions the the',\n",
              "  'spoiler': \"He converted it into a boat's cabin.\"},\n",
              " 314: {'roberta': 'KATtt million million million of the a a',\n",
              "  'spoiler': '1. You cannot go out tonight because you are practicing for the big recital. 2. You cannot go out tonight because you are making cross-stitches of every Kanye West tweet. 3. You cannot go out tonight because you have decided to wash your hair every other day instead of every day, but you will not go out with dirty hair. 4. You cannot go out tonight because it is not Halloween. 5. You cannot go out tonight because you do not want to miss a fire tweet from J.K. Rowling.'},\n",
              " 315: {'roberta': 'Lt million million millioner million million of a a',\n",
              "  'spoiler': 'I have no reason to tell you, and others, something that is not of interest to you.'},\n",
              " 316: {'roberta': 'Lt million million million of the',\n",
              "  'spoiler': 'While still abiding by the new dress code, Rivas is now showing up to work in head-to-toe costumes.'},\n",
              " 317: {'roberta': 'ts the a', 'spoiler': 'Digit'},\n",
              " 318: {'roberta': 'KKKCKCCtt million million million Trump million million of a of',\n",
              "  'spoiler': 'He would come into my classroom, grab a Post-It, write something and stick it to my desk on his way out. One read, ‘I love you so much, my empress’. He would always call me ‘my empress’.'},\n",
              " 319: {'roberta': 'A million million of the a the',\n",
              "  'spoiler': \"brave gentlemen perforates his phone's battery using the tip of a knife, causing a reaction that makes it explode into flames!\"},\n",
              " 320: {'roberta': 'C million million million of of the',\n",
              "  'spoiler': 'The bill would allocate $19.5 billion in funds to NASA in 2017, but it has a critical mission for the space agency: send men to Mars.'},\n",
              " 321: {'roberta': 'Kin million million million Trump million million to the the a a a',\n",
              "  'spoiler': 'Guns N’ Roses'},\n",
              " 322: {'roberta': 'C million million million Trump million million to of a a',\n",
              "  'spoiler': 'He said: \"Google is directly engaged with Hillary Clinton’s campaign\" and claimed the technology giant used the US State Department on a quid pro quo\" basis'},\n",
              " 323: {'roberta': 'T million million million Trump million million of the a a',\n",
              "  'spoiler': 'Clinton'},\n",
              " 324: {'roberta': 'C million million millions of of of a a',\n",
              "  'spoiler': 'She sits down in her shabby overcoat and... starts playing the piano. However, it isn’t just a bit of ‘Chopsticks’! No, indeed. This musical piece was composed by Natalie when she was just 13-years-old!'},\n",
              " 325: {'roberta': 'KLin million million million Trump millions the a',\n",
              "  'spoiler': 'El Chavo'},\n",
              " 326: {'roberta': 'KKBBBCCtt million million million Trump million millions of the',\n",
              "  'spoiler': 'transitioning my portfolio into more income-based opportunities that can provide steady money to cover my expenses each year'},\n",
              " 327: {'roberta': 'Kin million million million of of a a a',\n",
              "  'spoiler': '$15 million'},\n",
              " 328: {'roberta': 'Ltt million millions of a',\n",
              "  'spoiler': 'you’re startling it while it’s in its comfort zone. In worst case scenarios, it can react by hurting itself, breaking a household object, or remaining anxious for days'},\n",
              " 329: {'roberta': 'Kinin million million millions the the the',\n",
              "  'spoiler': '\"Learning to deal with death and grief.\"'},\n",
              " 330: {'roberta': 'Kin million million million Trump million million of of a a',\n",
              "  'spoiler': 'Hypnosis'},\n",
              " 331: {'roberta': 'Lter million million millioner millionerer million to a',\n",
              "  'spoiler': 'Originally written by Mr. Mellencamp as a valentine to his hometown of Seymour, Ind.,'},\n",
              " 332: {'roberta': 'Kin million million Trump million million million the the a a a',\n",
              "  'spoiler': 'Elmore Leonard'},\n",
              " 333: {'roberta': 'TTBBTTTttt millions of a',\n",
              "  'spoiler': 'island of Monserrat'},\n",
              " 334: {'roberta': 'Kin million million millions of a a',\n",
              "  'spoiler': 'someone in Miami, someone he respected, telling the superstar that going to the Cavaliers was the biggest mistake of his career'},\n",
              " 335: {'roberta': 'Tin million million million Trump million of of a a',\n",
              "  'spoiler': 'Plum Creek Timber Rick Holley'},\n",
              " 336: {'roberta': 'LLererer millioner million million millioner Trump million million to a to',\n",
              "  'spoiler': \"Hala Kamil the Oscar-nominated dir. wants to make Syria's crisis matter again\"},\n",
              " 337: {'roberta': 'CorKAACACCCtttint million million million Trump million millioner million million of of',\n",
              "  'spoiler': 'Kathryn Smith Two inmates took justice into their own hands and brutally attacked the woman in a stairwell that was absent of video surveillance. While ten other inmates watched, the two women went to working beating Smith, using a homemade knife to stab her and punch her numerous times. While Smith howled in pain, the others watched for four minutes as the attack continued, resulting in a cut across her cheek that was an inch in length.'},\n",
              " 338: {'roberta': 'Tin million million million of of a',\n",
              "  'spoiler': '$290 million'},\n",
              " 339: {'roberta': 'L million million millions of a a',\n",
              "  'spoiler': '15 percent of the new units it creates will go to low- or middle-income families'},\n",
              " 340: {'roberta': 'TBATTtt million million millions of the',\n",
              "  'spoiler': 'A 35-year-old woman in Melbourne, Australia collapsed after her pair of skinny jeans stopped the blood flow in her calf muscles!'},\n",
              " 341: {'roberta': 'A millions the a a',\n",
              "  'spoiler': \"Black Mirror's 'San Junipero': 2016's most life-affirming piece of television\"},\n",
              " 342: {'roberta': 'KBt million million million Trump million million the a a a',\n",
              "  'spoiler': 'The statue, titled \"Sleepwalker\" Wellesley College'},\n",
              " 343: {'roberta': 'Ct million million million of of of the the',\n",
              "  'spoiler': 'Type 2 diabetes'},\n",
              " 344: {'roberta': 'L million million millions of of to to the',\n",
              "  'spoiler': 'Janine Hall'},\n",
              " 345: {'roberta': 'KKKKTKKCttinttt million million Trump million million millions and',\n",
              "  'spoiler': '\"[I told her] ‘that’s fine’, with a smile, and she moved aside, waiting for a white person. I told my staff that she is racist and that we won’t be serving her, while I continued serving other customers with a smile.\"'},\n",
              " 346: {'roberta': 'Tt million million million Trump million million of of the',\n",
              "  'spoiler': 'the little girl says something so profound that it changes the mother’s mind'},\n",
              " 347: {'roberta': 'BTCtts of a',\n",
              "  'spoiler': 'trudged through 32.5km of heavy Alpine snow'},\n",
              " 348: {'roberta': 'Att million million million the a a',\n",
              "  'spoiler': 'wedding dress'},\n",
              " 349: {'roberta': 'Kt million million million of the a a',\n",
              "  'spoiler': 'retract and disappear if tampered'},\n",
              " 350: {'roberta': 'KKCttt millioner million million millions of a',\n",
              "  'spoiler': 'six or a seven'},\n",
              " 351: {'roberta': '$ to,',\n",
              "  'spoiler': 'There will be fights. There will be anger. There will be score-keeping. there will be hard times. It’s OK that marriage is hard.'},\n",
              " 352: {'roberta': 'Kin million million million of of of a',\n",
              "  'spoiler': 'Pounded by the Pound: Turned Gay by the Socioeconomic Implications of Britain Leaving the European Union'},\n",
              " 353: {'roberta': 'T million million million Trump million million the of a a a',\n",
              "  'spoiler': 'saving them to use in 2017 plan on taking advantage of the fact that their company lets them roll over unused days. many U.S. workers feel bound to their jobs'},\n",
              " 354: {'roberta': 'Ltt million million Trump million million millions a a',\n",
              "  'spoiler': 'Trey Songz'},\n",
              " 355: {'roberta': 'Tt million million million Trump million million of of a a',\n",
              "  'spoiler': 'pilot made sure the plane didn’t crash into an apartment building'},\n",
              " 356: {'roberta': 'KL million million millions of the the a the',\n",
              "  'spoiler': 'mediation gardens prayer rooms chapels'},\n",
              " 357: {'roberta': 'Cin million million million the of of a',\n",
              "  'spoiler': 'the S&P 500 rose in each of the first three quarters'},\n",
              " 358: {'roberta': 'C million million millions the the the to to a',\n",
              "  'spoiler': \"BUY CLOTHES OUT OF SEASON STOCKPILE WHEN THE PRICE IS RIGHT ASK FOR DISCOUNTS DON'T BE AFRAID TO PRAISE AND COMPLAIN CHECK FOR A CASHBACK DEAL\"},\n",
              " 359: {'roberta': 't million million million the the',\n",
              "  'spoiler': 'far too early to declare Rio any sort of ratings disaster'},\n",
              " 360: {'roberta': 'Kt million million millions of a',\n",
              "  'spoiler': 'newspaper pulp'},\n",
              " 361: {'roberta': 'VirginiaTrumpCt million million million of a a',\n",
              "  'spoiler': \"The poll found that 48 percent of Americans approve of Obama speaking on the phone with Iranian President Hassan Rouhani, while 31 percent said they disapprove (another 21 percent weren't sure).\"},\n",
              " 362: {'roberta': 'KCin million million millions of a a',\n",
              "  'spoiler': 'Oregano'},\n",
              " 363: {'roberta': 'K million million millioner million million of of of a of a a a',\n",
              "  'spoiler': \"This means that the day of St. Patrick's death — March 17 in the Gregorian calendar — falls on March 30 for the Russian Orthodox Church.\"},\n",
              " 364: {'roberta': 'TrumpBCCCtt Trump million million million Trump million Trump Trump million of of',\n",
              "  'spoiler': 'Californian state law defines rape as penetration by the penis.'},\n",
              " 365: {'roberta': 'Kinin million million million Trump million million the the a a the',\n",
              "  'spoiler': 'great tomatoes'},\n",
              " 366: {'roberta': 'C Trump million million million Trump million of a a',\n",
              "  'spoiler': \"California's economy grew by 4.1 percent in 2015, according to new numbers from the Bureau of Economic Analysis, tying it with Oregon for the fastest state growth The Kansas economy, on the other hand, grew 0.2 percent in 2015. That's down from 1.2 percent in 2014\"},\n",
              " 367: {'roberta': 'KKBBKtts the',\n",
              "  'spoiler': '\"BLACK PPL\" typed at the top of their receipt'},\n",
              " 368: {'roberta': 'Lt million million millions of the',\n",
              "  'spoiler': 'many women we spotted wearing the exact same outfit'},\n",
              " 369: {'roberta': 'KC million million millions of a a a',\n",
              "  'spoiler': 'you can only live there for ten months out of the year.'},\n",
              " 370: {'roberta': 'KCer million million million Trump million millioner million of to of of of',\n",
              "  'spoiler': 'Ruben Guerrero'},\n",
              " 371: {'roberta': 'KC million million millions of a a a', 'spoiler': 'Ramen'},\n",
              " 372: {'roberta': 'Lerer million million millioner millions to a',\n",
              "  'spoiler': 'Benjamin Deeds Comes Out As Gay'},\n",
              " 373: {'roberta': 'LLer Trump million million million Trump million of a a',\n",
              "  'spoiler': 'they were real! Or at least, were based on real stories'},\n",
              " 374: {'roberta': 'KCiner Trump Trump million million million Trump million of to',\n",
              "  'spoiler': 'Tawny Kitaen'},\n",
              " 375: {'roberta': 'KCin million million millions of the',\n",
              "  'spoiler': 'Maryland'},\n",
              " 376: {'roberta': 'T million million million of of a a',\n",
              "  'spoiler': 'going for a walk'},\n",
              " 377: {'roberta': 'ATCtt million million millions the a',\n",
              "  'spoiler': \"'The Choking Game,'\"},\n",
              " 378: {'roberta': 'Ttt million millioner million million million the',\n",
              "  'spoiler': 'What determines your success isn’t \"What do you want to enjoy?\" The question is, \"What pain do you want to sustain?\" The quality of your life is not determined by the quality of your positive experiences but the quality of your negative experiences.'},\n",
              " 379: {'roberta': 'KKABAAAttts of,',\n",
              "  'spoiler': 'double-checking that the high-quality option is checked browser could be causing your streaming issues While watching a movie or TV show, press Shift+Opt or Shift+Alt (for Windows) and left click to reveal the evasive Stream Manager menu. The options on this menu can help fix buffering.'},\n",
              " 380: {'roberta': 'Lss the a a',\n",
              "  'spoiler': 'Melissa McCarthy is Abby Yates, Kristen Wiig is Erin Gilbert, Kate McKinnon is Jillian Holtzmann and Leslie Jones is Patty Tolan.'},\n",
              " 381: {'roberta': 'Kt million million millions of a',\n",
              "  'spoiler': 'body image issues'},\n",
              " 382: {'roberta': 't million million million the a a a',\n",
              "  'spoiler': 'lucky first guess while playing the clock game'},\n",
              " 383: {'roberta': 'TLt Trump Trump million million million Trump million of of',\n",
              "  'spoiler': 'Katy Perry kept joking on set, calling herself ‘No. 1’ on the call sheet'},\n",
              " 384: {'roberta': 'Ktiner Trump million million million Trump million of of a',\n",
              "  'spoiler': 'Bizzle'},\n",
              " 385: {'roberta': 'Kt million million millions of a',\n",
              "  'spoiler': '\"When I make my first entrance. I’d like to come out of the door carrying a cane and then walk toward the crowd with a limp. After the crowd sees Willy Wonka is a cripple, they all whisper to themselves and then become deathly quiet. As I walk toward them, my cane sinks into one of the cobblestones I’m walking on and stands straight up, by itself; but I keep on walking, until I realize that I no longer have my cane. I start to fall forward, and just before I hit the ground, I do a beautiful forward somersault and bounce back up, to great applause.\"'},\n",
              " 386: {'roberta': 'KKKTttts the', 'spoiler': 'Coln River in Gloucestershire'},\n",
              " 387: {'roberta': 't million million million of the the', 'spoiler': 'slash'},\n",
              " 388: {'roberta': 'BCC Trump Trump million million Trump million of a a',\n",
              "  'spoiler': 'Vince Gill'},\n",
              " 389: {'roberta': 'KC million million millions of a a a',\n",
              "  'spoiler': \"A photo posted by Paris-Michael K. Jackson (@parisjackson) on OK, so she's there and there's possibly someone in the other front seat of the car. But who the fuck is that in the hat in the back seat lurking in the shadows? It's quite obviously MJ. No one else wears a fedora and a mask, right?\"},\n",
              " 390: {'roberta': '1The.',\n",
              "  'spoiler': 'An ongoing study out of Sweden seems to indicate that a shorter work day may actually result in more productivity.'},\n",
              " 391: {'roberta': 't million million million of a',\n",
              "  'spoiler': \"Uber let us get behind the wheel and experience what it's like to monitor a giant robot on wheels, but not everything went according to plan.\"},\n",
              " 392: {'roberta': 'T million million million of of a a',\n",
              "  'spoiler': 'They claim to have lost faith in the Croatian Football Federation and have accused it of widespread corruption.'},\n",
              " 393: {'roberta': 'A million million million of a a',\n",
              "  'spoiler': \"Virtual reality will get more physical VR is an industry-wide effort A wireless Rift or Vive won't happen soon\"},\n",
              " 394: {'roberta': 'KKKLter million million millions of a',\n",
              "  'spoiler': '\"Mandela: Long Walk to Freedom\" in 2017'},\n",
              " 395: {'roberta': 'KBCtin Trump Trump Trump million million million of,',\n",
              "  'spoiler': 'congestive heart failure, along with a liver mass'},\n",
              " 396: {'roberta': 'T million million million of a a',\n",
              "  'spoiler': 'the effect of jet lag increased the air pressure inside the cabin'},\n",
              " 397: {'roberta': 'Lin million million millions of a',\n",
              "  'spoiler': 'cheat programs'},\n",
              " 398: {'roberta': 'GGGt million million millions of a',\n",
              "  'spoiler': 'Self-crucifixion: Philippines Bunny killing: New Zealand Clay pot throwing: Corfu Inter-church rocket war: Chios Hill burning: Texas'},\n",
              " 399: {'roberta': 'KC million million million Trump million million of of the a a',\n",
              "  'spoiler': 'People who work in schools, higher ed, public institutions of education — they are government employees. Most of the not-for-profits we work with would be 50 to 90 percent government funded \"Take the mortgage deduction,\" he continued. \"This is to stimulate homeownership amongst people who are already going to own homes. That is worth, to a middle-income family, a hundred bucks a year. There is data for the number of firearms manufactured, licenses, inspections'},\n",
              " 400: {'roberta': 'KTTCtt million million millions the',\n",
              "  'spoiler': 'So, when his buddy Danny Kolzow, a 23-year-old nurse at Baylor All Saints in Fort Worth, TX, found himself in need of a kidney, McMillan didn’t hesitate to find out if he could donate one of his.'},\n",
              " 401: {'roberta': 'A million million million of of of the',\n",
              "  'spoiler': 'They believe that selflessness at work leads to exhaustion, and ironically often hurts the people you intended to help'},\n",
              " 402: {'roberta': 'C million million millions of of a a',\n",
              "  'spoiler': 'Have a Bunch of Money'},\n",
              " 403: {'roberta': 'A million million million of of a a a',\n",
              "  'spoiler': 'Braconid'},\n",
              " 404: {'roberta': 'KCt million million millions of a a',\n",
              "  'spoiler': 'Cured egg yolks'},\n",
              " 405: {'roberta': 'Kt million million millions of a a',\n",
              "  'spoiler': 'iconic John Williams music'},\n",
              " 406: {'roberta': 'BTTttts of a',\n",
              "  'spoiler': '1. Take a long, warm shower with sweet-smelling essential oils. 2. Create the perfect ambience before you head over to sleep. 3. Let your bed exclusively be for sleeping. 4. Read or listen to something soothing. 5. Tell yourself a story.'},\n",
              " 407: {'roberta': 'KKBBBTttts of a', 'spoiler': 'distraction'},\n",
              " 408: {'roberta': 'KKBBCCCmanmanermanererer Trump million million millioner million million Trump million Trump Trump million of of',\n",
              "  'spoiler': 'Lord Ivar Mountbatten'},\n",
              " 409: {'roberta': 'Kt million million million Trump million million of the a a',\n",
              "  'spoiler': '\"Don\\'t sacrifice happiness for that final number in the bank.\"'},\n",
              " 410: {'roberta': 'Lt million million millions of of a',\n",
              "  'spoiler': '\"Who is watching your kids?\"'},\n",
              " 411: {'roberta': 't million million million to the',\n",
              "  'spoiler': \"To ensure you're building a good rapport with users, don't be disingenuous or misrepresent your brand's identity on social media.\"},\n",
              " 412: {'roberta': 'L million million millioner million million the of of a a a',\n",
              "  'spoiler': 'Celia Cruz'},\n",
              " 413: {'roberta': 'C million million millions of of a a a',\n",
              "  'spoiler': '\"I\\'ve got three more years on my contract with United and the desire to win more titles here.\"'},\n",
              " 414: {'roberta': 'TLt million million millioner million million the the a of a a a',\n",
              "  'spoiler': '\"Malibu,\" \"The Beach Boy,\" Homesick for Another World '},\n",
              " 415: {'roberta': 'Cin million million million the the a a a',\n",
              "  'spoiler': '$117 billion'},\n",
              " 416: {'roberta': 'Ttt million million million the of of a',\n",
              "  'spoiler': \"She wanted to know why the Chihiro's parents turned into pigs, what was in their food, and how she managed to pass her final exam. In the letter Studio Ghibli explained that the swine transformation represented the greed that took place during Japan's bubble economy of the 80s.\"},\n",
              " 417: {'roberta': 'KKman million million million Trump million of a a',\n",
              "  'spoiler': 'The title, Al-Maria says, is in part a reference to the American \"national holiday of shopping,\" but also a nod to the holy nature of Fridays in Islam.'},\n",
              " 418: {'roberta': 'T million million million the of of a a',\n",
              "  'spoiler': '1. Get an education 2. Say hello to risk 3. Take that risk through index funds 4. Put a lid on fees 5. Use a Roth IRA or a Roth 401(k)'},\n",
              " 419: {'roberta': 'K million million million of of of a',\n",
              "  'spoiler': 'New York'},\n",
              " 420: {'roberta': 'KKBBBCmanmanerer Trump Trump million million million Trump million of,',\n",
              "  'spoiler': 'Michael Sam'},\n",
              " 421: {'roberta': 'As the a a',\n",
              "  'spoiler': 'Police in western Sweden initially thought a man whose car was stuck in the snow was Danish, but after a while they realized he was slurring his words because he was drunk.'},\n",
              " 422: {'roberta': 'T million million millions the a a a',\n",
              "  'spoiler': 'Swoggi actually delivers its promises'},\n",
              " 423: {'roberta': 'LLerer millioner million million millions a a',\n",
              "  'spoiler': 'yellow'},\n",
              " 424: {'roberta': 'TCt million million millioner million million the the a of',\n",
              "  'spoiler': 'The Polished Man Project which aims to raise awareness about the number of children who face sexual abuse'},\n",
              " 425: {'roberta': 'Lt million million millions a a a',\n",
              "  'spoiler': \"Gender matters not when you're rocking the Force like you are, baby!\"},\n",
              " 426: {'roberta': 'KKinererer million million million Trump million of a a',\n",
              "  'spoiler': 'men had just left Officer Smith’s funeral'},\n",
              " 427: {'roberta': 'Ct million million million Trump million of a a',\n",
              "  'spoiler': 'climate change'},\n",
              " 428: {'roberta': 'C million Trump million million million of to a a a',\n",
              "  'spoiler': 'Trump ran up huge margins with white Florida voters -- who comprise 64 percent of the state’s voting rolls -- and have always been the most reliable voters in terms of turnout.'},\n",
              " 429: {'roberta': 'Tt million million million of the',\n",
              "  'spoiler': 'Y Combinator to test a scheme called Universal Basic Income'},\n",
              " 430: {'roberta': 'K million million million of the', 'spoiler': '1. Iceland'},\n",
              " 431: {'roberta': 'Tt million million million to the a a',\n",
              "  'spoiler': 'In short: he is giving his owner some seriously relentless side-eye'},\n",
              " 432: {'roberta': 'KC million million millions of a a a',\n",
              "  'spoiler': 'supermarkets'},\n",
              " 433: {'roberta': 'T million million million of of a a',\n",
              "  'spoiler': 'not available for pre-order'},\n",
              " 434: {'roberta': 't million million million to the', 'spoiler': 'textbooks'},\n",
              " 435: {'roberta': 'KBBTTttt of,',\n",
              "  'spoiler': 'he belts out a few of his favorite Broadway tunes'},\n",
              " 436: {'roberta': 'Tt million million millions of a a',\n",
              "  'spoiler': \"People waste years of their lives not being willing to waste hours of their lives. If you mistake busyness for importance, which we do a lot, you're not able to see what really is important\"},\n",
              " 437: {'roberta': 'VirginiaKGKGGGKttts of a', 'spoiler': 'Austin, Texas'},\n",
              " 438: {'roberta': 'Kin million million million of the a a',\n",
              "  'spoiler': 'juvenile-onset disorder'},\n",
              " 439: {'roberta': 'C million million millioner million million to of a a',\n",
              "  'spoiler': '\"YInMn blue.\"'},\n",
              " 440: {'roberta': 't million million million of of a', 'spoiler': 'blue'},\n",
              " 441: {'roberta': 'KBCCin million million millions the a',\n",
              "  'spoiler': 'It is a form of apophenia, when people see patterns in random, unconnected data'},\n",
              " 442: {'roberta': 'Kin million million million of of a a',\n",
              "  'spoiler': '\"Parenthood,\"'},\n",
              " 443: {'roberta': 'KKt million million millioner million million the of of a a',\n",
              "  'spoiler': 'strong winds can still carry the stench back over to the city on a hot day'},\n",
              " 444: {'roberta': 'As the a a a', 'spoiler': 'albino raccoon'},\n",
              " 445: {'roberta': 'KLter million million million Trump million million of a a',\n",
              "  'spoiler': 'Jason Aldean'},\n",
              " 446: {'roberta': 'KKTttts the',\n",
              "  'spoiler': 'he called up JK and asked if he could say ‘Harry put it in his pocket’ instead. She thought for a moment, then said ‘no’, and hung up.'},\n",
              " 447: {'roberta': 'L million million millions of of the the the',\n",
              "  'spoiler': '1,448.7 days'},\n",
              " 448: {'roberta': 'Tt million million millions of a',\n",
              "  'spoiler': 'Before a game, I’m always listening to gospel music.'},\n",
              " 449: {'roberta': '$ million million millions of the',\n",
              "  'spoiler': 'can afford more than that'},\n",
              " 450: {'roberta': 'Kiner million million million Trump million the a a',\n",
              "  'spoiler': 'Rodgers earned the role on his talent alone'},\n",
              " 451: {'roberta': 'C million million millions of of a a',\n",
              "  'spoiler': 'Thea: The Awakening added multiplayer'},\n",
              " 452: {'roberta': 'Lt million million millions of the the',\n",
              "  'spoiler': \"1) Readers wanted to read about it 2) Mad Men debuted right after The Sopranos ended and filled the void the show left 3) There just wasn't as much good TV on 4) Mad Men was actually pretty popular, when all was said and done 5) Never underestimate passion\"},\n",
              " 453: {'roberta': '$ the',\n",
              "  'spoiler': '1. Eat legumes 2. Eat plant sterols, margarines and spreads 3. Eat nuts 4. Use olive oil 5. Avoid junk food'},\n",
              " 454: {'roberta': 'K million million million the of a the',\n",
              "  'spoiler': 'Pennsylvania'},\n",
              " 455: {'roberta': 'CCin million million millions of the',\n",
              "  'spoiler': 'plastic debris'},\n",
              " 456: {'roberta': 'K million million millioner million million the of the a a a',\n",
              "  'spoiler': \"According to new findings from the Pew Research center, 40 percent of marriages involve one spouse who's been married before.\"},\n",
              " 457: {'roberta': 'Kt million million millions of of the',\n",
              "  'spoiler': \"Amber Galloway Gallego is not your everyday American Sign Language (ASL) interpreter — she specialises in the interpretation of music, in real time and in front of thousands of people. Taylor Anderton and Michael Cox, who were both born with Down syndrome, have known each other for six years and been a serious couple since last year. When Michael partnered Taylor at the 16th Gold Coast Debutante Ball for Disability, they had already decided they would spend the rest of their lives together — and our video of their love story has had more than 13 million views on social media. Louise the infant koala — a squeaking, wet, grey bundle of fur found on the ground after the east coast storms — became a symbol of hope after wild weather hit Australia's east coast in June. The storm cell brought tragedy and tears to many who lost loved ones, livelihoods, and homes, but when the iconic little Australian was rescued — and named after rescuer Louise Haynes — her fate was soon being followed by thousands of fans on social media.\"},\n",
              " 458: {'roberta': 'TTtt million millions of a',\n",
              "  'spoiler': 'Kili the Senegal parrot has somehow managed to become a master at \"playing dead.\"'},\n",
              " 459: {'roberta': 'Ktt million millioner million million million the of of a of',\n",
              "  'spoiler': 'Miley Cyrus'},\n",
              " 460: {'roberta': '$ the',\n",
              "  'spoiler': 'The items on the lengthy checklist include childhood favorites, such as going on a bike or scooter ride, eating ice cream or candy and watching a favorite TV program'},\n",
              " 461: {'roberta': 'Kin million million millions of a a a',\n",
              "  'spoiler': 'roll back the time in the device by around 25 minutes.'},\n",
              " 462: {'roberta': 'C million million millioner million million of of the a a',\n",
              "  'spoiler': 'handwashing habits'},\n",
              " 463: {'roberta': 'KLman Trump Trump Trump million million Trump million of a',\n",
              "  'spoiler': 'Jussie Smollett'},\n",
              " 464: {'roberta': 'ts the a',\n",
              "  'spoiler': '\"Many Republican voters have an unfavorable impression of him — worse still, many don’t even think he has the knowledge, temperament or qualifications to be president,\" Cohn wrote. \"If Mr. Trump can’t reunify Republican-leaning voters, Mrs. Clinton really could win in a landslide.\"'},\n",
              " 465: {'roberta': 'Ats the a',\n",
              "  'spoiler': 'post pictures of your boarding pass on social media'},\n",
              " 466: {'roberta': 'Kin million million million Trump million million to of of',\n",
              "  'spoiler': 'the \"Delaware loophole\" — the unassuming building at 1209 North Orange Street — has become, as the Guardian described Reportedly dozens of Fortune 500 companies — Coca-Cola, Walmart, American Airlines, and Apple, to name a few — use Delaware’s strict corporate secrecy laws and legal tax loopholes by registering the North Orange Street address for official business.'},\n",
              " 467: {'roberta': 'Lt million million million Trump million million the a a a',\n",
              "  'spoiler': 'Kevin Hart'},\n",
              " 468: {'roberta': 'L million million millioner million million the the a a a',\n",
              "  'spoiler': \"HBO Go is also coming to Amazon's new $99 streaming media player, Amazon Fire, by the end of the year\"},\n",
              " 469: {'roberta': 'KKKBKKtts of,', 'spoiler': '$25,000 per month'},\n",
              " 470: {'roberta': 'C million million million to of a a a',\n",
              "  'spoiler': '#1: Goodbye Sapphire? #2: New Home Button, New Limitations'},\n",
              " 471: {'roberta': 'As the a', 'spoiler': 'University of Vermont'},\n",
              " 472: {'roberta': 'BTt million million million Trump million million of of the',\n",
              "  'spoiler': 'Doors open based on who you know As you build your network of contacts, remember to follow up In a new situation, aim to assimilate Dress for the job you want Be humble and willing to learn from any task'},\n",
              " 473: {'roberta': 'KKKCtt million millions the,',\n",
              "  'spoiler': '1. Museum of Modern Art 2. Metropolitan Museum of Art 3. Guggenheim 4. The Whitney Museum of American Art 5. Musée du Louvre'},\n",
              " 474: {'roberta': 'KBBBttt million million millions a',\n",
              "  'spoiler': 'a vegan athlete can compete effectively at a high level'},\n",
              " 475: {'roberta': 'KKCCt million million millions the a',\n",
              "  'spoiler': \"Bolivia's chaotic San Pedro Prison became a tourist attraction, a place where backpackers stayed for weeks and partied with inmates\"},\n",
              " 476: {'roberta': 'Cin million million million of a a',\n",
              "  'spoiler': 'a charcoal-black moon'},\n",
              " 477: {'roberta': 'Ats the a a', 'spoiler': 'Tokyo Chicago London'},\n",
              " 478: {'roberta': 'Kter millioner million million millions the a',\n",
              "  'spoiler': \"they'll be teasing their collaboration on Snapchat\"},\n",
              " 479: {'roberta': 'Cin million million million Trump million million to of a a a',\n",
              "  'spoiler': 'Mexico City'},\n",
              " 480: {'roberta': 'T million million million of of a', 'spoiler': 'MS Dhoni'},\n",
              " 481: {'roberta': 'KBBBttt million millions of the',\n",
              "  'spoiler': '4 mm tumor in her brain'},\n",
              " 482: {'roberta': 'Lt million million millioner million million the the the',\n",
              "  'spoiler': \"basically the 'undercut,'\"},\n",
              " 483: {'roberta': 'Lter million million million Trump million million of to a',\n",
              "  'spoiler': 'she had plastic surgery years ago to make her eyes look \"bigger\"'},\n",
              " 484: {'roberta': 'LLerer Trump Trump Trumper Trump million of a',\n",
              "  'spoiler': 'Shape Magazine - Meryl Streep and Robert Redford shock the world with plans of getting married after years of keeping their relationship a secret.'},\n",
              " 485: {'roberta': 'K million million millions of a a a',\n",
              "  'spoiler': 'tons of benefits to metcon workouts, including improved fat loss, increased calorie burn compared to steady-state workouts, muscular strength gains, and improved cardiovascular function'},\n",
              " 486: {'roberta': 'Ler millioner million million million Trump million the,',\n",
              "  'spoiler': 'Katy Perry'},\n",
              " 487: {'roberta': 'Cin million million million of a a',\n",
              "  'spoiler': 'Netherlands'},\n",
              " 488: {'roberta': 'KKKCinininererer millioner million million million Trump million million of,',\n",
              "  'spoiler': 'continued posting photos to her account, along with perfect messages for her critics'},\n",
              " 489: {'roberta': 'in millions the a',\n",
              "  'spoiler': 'DICE showed him a small demo of what it had in mind'},\n",
              " 490: {'roberta': 'A million million million the a the a a',\n",
              "  'spoiler': 'photograph of a doormat'},\n",
              " 491: {'roberta': 'BTttt million million millions of a',\n",
              "  'spoiler': '1. A SINGLE TABLESPOON HAS MORE SUGAR THAN A CHOCOLATE CHIP COOKIE. 2. IT OVERPOWERS EVERYTHING IT TOUCHES. 3. THAT GLOPPY TEXTURE IS FOUL. 4. THE SMELL. 5. I CAN NEVER ESCAPE IT.'},\n",
              " 492: {'roberta': 'A million million million the the a the',\n",
              "  'spoiler': 'Kat Cole'},\n",
              " 493: {'roberta': 'Kts million million the,',\n",
              "  'spoiler': '1937 Bugatti Type 57S amid piles of medical machinery 1,500 beer steins thousands of receipts World War II spy drone'},\n",
              " 494: {'roberta': 'Kin million million million Trump million of a a',\n",
              "  'spoiler': '\"I feel like a Republican now because they don’t stand for same-sex marriage, and that appeals to me,\" he said.'},\n",
              " 495: {'roberta': 'LLererer million million millioner millionerer P the a a a',\n",
              "  'spoiler': 'Peter Pilotto'},\n",
              " 496: {'roberta': 'GBBCtt million million millioner million million the of of of the the the a of the',\n",
              "  'spoiler': 'irregular working hours and work stress in the IT, media industry and BPOs'},\n",
              " 497: {'roberta': 'Ct Trump million million million Trump million of the',\n",
              "  'spoiler': \"'Hey Trump, f*** you!'\"},\n",
              " 498: {'roberta': 'BCCt Trump Trump million million million of,',\n",
              "  'spoiler': 'Nebraska allocates its electoral college votes not in the usual winner-take-all method, but by congressional district.'},\n",
              " 499: {'roberta': 'C million million million Trump million million to of of',\n",
              "  'spoiler': 'Introducing what he billed as an \"energy plan,\" Trump promised to \"cancel the Paris Climate Plan.\"'},\n",
              " 500: {'roberta': 'K million million million the the the to the the',\n",
              "  'spoiler': \"1. Adventurous Christmas: Chile 2. Premium Christmas: Switzerland 3. Christmas in Santa's home: Finland 4. Christmas-at-sea: Genting 5. Break the bank Christmas: The Maldives\"},\n",
              " 501: {'roberta': 'in million million million of a',\n",
              "  'spoiler': 'SECOND WIND NEW REALITY FRESH FACES LOCATION, LOCATION, LOCATION GENERATION X'},\n",
              " 502: {'roberta': 'TrumpKCCCt million million millions of the',\n",
              "  'spoiler': '\"Despite being awake, they are not physically active. They spend their day sitting inside cars, at their desk or in meeting rooms. They usually lie down while watching TV or checking their smartphones,\"'},\n",
              " 503: {'roberta': 't million million the a',\n",
              "  'spoiler': 'online multiplayer unplayable'},\n",
              " 504: {'roberta': 'KCCtss of a', 'spoiler': 'raw apple and raw lettuce'},\n",
              " 505: {'roberta': 'FloridaTTTCCTttts of the',\n",
              "  'spoiler': 'allergic reaction from a wasp like John was stung by'},\n",
              " 506: {'roberta': 'KKCCtt million millions of a',\n",
              "  'spoiler': 'fettuccine Alfredo'},\n",
              " 507: {'roberta': 'Ct million million millions of of a a',\n",
              "  'spoiler': 'serious heart condition, and also attributes her two miscarriages'},\n",
              " 508: {'roberta': 'BATTTtts the',\n",
              "  'spoiler': 'remove sugar from my diet, stop late-night eating and start using the gym membership'},\n",
              " 509: {'roberta': 'KKKCininin million million million Trump million of,',\n",
              "  'spoiler': 'Fire authorities in Western Australia say no properties were lost during a bushfire around the town of Gwindinup south-east of Bunbury, which has now been contained.'},\n",
              " 510: {'roberta': 'KBCCCttt million million millions of a',\n",
              "  'spoiler': '1.) Housing 2.) School Fees 3.) Food medicines for chronic illness (Cancer, HIV, etc), livestock for income, and clothing & shoes. no evidence that vice consumption is different from others in the community who did not receive transfers.'},\n",
              " 511: {'roberta': 'A million million million the of of a a the',\n",
              "  'spoiler': 'The logic of the drugs war only leads one way: the police get smarter, so the criminals get nastier'},\n",
              " 512: {'roberta': 'BTCtts of a',\n",
              "  'spoiler': 'Gulls are what\\'s known as \"opportunistic kleptoparasites,\" meaning they don\\'t derive 100 percent of their diets from heist but rather just steal when the opportunity arises. And a big vat of tikka masala is an opportunity no gull can afford to miss.'},\n",
              " 513: {'roberta': 'Kin Trump million million million of of the the',\n",
              "  'spoiler': 'the title page'},\n",
              " 514: {'roberta': 'Kin Trump Trump million million million of a a a',\n",
              "  'spoiler': 'Shamus Beaglehole'},\n",
              " 515: {'roberta': 'Kin million million million Trump million million to a of',\n",
              "  'spoiler': 'When you trade your DNA for information you are also giving it away -- and the database you give it to, owns it. What if that company goes out of business and sells the database? Do the protections you have been promised still stand?'},\n",
              " 516: {'roberta': 'BTCtt million million millions the a',\n",
              "  'spoiler': 'Wolverampton'},\n",
              " 517: {'roberta': 'KLinin million million millions of a',\n",
              "  'spoiler': 'colourful igloo'},\n",
              " 518: {'roberta': 'LLerer million million million Trump million millions a a',\n",
              "  'spoiler': 'none can claim to rival the atmosphere and immersion of Unbreakable'},\n",
              " 519: {'roberta': 'KKCinin million million millions of a',\n",
              "  'spoiler': 'free burger (up to $10 value) with the purchase of an adult entree on Sunday at Ruby Tuesday BurgerFi is selling $5 cheeseburgers on Sunday Delta (DAL) will serve Shake Shake (SHAK) burgers aboard select flights from JFK to LAX on Sunday. In addition, customers on the flights will receive a voucher for a free ShackBurger at the LA West Hollywood Shake Shack. Hurricane Grill & Wings is offering $5.99 beef, turkey or veggie burgers with cheese, lettuce, tomato, onion and pickles on Sunday'},\n",
              " 520: {'roberta': 'T million million million of of a a',\n",
              "  'spoiler': 'Investors are now expected to pay for the privilege of lending money to the Swiss for a half-century at a time.'},\n",
              " 521: {'roberta': 'Tt million million millions of a', 'spoiler': 'towel'},\n",
              " 522: {'roberta': 'KKKBtt million million millions the a',\n",
              "  'spoiler': 'J. Crew'},\n",
              " 523: {'roberta': 'Kt million million millions of a a',\n",
              "  'spoiler': 'Andy Stern \"tectonic shifts\" in the labor market that will see more and more workers replaced by robots'},\n",
              " 524: {'roberta': 'KKKKTKTtts of,',\n",
              "  'spoiler': 'might have been a result of something that happened to Walt and his family. Something tragic that involved his mother'},\n",
              " 525: {'roberta': 'Kin million million million Trump million million of of a a a',\n",
              "  'spoiler': 'him breaking a bottle of champagne on a Playboy-branded limo while several of the playmates are visiting New York City'},\n",
              " 526: {'roberta': 'At million million million the the a the',\n",
              "  'spoiler': 'a watch'},\n",
              " 527: {'roberta': 'KBCtt million millions of a a', 'spoiler': 'dead whale'},\n",
              " 528: {'roberta': 'Ler million million million Trump million million to of to',\n",
              "  'spoiler': 'shooter game \"Destiny\"'},\n",
              " 529: {'roberta': 'K million million million of of a a',\n",
              "  'spoiler': 'Xbox head Phil Spencer told Gamespot he doesn’t believe in the piece by piece upgrading of systems, and you won’t be seeing that with the Xbox One.'},\n",
              " 530: {'roberta': 'K million million millioner million million of of of a a a',\n",
              "  'spoiler': 'Costco AmEx cards should switch over to the new Costco Visa Anywhere card'},\n",
              " 531: {'roberta': 'BBBCCtt Trump Trump million million million Trump million of of',\n",
              "  'spoiler': 'online gaming'},\n",
              " 532: {'roberta': 'KBLttt million million millions a',\n",
              "  'spoiler': 'professional Kim Jong Un impersonator'},\n",
              " 533: {'roberta': 'Ct million million million Trump million million to of a a',\n",
              "  'spoiler': 'Only 1 participant out of 85 correctly recalled the Apple logo, and fewer than half of all participants correctly identified the logo'},\n",
              " 534: {'roberta': 'KKKKBKKCKKininin million millions of a',\n",
              "  'spoiler': 'requested Internet access'},\n",
              " 535: {'roberta': 'Kt million million million Trump million million of of a a',\n",
              "  'spoiler': 'frostbite'},\n",
              " 536: {'roberta': 'At million millions the a',\n",
              "  'spoiler': 'The rumor was released by a fake (FAKE, PEOPLE!) @Unreel_News'},\n",
              " 537: {'roberta': 'KCiner million million millions a a',\n",
              "  'spoiler': 'Christine \"Tink\" Newman four continuous hours of CPR'},\n",
              " 538: {'roberta': 'Ktt million million millions of the the',\n",
              "  'spoiler': 'used condoms being discarded under beds women flashing the porter instead of giving a tip parents forgetting to collect their children from the kids club because they were drunk call staff to the room, and then make them wait outside while they get intimate answer the door naked'},\n",
              " 539: {'roberta': 'Lin million million millions of a',\n",
              "  'spoiler': 'Bad language...ill thoughts...rude ideas...negative radical views...I don’t need.'},\n",
              " 540: {'roberta': 'Lerer million million million Trump million million the a a a',\n",
              "  'spoiler': 'José José'},\n",
              " 541: {'roberta': 'GBTCCCt million million millions the a',\n",
              "  'spoiler': 'Worldwide, developed nations would have to pay more than $1.1 trillion'},\n",
              " 542: {'roberta': 'Kts of a',\n",
              "  'spoiler': 'The unemployment rate in November fell to 4.6 percent, the lowest in nine years With 14 total world championship medals, 19-year-old Simon Biles arrived in Rio already a star. It was her near-perfect performance at the Olympics, however — which netted four golds and a bronze it was hard not celebrate alongside the Chicago Cubs after they won their first World Series in 108 years. Several women of color made political history last month when they were elected to prominent leadership positions within their respective states. Few albums in history have managed to be as simultaneously vulnerable and powerful as \"Lemonade\" — Beyonce\\'s sixth solo record that laid bare an important slice of black womanhood.'},\n",
              " 543: {'roberta': 'A million million of a a',\n",
              "  'spoiler': 'Who can guess what evolution has in store for us ten thousand years hence?'},\n",
              " 544: {'roberta': 'BBBCBCmanmant million million millions of the',\n",
              "  'spoiler': 'foetus was detected with anencephaly, a serious defect where parts of the skull are not adequately developed'},\n",
              " 545: {'roberta': 'C Trump million million million of of a',\n",
              "  'spoiler': 'it’s been time to panic for a while'},\n",
              " 546: {'roberta': 'Ct million million millions of a',\n",
              "  'spoiler': 'Wax inside a whale’s ear stores all sorts of useful information on the animal’s exposure to pollutants and stress levels throughout life'},\n",
              " 547: {'roberta': 'KBBBCttt million millions of a a',\n",
              "  'spoiler': 'replace all the larger vessels with those of smaller sizes'},\n",
              " 548: {'roberta': 'Lter million million millions of of a',\n",
              "  'spoiler': '\"No,\" she apparently replied, before pointing at the policeman and adding: \"But he has.\"'},\n",
              " 549: {'roberta': 'Lin million million millioner million of of a a',\n",
              "  'spoiler': 'May 8'},\n",
              " 550: {'roberta': 'Lt million million million Trump million million of the a a',\n",
              "  'spoiler': '\"The Woman in Black\" it was different enough that people realized it was a different reason to go and see it. And the fact that it did so well and it was a really good movie'},\n",
              " 551: {'roberta': 'KKKCCttts of a', 'spoiler': 'The Rock'},\n",
              " 552: {'roberta': 'KBBBCBCtt Trump Trumps of a', 'spoiler': 'Jackson Vroman'},\n",
              " 553: {'roberta': 't million million of the',\n",
              "  'spoiler': '1. Ideas Have Consequences 2. The Road to Serfdom 3. The Closing of the American Mind 4. A Choice Not an Echo 5. Capitalism and Freedom'},\n",
              " 554: {'roberta': 't million million million of a a',\n",
              "  'spoiler': 'Researchers from the Massachusetts Institute of Technology (MIT) have developed a prototype imaging system that’s able to read pages of a book without opening it.'},\n",
              " 555: {'roberta': 'Lt million million million Trump million million the a a',\n",
              "  'spoiler': 'difficult behavior'},\n",
              " 556: {'roberta': 'BTTttt million millions of the',\n",
              "  'spoiler': 'Kiwi Tart Cherries and Tart Cherry Juice Malted Milk and Nighttime Milk Fatty Fish Nuts'},\n",
              " 557: {'roberta': 'LLer Trump million million million Trump million of of a a a',\n",
              "  'spoiler': 'potatoes'},\n",
              " 558: {'roberta': 'Kt million million millions of a a',\n",
              "  'spoiler': 'According to the top beauty gurus in the business, applying your foundation, blusher and concealer incorrectly can add decades to your looks.'},\n",
              " 559: {'roberta': 'Kin Trump million million million Trump million of a a',\n",
              "  'spoiler': 'Jay Pharoah and Taran Killam'},\n",
              " 560: {'roberta': '$ of.', 'spoiler': 'Idaho'},\n",
              " 561: {'roberta': 'ATLtt million million millions the a a',\n",
              "  'spoiler': 'more empathetic towards others'},\n",
              " 562: {'roberta': 'Ats the the a',\n",
              "  'spoiler': 'Katrina Henry Admit it and go get help'},\n",
              " 563: {'roberta': 'tt million million the,',\n",
              "  'spoiler': 'Hip hop fans don’t need to feel pressured to only listen to music that is considered smart. Smart isn’t everything, and just because a song is intellectually smart, doesn’t necessarily mean it is enjoyable to listen to.'},\n",
              " 564: {'roberta': 'Kt million million million Trump million million of of a',\n",
              "  'spoiler': '77'},\n",
              " 565: {'roberta': 'At million million of of the',\n",
              "  'spoiler': 'Your brain needs to prune a lot of those connections away and build more streamlined, efficient pathways. It does that when we sleep.'},\n",
              " 566: {'roberta': 't million million the the',\n",
              "  'spoiler': 'enough medals to throw any Olympic predictions seriously out of whack. And many of the medals that might have otherwise gone to Russian athletes would likely go to competitors from the US'},\n",
              " 567: {'roberta': 'Ats the a a', 'spoiler': 'penis'},\n",
              " 568: {'roberta': 'Ttt million million million of,',\n",
              "  'spoiler': \"1. Don't touch her bump 2. Don't ask if she has chosen a name 3. Don't look shocked when you hear she's pregnant 4. Don't tell her she's getting bigger (and bigger) 5. Don't tell her she's not big enough\"},\n",
              " 569: {'roberta': 'KKKLtss the', 'spoiler': 'years as a single mother'},\n",
              " 570: {'roberta': 'ts to the the the', 'spoiler': '$765,759'},\n",
              " 571: {'roberta': 'C million million million of of a a',\n",
              "  'spoiler': 'save lives on the road, and protect young people from other hazards of drinking'},\n",
              " 572: {'roberta': 'Kin million million million of a a', 'spoiler': 'Oregon'},\n",
              " 573: {'roberta': 'Kt Trump Trump Trump million million million Trump million of to',\n",
              "  'spoiler': 'incredible eye for design'},\n",
              " 574: {'roberta': 'KKBmanmanman million million millions the a',\n",
              "  'spoiler': 'in an ambassadorial capacity'},\n",
              " 575: {'roberta': 'KLman Trump million million million Trump million of of a a a',\n",
              "  'spoiler': \"'Dumb And Dumber' Sequel\"},\n",
              " 576: {'roberta': 'At million million the,', 'spoiler': 'cancer'},\n",
              " 577: {'roberta': 't million million of the',\n",
              "  'spoiler': 'fell an average of 13.7%'},\n",
              " 578: {'roberta': 'KKCtt million million millions of a',\n",
              "  'spoiler': 'tahini-walnut spread'},\n",
              " 579: {'roberta': 'Tt million million million Trump million of of of',\n",
              "  'spoiler': 'singing the set\\'s opening number \"Prancer\" sitting on a couch, reading newspaper and sipping tea.'},\n",
              " 580: {'roberta': 'BBBBTttts of a', 'spoiler': 'strawberry rhubarb'},\n",
              " 581: {'roberta': 'Kin million million million the a a',\n",
              "  'spoiler': 'paper bag.'},\n",
              " 582: {'roberta': '$ the',\n",
              "  'spoiler': 'And instead of boiling them, we should be steaming, stir-frying or even microwaving Brussels to get that perfectly crunchy consistency.'},\n",
              " 583: {'roberta': 'BTttt million millions of the', 'spoiler': 'Moisturizer'},\n",
              " 584: {'roberta': 'Tin million million millions of the',\n",
              "  'spoiler': 'Jimmy Smits'},\n",
              " 585: {'roberta': 'T million million million of of a a',\n",
              "  'spoiler': 'There’s a clear winner here, and unless Linux rectifies its performance disparity, lack of natively supported control options and impoverished game library, the OS to beat for PC gaming will remain Windows 10.'},\n",
              " 586: {'roberta': 't million million million of the',\n",
              "  'spoiler': '1. Stand in front of her 2. Have a relaxed, easy-going smile. 3. Is she hasn’t already looked up at you, simply get her attention with a wave of your hand. Wave your hand in her direct line of vision so she can’t ignore it. 4. When she looks at you, smile and point to her headphones and say, \"Take off your headphones for a minute\" and pretend to be taking headphones off your head, so she fully understands. 5. Then, do what we call \"Acknowledging the Awkwardness\" by quickly mentioning something about the awkwardness of the moment'},\n",
              " 587: {'roberta': 'Lt million million millioner million million to of the',\n",
              "  'spoiler': 'rejecting a fans’s gift and calling it ‘shit’'},\n",
              " 588: {'roberta': 'T million million million the a a',\n",
              "  'spoiler': '1. Half-Life 3 2. The Last Guardian 3. Final Fantasy XV 4. Kingdom Hearts 3 5. Beyond Good & Evil 2'},\n",
              " 589: {'roberta': 'Ats of of the',\n",
              "  'spoiler': 'the guard screamed \"Step back from the Queen\\'s Guard!\" and thrust his rather-deadly gun complete with rather-pointy bayonet in his face'},\n",
              " 590: {'roberta': 'VirginiaAAttts the', 'spoiler': 'playing hide-and-seek'},\n",
              " 591: {'roberta': 'Ct million million million Trump million of of a a',\n",
              "  'spoiler': 'Laura Bush'},\n",
              " 592: {'roberta': 'Kin million million million Trump million million to of a a a',\n",
              "  'spoiler': 'A man dubbed the ‘cash card Casanova’'},\n",
              " 593: {'roberta': 'KKCttss of a', 'spoiler': 'pizza'},\n",
              " 594: {'roberta': 'KCt Trump Trump million million million of a a',\n",
              "  'spoiler': 'JT managed to swat away the woman -- while still keeping rhythm.'},\n",
              " 595: {'roberta': 'K million million million the of the',\n",
              "  'spoiler': 'Depending on your priorities, it might be'},\n",
              " 596: {'roberta': 'GKGBCKKKtts of,', 'spoiler': 'Colorado'},\n",
              " 597: {'roberta': 'TrumpC million million million of of a a',\n",
              "  'spoiler': 'Claims, which count the number of people who applied for unemployment insurance for the first time since the past week, fell to 234,000. This marks the 101st week straight of claims being below 300,000'},\n",
              " 598: {'roberta': 'C million million millions of of a a',\n",
              "  'spoiler': \"couldn't make enough toys to satiate demand in North America, and needed a break while it boosted capacity at its factories and increased its workforce by nearly a quarter.\"},\n",
              " 599: {'roberta': 'Kin Trump million million million of of a a',\n",
              "  'spoiler': 'not killing off Harrison [Ford] at the end'},\n",
              " 600: {'roberta': 'Kin million million million the a a',\n",
              "  'spoiler': 'New generation e-passport featuring enhanced security features such as bio-metric details may soon be rolled out by the government.'},\n",
              " 601: {'roberta': 'L million million millions of of a a',\n",
              "  'spoiler': '1. The action’s going to pick back up in Fall of 1984. 2.\"I think we talked like a larger time jump where the kids are older now and it’s a different decade,\" 3. Expect four brand new characters. 4.Bringing the show’s tally of cool guys the Duffer Brothers admire up to 6 million 5. Good old monster-infested Hawkins won’t be the only location.'},\n",
              " 602: {'roberta': 'Lt Trump Trump Trump million million million of to',\n",
              "  'spoiler': \"John Slattery 'God's Pocket'\"},\n",
              " 603: {'roberta': 'ts the a', 'spoiler': 'money'},\n",
              " 604: {'roberta': 'KBK million million millions of of the the',\n",
              "  'spoiler': 'he slowed his vehicle in a tree-covered section of the road. As the Honda Accord kept moving, he climbed almost imperceptibly out of the driver’s window'},\n",
              " 605: {'roberta': 'KBBCttin million million millions a',\n",
              "  'spoiler': 'Tulsiram Manere'},\n",
              " 606: {'roberta': 'Kin million million million of of a a',\n",
              "  'spoiler': 'she fell'},\n",
              " 607: {'roberta': 'PTTTtts of a',\n",
              "  'spoiler': 'counting each small win we make'},\n",
              " 608: {'roberta': 'K million million millions of of a a the',\n",
              "  'spoiler': 'Chicago'},\n",
              " 609: {'roberta': 'Kin million million million of a a a',\n",
              "  'spoiler': 'being a sex object is empowering'},\n",
              " 610: {'roberta': 'KKACCCttts of,',\n",
              "  'spoiler': 'Access to prisons has been vastly curtailed. There is no way to know what truly happens inside but to go there.'},\n",
              " 611: {'roberta': 'CorTrumpACCCtt million million milliont of a',\n",
              "  'spoiler': 'french fry'},\n",
              " 612: {'roberta': 'KKBKBCKKCininererer million million millioner million Trump million million of of of a a',\n",
              "  'spoiler': 'a vast chamber where several stalagmites had been deliberately broken. Most of the 400 pieces had been arranged into two rings—a large one between 4 and 7 metres across, and a smaller one just 2 metres wide. Others had been propped up against these donuts. Yet others had been stacked into four piles. Traces of fire were everywhere, and there was a mass of burnt bones. These weren’t natural formations, and they weren’t the work of bears. They were built by people.'},\n",
              " 613: {'roberta': 'Cin million million millioner million million to of of the',\n",
              "  'spoiler': 'got distracted by her own ‘erotic thoughts.’'},\n",
              " 614: {'roberta': 'Kin million million million of the a a',\n",
              "  'spoiler': \"My ex raised the thing to eye level, looked at me, and pushed the button. And the thing in his hand sprang open. My relative's foot shavings ERUPTED from within the Ped Egg like an explosion of flesh confetti\"},\n",
              " 615: {'roberta': 'A million million of the a a a',\n",
              "  'spoiler': 'they cannot take 2s and expect to beat 3s'},\n",
              " 616: {'roberta': 'Kt million million millions of a', 'spoiler': 'robots'},\n",
              " 617: {'roberta': 'KLin million million millions of the',\n",
              "  'spoiler': '$10,527,843,932'},\n",
              " 618: {'roberta': 'A million million million the a a a',\n",
              "  'spoiler': 'By dialing the secret phone number 1-999-367-3767, you’re automatically connected to \"Black Cellphones\" and soon after an explosion is triggered (for reasons best known to the creators).'},\n",
              " 619: {'roberta': 'Kin million million million the of of a',\n",
              "  'spoiler': 'colleges'},\n",
              " 620: {'roberta': 'KKCin million million millions of a',\n",
              "  'spoiler': 'TEL AVIV – The sister of a Palestinian terrorist who plowed his truck into a group of soldiers in Jerusalem on Sunday, killing four and wounding 16, said the family was \"thankful\" for his \"most beautiful martyrdom.\"'},\n",
              " 621: {'roberta': 'Ler million million million Trump million million the a a',\n",
              "  'spoiler': 'Neither Ben nor Jen will confirm or deny the divorce allegations.'},\n",
              " 622: {'roberta': 'C million million million Trump million million of of of to',\n",
              "  'spoiler': 'On 5 May last year, Ms Chien took her own life, using the same drug she used to put down animals. She said she wanted to help people understand what happens to strays in Taiwan'},\n",
              " 623: {'roberta': 'TC million million million Trump Trump million of of a',\n",
              "  'spoiler': 'High Country Crime Stoppers for their invaluable service'},\n",
              " 624: {'roberta': 'VirginiaKKKKAKKBKKCKttt million million milliont million Trump million millions of of the',\n",
              "  'spoiler': 'a wave and sleeping face emoji'},\n",
              " 625: {'roberta': 'KBCtt million million million Trump million millions a a',\n",
              "  'spoiler': 'Menelik Watson'},\n",
              " 626: {'roberta': 'Lt million million millions of a',\n",
              "  'spoiler': 'It was definitely \"Feeling This.\" Unless it was actually \"I Miss You.\"'},\n",
              " 627: {'roberta': 'Lmanererer million million millioner million Trump million the a to',\n",
              "  'spoiler': 'Kim Kardashian stepped out in leggings Tuesday for a Pilates class in Los Angeles'},\n",
              " 628: {'roberta': 'L million million millions of of the a',\n",
              "  'spoiler': 'Ana, the kickass heal-sniper'},\n",
              " 629: {'roberta': 'Ct Trump Trump Trump million Trump Trumps a',\n",
              "  'spoiler': 'Last week, Barkley said James was being \"whiny\" for publicly saying the Cavs need to bring in another playmaker.'},\n",
              " 630: {'roberta': 'KBCCDCt Trump million million million Trump millions of of the',\n",
              "  'spoiler': 'The shooter’s autopsy was also conducted in another building, away from the victims.'},\n",
              " 631: {'roberta': 'Kin million million million the of of a',\n",
              "  'spoiler': 'Korean pear juice'},\n",
              " 632: {'roberta': 'KLer million million Trump million million million of of a a a',\n",
              "  'spoiler': 'Blake McIver'},\n",
              " 633: {'roberta': 'Kin million million million Trump million million of the the',\n",
              "  'spoiler': \"As of Sunday in Illinois, it is now legal to hunt for catfish with pitchforks or spears Farther west in Oregon, the state legislature banned the use of 'sky lanterns' In Pennsylvania, beer customers are now able to purchase six-packs at local grocery stores, and are able to purchase beverages in any quantity from a beer distributor.\"},\n",
              " 634: {'roberta': 'KKin million million millioner million million to the the a a a',\n",
              "  'spoiler': '\"Son, you go back to study. You’re not going to be a football player. You are Chinese.\"'},\n",
              " 635: {'roberta': 'K million million million Trump million million of of a a a',\n",
              "  'spoiler': 'most-searched people of 2013'},\n",
              " 636: {'roberta': 'Tt million million millioner million of of a',\n",
              "  'spoiler': 'stopped the band and laughed off her mistake declaring: \"S***, wrong words!\"'},\n",
              " 637: {'roberta': 'Kmaner million million million Trump million of a a',\n",
              "  'spoiler': 'The quote reads: \"I need feminism because I intend on marrying rich and I can’t do that if my wife and I are making .75 cent for every dollar a man makes.\"'},\n",
              " 638: {'roberta': 'KBBLLttts the', 'spoiler': 'a great big belly rub'},\n",
              " 639: {'roberta': 'L million million million Trump million million the of a a',\n",
              "  'spoiler': 'parody on tonight’s episode of \"Inside Amy Schumer.\"'},\n",
              " 640: {'roberta': 'C million million million of of a',\n",
              "  'spoiler': 'For the last question, the interviewer asked: \"A right triangle has a hypotenuse equal to 10 and an altitude to the hypotenuse equal to 6. Find the area of the triangle.\"'},\n",
              " 641: {'roberta': 'A million million million of of of a a a',\n",
              "  'spoiler': 'U.K. Prime Minister has said that negotiations with the European Union over the country’s exit will require the engagement of the Scottish, Welsh and Northern Ireland governments.'},\n",
              " 642: {'roberta': 'A million million million of of the',\n",
              "  'spoiler': 'it’s a calling because you find a deep sense of purpose and positive impact in your role'},\n",
              " 643: {'roberta': 'KKKCtt million millions of a',\n",
              "  'spoiler': '60-year-old woman sitting alone in her car reading a book'},\n",
              " 644: {'roberta': 'Lss the a a',\n",
              "  'spoiler': \"that Rey is in fact Emperor Palpatine's granddaughter\"},\n",
              " 645: {'roberta': 'BLmanmanmanererer million million millioner million Trump million of a',\n",
              "  'spoiler': 'Lisa Brown'},\n",
              " 646: {'roberta': 'Tt million million million the of a a a',\n",
              "  'spoiler': 'With the reported $115 million raised from the Ice Bucket Challenge, the ALS Association funded six research projects,'},\n",
              " 647: {'roberta': 'Ctt million million millioner million million the a',\n",
              "  'spoiler': 'These small circles are called \"Venus Holes\" and are formed on the lower back of women. However, some men can also have them and in that case they are called \"Apollo Holes\"'},\n",
              " 648: {'roberta': 'Kin million million million Trump million of of a a',\n",
              "  'spoiler': 'cupping'},\n",
              " 649: {'roberta': 'Tt million million million of a a',\n",
              "  'spoiler': 'This week security forces of the Palestinian Islamist movement Hamas have arrested dozens of Gaza residents for participating in protests against cuts to the electricity supply'},\n",
              " 650: {'roberta': 't million million million of the',\n",
              "  'spoiler': 'It depends on how you define \"sleep,\" but trees do relax their branches at night, which might be a sign of snoozing, scientists said.'},\n",
              " 651: {'roberta': 'TrumpC million Trump million million million of of to',\n",
              "  'spoiler': '\"there will always be exceptions.\"'},\n",
              " 652: {'roberta': 'Tt million millions the a', 'spoiler': 'Berlin'},\n",
              " 653: {'roberta': 'KTCt million million millioner million million of of the',\n",
              "  'spoiler': '(SPOT.ph) For those of us who don’t work there, attending a conference or a meeting at the Asian Development Bank (ADB) in Ortigas requires an appointment in the system, a photo ID taken on the spot, and a bout with two X-ray machines. But the bank’s real treasures aren’t in some vault, they’re in plain sight—the ADB canteen offers scrumptious meals from several Asian cuisines at affordable prices, as well as a bakery that serves up all kinds of breads, pies, and pastries.'},\n",
              " 654: {'roberta': 'Tt million million million the,',\n",
              "  'spoiler': 'breathe in for four, hold for seven, and breathe out for eight. You must inhale through your nose and exhale through your mouth.'},\n",
              " 655: {'roberta': 'KKBBKBCtt Trumps of,', 'spoiler': 'college graduation'},\n",
              " 656: {'roberta': 'KCt million million millions the a the',\n",
              "  'spoiler': 'chopsticks holder'},\n",
              " 657: {'roberta': 'Kin million million millions of the the',\n",
              "  'spoiler': 'great organizational tools'},\n",
              " 658: {'roberta': 'KTrumpCCinin million million million of,',\n",
              "  'spoiler': 'While members of sports teams — and other male-dominated groups like fraternities and the military — are indeed more likely to commit gang rapes than the average person, there are few conclusive studies to show that athletes are more likely to commit sex crimes.'},\n",
              " 659: {'roberta': 'T million million million of of the',\n",
              "  'spoiler': 'losing weight. Trim your pubic hair. make sure your boner is at maximum capacity there are some pills you can take.'},\n",
              " 660: {'roberta': 'Cin million million million of the the',\n",
              "  'spoiler': 'nothing morally wrong with voting for a flawed candidate if you think he will do more good for the nation than his opponent'},\n",
              " 661: {'roberta': 'BTt million million millions of the',\n",
              "  'spoiler': 'Watermelon seeds are packed with nutrients that are beneficial and healthy for you.'},\n",
              " 662: {'roberta': 'Tin million million millions of a',\n",
              "  'spoiler': 'Ben Bernanke'},\n",
              " 663: {'roberta': 'KL million million million Trump million millions the a a a',\n",
              "  'spoiler': 'Daily Show companion series will air Monday-Thursday at 11:30 p.m. ET/PT, beginning this fall'},\n",
              " 664: {'roberta': 't million million million of a a',\n",
              "  'spoiler': 'your activities could be monitored and private information collected'},\n",
              " 665: {'roberta': 'KCt million million millions of the',\n",
              "  'spoiler': 'sleep deprivation'},\n",
              " 666: {'roberta': 'L million million millions of a a a', 'spoiler': 'Feb. 24'},\n",
              " 667: {'roberta': '$ the',\n",
              "  'spoiler': 'In very small amounts (say, a few teaspoons), and if the blood is free from pathogens (such as the many blood-borne diseases), blood might not harm you. Beyond that, watch out.'},\n",
              " 668: {'roberta': 'C million million million of of the',\n",
              "  'spoiler': 'talk about their money'},\n",
              " 669: {'roberta': 'Tt million million millions of a',\n",
              "  'spoiler': 'polymyalgia rheumatica'},\n",
              " 670: {'roberta': 'KBACtts the',\n",
              "  'spoiler': 'the Cubs poured it on in the 10th inning, scoring 2 runs to provide the cushion they needed. World Series M.V.P. Ben Zobrist hit a go-ahead double, and Miguel Montero followed up with an R.B.I. single to give the Cubs the insurance run they needed.'},\n",
              " 671: {'roberta': 'C million million millions of a a a',\n",
              "  'spoiler': 'Don’t Don’t! Just don’t. Scope Out Your Next Pokéstop Before Going There Leave Your Phone On, But On Silent And Hidden Get Out And Walk Around Anyway'},\n",
              " 672: {'roberta': 'C million million million of of the',\n",
              "  'spoiler': 'drink coaster'},\n",
              " 673: {'roberta': 't million million million to the a the',\n",
              "  'spoiler': 'a pet'},\n",
              " 674: {'roberta': 'KLLtt Trump million million millions the a',\n",
              "  'spoiler': '\"I live inside my own heart, Matt Damon.\"'},\n",
              " 675: {'roberta': 'Aer million million million of a a a',\n",
              "  'spoiler': 'She creates a complex, full-bodied character without any body at all'},\n",
              " 676: {'roberta': 'TTTCttt million millions of a',\n",
              "  'spoiler': 'descended from a single European'},\n",
              " 677: {'roberta': 'KKKKBKKintererer Trump Trump million Trump million million millions of of the',\n",
              "  'spoiler': 'Dawn Grace lost her son four years ago, and now she meets the man who received his heart in a transplant.'},\n",
              " 678: {'roberta': 'Ttt million million millions of a',\n",
              "  'spoiler': 'It will change your life because you will be received so much differently'},\n",
              " 679: {'roberta': 'T million million million of of a',\n",
              "  'spoiler': 'THE MUSIC THE MYSTERIOUS ORIGINS THE CAMARADERIE THE TRADITION THE RELIEF'},\n",
              " 680: {'roberta': 'HillaryAKCCts of a',\n",
              "  'spoiler': 'adding a teaspoon of coconut oil to boiling water with a half-cup of non-fortified white rice, letting it simmer for 20 to 40 minutes, then refrigerating it for 12 hours may reduce the number of calories your body takes in by 50 to 60 percent'},\n",
              " 681: {'roberta': 'TTtss of a',\n",
              "  'spoiler': '1. There are many theories about why cats knead their paws into you, but many researchers agree that they’re trying to return the affection that you’re showing them. They just don’t know that it actually hurts. 2. Sure, it’s kind of gross when cats bring dead animals into the house, but they do it because they consider you family – and they’re trying to teach you how to catch your own prey. 3. When cats rub their head against you, they’re marking you as one of their own with the concentrated scent glands in their cheeks and head. Congratulations, you’re family. 4. A cat’s unending stare can be a little bit disconcerting, but don’t fret. First of all, they’re not really staring. Their eyes are evolved to blink less frequently than ours. But more than that, a steady, soft gaze means that they feel safe and comfortable with you. 5. Like most creatures, cats don’t like to make themselves vulnerable. So if they roll over and present their belly to you, it’s a sign of unwavering trust.'},\n",
              " 682: {'roberta': 'G million million million the of of a',\n",
              "  'spoiler': 'spy camera'},\n",
              " 683: {'roberta': 'LKererer million million millioner million Trump million the a of a',\n",
              "  'spoiler': \"Watch Hill, Rhode Island Hamilton Island on Australia's Great Barrier Reef Cuba southern Italy town of Puglia Kenya\"},\n",
              " 684: {'roberta': 'KKKLmanmanman Trumps a',\n",
              "  'spoiler': 'Louisville, Kentucky Where Opportunity Knox'},\n",
              " 685: {'roberta': 'C Trump Trump million million million Trump million of the',\n",
              "  'spoiler': 'racial'},\n",
              " 686: {'roberta': 'Tt million million million Trump million million of of a a',\n",
              "  'spoiler': '\"Men’s shirts have more room, so it’s easier to slip them off over your head; women don’t have that option,\"'},\n",
              " 687: {'roberta': 'C million million million Trump million of of a a',\n",
              "  'spoiler': 'the Stark sigil was back in its rightful place'},\n",
              " 688: {'roberta': 'Lererer millioner million million millioner Trump million the to',\n",
              "  'spoiler': 'lime green'},\n",
              " 689: {'roberta': 'VirginiaATTtts the', 'spoiler': 'Aaron Judge'},\n",
              " 690: {'roberta': 'Kt million million millions of a',\n",
              "  'spoiler': '460. Coimbatore, India 461. Bhubaneswar, India 462. Cairo, Egypt 463. Mangalore, India 464. Thiruvananthapuram, India'},\n",
              " 691: {'roberta': 'C million million millions of to a to', 'spoiler': 'pizza'},\n",
              " 692: {'roberta': 'Kt million million millions of the the',\n",
              "  'spoiler': '1. Ecuador 2. Nicaragua 3. Thailand 4. Belize 5. Panama'},\n",
              " 693: {'roberta': 'T million million million of the the',\n",
              "  'spoiler': 'This new study is one of the first to suggest that not getting enough sleep is one way of making our gut bacteria unhealthy'},\n",
              " 694: {'roberta': 'L million million millions of a a a',\n",
              "  'spoiler': 'In the remake, Belle (Emma Watson) is more than a bold, beautiful bookworm and devoted daughter -- she’s also a teacher and inventor. It was never clear who Belle’s mother was or what happened to her. But thanks to the remake, those questions have finally been answered. the audience assumes the Beast was born like that, his attitude simply accompanying the crown. In the remake, we found out that’s not entirely the case. Lyrics to one of the movie’s most popular songs, \"Gaston,\" sounded a little different than the original. the new Beast is highly educated'},\n",
              " 695: {'roberta': 'Lterer million million millioner million of to',\n",
              "  'spoiler': \"Acoustic Take On 'Adore You'\"},\n",
              " 696: {'roberta': 'in million million million of the the', 'spoiler': 'moral'},\n",
              " 697: {'roberta': 'Kin million million million Trump million of of a a',\n",
              "  'spoiler': 'a show whose tone shifted rapidly between horror, melodrama, action, and pure comedy. The name tells you all of that'},\n",
              " 698: {'roberta': 'BAKBBDt million million millions of a a',\n",
              "  'spoiler': 'The company wants you to change your last name to \"burger.\"'},\n",
              " 699: {'roberta': 'KKCCt million million millions of a',\n",
              "  'spoiler': 'Hot dogs'},\n",
              " 700: {'roberta': 'tt million million million the the',\n",
              "  'spoiler': \"Peter Facinelli 'Breaking Bad' Obsession\"},\n",
              " 701: {'roberta': 'Ln Trump Trump million million millions the a',\n",
              "  'spoiler': 'Chris Pratt'},\n",
              " 702: {'roberta': 'L million million millioner million million the of the the',\n",
              "  'spoiler': 'Diagnostic medical sonographer compliance officer Operations research analyst'},\n",
              " 703: {'roberta': 'T million million million of the the a the',\n",
              "  'spoiler': 'CEO Lawyer Media Salesperson Surgeon'},\n",
              " 704: {'roberta': 'KBBCCttt million millions of the', 'spoiler': 'rosemary'},\n",
              " 705: {'roberta': 'Lmanman Trump Trump million million million of a a a',\n",
              "  'spoiler': '1,080'},\n",
              " 706: {'roberta': 'T million million million of a a', 'spoiler': 'Google'},\n",
              " 707: {'roberta': 'TrumpTTCtt million million millions of of a a',\n",
              "  'spoiler': '1. CHILI PEPPERS 2. WHOLE GRAINS AND VEGETABLES 3. BLACK PEPPER 4. GINGER 5. GARLIC'},\n",
              " 708: {'roberta': 'C Trump Trump Trump million million million Trump million to a to',\n",
              "  'spoiler': 'Intrepid'},\n",
              " 709: {'roberta': 'Kt million million millioner million million to of a a a',\n",
              "  'spoiler': 'in the MORNINGS'},\n",
              " 710: {'roberta': 'TrumpBBCCC million million million Trump million million of of',\n",
              "  'spoiler': 'University of California, Los Angeles'},\n",
              " 711: {'roberta': 'BBATTttts the', 'spoiler': 'busybodies'},\n",
              " 712: {'roberta': 'ts the a', 'spoiler': '‘Pigeon game mode’'},\n",
              " 713: {'roberta': 'KLss the a', 'spoiler': 'Chal Na Katrina'},\n",
              " 714: {'roberta': 'LLerer million million million Trump million million the the a a a',\n",
              "  'spoiler': 'February 24 at 9 p.m.'},\n",
              " 715: {'roberta': 'KTCt million million millions of the',\n",
              "  'spoiler': 'seven hours'},\n",
              " 716: {'roberta': 'Lererer Trump million millioner million million million the a',\n",
              "  'spoiler': 'By watching the trailer for \"G.B.F.\"'},\n",
              " 717: {'roberta': 'KKCCCin million million millions the',\n",
              "  'spoiler': 'Then get in touch with Muncie Animal Shelter because they want you to take their dogs for a walk while you’re out capturing Meowths and Rattatas.'},\n",
              " 718: {'roberta': 'C Trump million million million Trump million of a a',\n",
              "  'spoiler': 'pussy'},\n",
              " 719: {'roberta': 'Cs of a a',\n",
              "  'spoiler': 'Donald Trump already has the authorization to build a wall on the U.S.-Mexico border'},\n",
              " 720: {'roberta': 'KKin million million million Trump million million of of of a a a the',\n",
              "  'spoiler': 'new wars in the Middle East'},\n",
              " 721: {'roberta': 'KBCCCt Trump Trump Trump million million million Trump million of a the',\n",
              "  'spoiler': 'Lawrence Phillips'},\n",
              " 722: {'roberta': 'Kin million million million Trump million the of a a a',\n",
              "  'spoiler': 'Carrer Avinyo'},\n",
              " 723: {'roberta': 'Tin million million million the of of a a',\n",
              "  'spoiler': 'International travel firm Marriott Rewards has committed to sponsor Johnson’s mission to catch the remaining Pokémon in Europe, Japan and Australia.'},\n",
              " 724: {'roberta': 'KBBTttts of a',\n",
              "  'spoiler': 'Tawny Nelson the older man wanted was for me to never give up and keep being an amazing mom'},\n",
              " 725: {'roberta': 'Kin million million million Trump million million to a of',\n",
              "  'spoiler': 'I mean localized, renewable energy creates backup support systems on the grid'},\n",
              " 726: {'roberta': 'KBKKBKCttt milliont millions of the',\n",
              "  'spoiler': \"The technical term for this is superfetation. In humans, it's possible, but it's very uncommon.\"},\n",
              " 727: {'roberta': 'KC Trump Trump Trump million million million Trump million of a a',\n",
              "  'spoiler': 'Colin Quinn'},\n",
              " 728: {'roberta': 'ts the a',\n",
              "  'spoiler': 'you can imagine an AI-based chatbot that truly comprehends intention behind language, a bot that you can have long-term discussions with about the news the way you would with a well-informed friend'},\n",
              " 729: {'roberta': 'BBBCCCttts of a', 'spoiler': 'fossil'},\n",
              " 730: {'roberta': '$ the', 'spoiler': '$1,427'},\n",
              " 731: {'roberta': '$ the',\n",
              "  'spoiler': '1. Lunch Boxes all of a sudden, some things started coming out of the woodwork,\" Dixey added. \"You go back to the old adage of supply and demand. All of a sudden there\\'s a glut of them 2. Beanie Babies the company that developed Beanie Babies abruptly announced that it would stop producing the toys 3. Baseball Cards America simply doesn\\'t have the love affair with its pastime that we once did. 4. Commemorative Plates there was never any market 5. Comic Books many people have of lost riches in the paperbacks of their youth, the reality is far more mundane'},\n",
              " 732: {'roberta': 'T million million million of of to',\n",
              "  'spoiler': 'The evacuation of civilians and opposition fighters from eastern Aleppo have been suspended after rebels opened fire on a convoy at one of the crossing points of the rebel-held enclave, according to Syrian state TV.'},\n",
              " 733: {'roberta': 'BBCCttts the',\n",
              "  'spoiler': 'lost and hidden city of Skara Brae'},\n",
              " 734: {'roberta': 'Lt million million millions the a a',\n",
              "  'spoiler': 'The Jump Sir Bradley Wiggins Jade Jones Kadeena Cox Gareth Thomas Jason Robinson'},\n",
              " 735: {'roberta': 'Kin million million million the of of a',\n",
              "  'spoiler': 'firing a gun'},\n",
              " 736: {'roberta': 'Cer Trump Trump Trump million million million of a',\n",
              "  'spoiler': 'If Elliott can give this offense what DeMarco Murray provided in 2014, I can see Romo playing all 16 games.'},\n",
              " 737: {'roberta': '$ the', 'spoiler': 'Pikaqiu'},\n",
              " 738: {'roberta': 'KLermanererer million million millioner millions a a',\n",
              "  'spoiler': \"Whitney Thompson Panache Lingerie's new Sculptresse line\"},\n",
              " 739: {'roberta': 'TCCt million million millions of the',\n",
              "  'spoiler': 'greenhouse gases'},\n",
              " 740: {'roberta': 'As the a a',\n",
              "  'spoiler': 'an exiled Russian prince sued MGM in 1933 over the studio’s Rasputin biopic, claiming that the American production did not accurately depict Rasputin’s murder. And the prince ought to have known, having murdered him.'},\n",
              " 741: {'roberta': 'GTTTttts of a',\n",
              "  'spoiler': '\"Cats have to be suspicious of the unknown: It could represent the danger of a snake or another predator,\"'},\n",
              " 742: {'roberta': 'C million million millions of the', 'spoiler': 'water'},\n",
              " 743: {'roberta': 'Kin Trump Trump million million million of a a a',\n",
              "  'spoiler': '\"Captain Phillips\"'},\n",
              " 744: {'roberta': 'KBTttt million million millions of of a',\n",
              "  'spoiler': \"It looks like it's all going to end in complete disaster but just as the big cat goes to pounce, the man spins around and starts laughing and playing with him like they're bezzie mates.\"},\n",
              " 745: {'roberta': '$ the', 'spoiler': 'a humble rag'},\n",
              " 746: {'roberta': 'KCt million million million Trump million millioner million million of of of to',\n",
              "  'spoiler': 'Blue Lives Matter is a movement that counters BLM by encouraging citizens to support police officers that put their lives on the line every single day for citizens across the country'},\n",
              " 747: {'roberta': 'KKin million million million of of the',\n",
              "  'spoiler': 'Houston'},\n",
              " 748: {'roberta': 'Kin million million million of the a a',\n",
              "  'spoiler': 'In May, after almost a decade and three moves, they finally decided to open the box, and found two hand-written notes wrapped around some cash, as well as wine glasses and bath products. The note to Kathy told her to buy pizza (\"or something you both like\") and prepare a bath, and the note to Brandon said, \"Go get flowers and a bottle of wine.\"In the end, Kathy wrote that they had been enjoying this gift for almost a decade, even though the box – which she calls \"the greatest wedding gift of all\" – hadn\\'t been opened.\"I realized that the tools for creating a strong, healthy marriage were never within that box – they were within us.'},\n",
              " 749: {'roberta': '$ the',\n",
              "  'spoiler': 'You see, if you swipe a chip card instead of inserting it into slot, the merchant is responsible for covering any fraudulent charges — not the bank.'},\n",
              " 750: {'roberta': 'Lter Trump Trump million million million Trump million the a a',\n",
              "  'spoiler': 'Elle'},\n",
              " 751: {'roberta': 'Tt million million millions of a',\n",
              "  'spoiler': \"A few years ago, one of McQueen's students wore his jeans for 15 months straight without a single wash and then tested the level of bacteria on them. The student-teacher team was surprised to find that the unwashed jeans carried nearly the same amount of bacteria as those same pants after they had been washed and then worn for another 13 days.\"},\n",
              " 752: {'roberta': 'ts of a',\n",
              "  'spoiler': 'Many tweeters noticed some strange behavior on Trump’s part; in addition to his seeming inability to control the volume of his speech, the man couldn’t stop sniffling (see the supercut below).'},\n",
              " 753: {'roberta': 'KBCCttts of a', 'spoiler': 'coconut oil pinch of turmeric'},\n",
              " 754: {'roberta': 'Bt million million millions the a a a',\n",
              "  'spoiler': 'It’s chock-full of nutrients and electrolytes like potassium, which not only keep you hydrated but also take longer for your body to process so you buy some time between bathroom breaks.'},\n",
              " 755: {'roberta': 'At million million of the',\n",
              "  'spoiler': 'laughing face emoji'},\n",
              " 756: {'roberta': 'Kt million million million Trump million million to a a',\n",
              "  'spoiler': 'Chris Crocker'},\n",
              " 757: {'roberta': 'KCCtt million millions of a',\n",
              "  'spoiler': 'Talk to a doctor before shoveling if you have a history of heart disease Stop immediately if you feel dizzy or tightness in the chest'},\n",
              " 758: {'roberta': 'Kt million million millions of a',\n",
              "  'spoiler': 'Newborn Smiling In Her Recently Deceased Father’s Gloves'},\n",
              " 759: {'roberta': 'Kiner million million millions of a',\n",
              "  'spoiler': 'number 7 with medium fries and a Coke'},\n",
              " 760: {'roberta': 'CaliforniaC million million millions of a a a',\n",
              "  'spoiler': 'Minnesota'},\n",
              " 761: {'roberta': 'KCt million million million Trump million million of to of of',\n",
              "  'spoiler': '\"If you are dropping off your son’s forgotten lunch, books, homework, equipment, etc., please TURN AROUND and exit the building,\" the sign read. \"Your son will learn to problem-solve in your absence.\"'},\n",
              " 762: {'roberta': 'C million million millions of a a a',\n",
              "  'spoiler': '2 human players and 2 CPUs resulted in no lag. gets better if you plug in the charger while playing'},\n",
              " 763: {'roberta': 'Lt million million million of of a',\n",
              "  'spoiler': 'stealing her jewelry'},\n",
              " 764: {'roberta': 'KCerer Trump Trump Trump million Trump Trump of a',\n",
              "  'spoiler': 'Cecily Strong'},\n",
              " 765: {'roberta': 'TBTTCttts the',\n",
              "  'spoiler': '1. When it comes to food, you won’t just eat any old thing. 2. Because you’re partial to fancy hipster foods, like expensive artisanal bread. 3. However, you’ll happily skimp on other items to afford your luxurious foodie lifestyle. 4. You know exactly what you like to drink and won’t order anything else, even if it’s cheaper. 5. But no glasses? No problem!'},\n",
              " 766: {'roberta': 'T million million million the of the',\n",
              "  'spoiler': 'Arizona'},\n",
              " 767: {'roberta': 'Kt million million millions the a a',\n",
              "  'spoiler': 'airlines rarely follow those recommendations'},\n",
              " 768: {'roberta': 'BCtt million millions of the',\n",
              "  'spoiler': 'Springfield, Oregon'},\n",
              " 769: {'roberta': 'BBttt million millions the a',\n",
              "  'spoiler': 'Norah picked her head up'},\n",
              " 770: {'roberta': 't million million million of a a',\n",
              "  'spoiler': 'Sony confirmed that the company was not against cross-console support and open to the idea of allowing PlayStation 4 players to play online against other consoles'},\n",
              " 771: {'roberta': 'Kin Trump Trump million million millions of a',\n",
              "  'spoiler': 'the canisters Bruce Wayne/Batman used have \"Pb\"'},\n",
              " 772: {'roberta': 'KTt million million millions of the',\n",
              "  'spoiler': '\"post-micturition convulsion syndrome.\"'},\n",
              " 773: {'roberta': 'TrumpC million million millions of a a a',\n",
              "  'spoiler': 'The end is not near, as a ‘near Earth pass’ isn’t actually that near at all – in fact, the asteroid is going to go past the Earth 7.3 million miles away (that’s 30 times further away than the moon), reports the Daily Star.'},\n",
              " 774: {'roberta': 't million million million of of of a the',\n",
              "  'spoiler': '\"Where do you see yourself in five years?\"'},\n",
              " 775: {'roberta': 'A million million of of of to the a a',\n",
              "  'spoiler': 'Dumb British blond fucks 15 million people at once.'},\n",
              " 776: {'roberta': 'LLL Trump Trump Trump million million millions a a',\n",
              "  'spoiler': 'Pat Patterson'},\n",
              " 777: {'roberta': 'CC Trump Trump Trump million million million Trump million of a a a',\n",
              "  'spoiler': 'bullshitting'},\n",
              " 778: {'roberta': 'KL Trump Trump Trump million million million Trump million of of a a a',\n",
              "  'spoiler': 'Yvette Nicole Brown'},\n",
              " 779: {'roberta': 'VirginiaKKKGKKCint millionin million million millions the a the a of',\n",
              "  'spoiler': 'Florida'},\n",
              " 780: {'roberta': 'Lin million million millions of of a a',\n",
              "  'spoiler': 'not have enough happy memories to produce a Patronus'},\n",
              " 781: {'roberta': 'C million Trump million million million of of a a',\n",
              "  'spoiler': 'Democrat Jason Kander looks to be running very strongly in the exit polls.'},\n",
              " 782: {'roberta': 'Lt million million million of a a',\n",
              "  'spoiler': 'from 33 seconds to 44 minutes'},\n",
              " 783: {'roberta': 'T million million million Trump million million of of a a a',\n",
              "  'spoiler': 'he could not locate the guy sitting right in front of him.'},\n",
              " 784: {'roberta': 'A million million the a of a a',\n",
              "  'spoiler': 'monoclonal antibodies as scientific tools and for the treatment of diseases'},\n",
              " 785: {'roberta': 't million million million of the',\n",
              "  'spoiler': 'BOOM – Portugal Meadows in the Mountains – Bulgaria Burning Man - Black Rock Desert Electric Forest - Rothbury, Michigan AfrikaBurn - Tankwa Karoo'},\n",
              " 786: {'roberta': 'CCin million million millions a a a',\n",
              "  'spoiler': 'white pepper'},\n",
              " 787: {'roberta': 'Ler million million millioner million of the a a',\n",
              "  'spoiler': 'anal sex'},\n",
              " 788: {'roberta': 'Tt million million millions of the',\n",
              "  'spoiler': \"earned much of their fortune making unintentionally laughable direct-to-video movies Mary-Kate was somehow linked with actor Heath Ledger's death successful fashion designers people began noticing just how different Mary-Kate looked compared to her younger self and her fraternal twin sister Fuller House, the Olsen twins were the only ones to decline Elizabeth is outgoing, vivacious, and comes off like an extremely well-adjusted regular person\"},\n",
              " 789: {'roberta': 'TTTCTtt million million millions of of the',\n",
              "  'spoiler': 'Klefki Rotom Luvdisk Unknown (Unown) Vanilluxe'},\n",
              " 790: {'roberta': 'Attss of a a',\n",
              "  'spoiler': 'I don’t pretend to be an ordinary housewife People always assume you’re going to carry a grudge, but I don’t do that there are plenty of fish in the sea Disagreements are an inevitable part of any relationship I’ve always admitted that I’m ruled by my passions'},\n",
              " 791: {'roberta': 'T Trump Trump Trump million Trump Trump of of a',\n",
              "  'spoiler': 'Hillary Clinton'},\n",
              " 792: {'roberta': 'KBTttt million million millions of the',\n",
              "  'spoiler': 'This was clearly a crack house, and he was standing there with a dealers stash. People have gotten killed for less. Well guess what? The dealer comes back!'},\n",
              " 793: {'roberta': 'A million million million the a a a',\n",
              "  'spoiler': 'China is too quick to rebalance its services sector'},\n",
              " 794: {'roberta': 'KKABBTtts the',\n",
              "  'spoiler': 'the plant the dog was chewing on was deadly water hemlock'},\n",
              " 795: {'roberta': 'BBttt million million millions a a',\n",
              "  'spoiler': 'The video below shows the stunned cleaner initially refusing to accept the tip, before another hotel worker reassured her by saying, \"You deserve it.\"'},\n",
              " 796: {'roberta': 'TrumpC Trump Trump million million million of a a',\n",
              "  'spoiler': 'Christopher Suprun'},\n",
              " 797: {'roberta': 'KKBtts of a',\n",
              "  'spoiler': 'Rachel Crawley High fat vegan plant based diet (nothing processed) Real whole foods'},\n",
              " 798: {'roberta': 't million million of the',\n",
              "  'spoiler': 'Julian Assange’s internet link has been intentionally severed by a state party'},\n",
              " 799: {'roberta': 'KKel Trump Trump Trump million Trump Trumps the a',\n",
              "  'spoiler': 'Richard Belzer'}}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/NLP_Project/roberta_10_eval_1b.json\"\n",
        "\n",
        "with open(file_path, \"w\") as json_file:\n",
        "    json.dump(results, json_file, indent=4)"
      ],
      "metadata": {
        "id": "yQwMqqDxeOZo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96725cc93eb7421eb998479d96405d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2cc365952c44dedab6bef898ea53430",
              "IPY_MODEL_5e14698d98134d1f952d0cd9b9f3f1bf",
              "IPY_MODEL_8052d45e020b4d91b1e5e1375197f7f7"
            ],
            "layout": "IPY_MODEL_fc369d8d05b149d792982454ba7e3e0a"
          }
        },
        "d2cc365952c44dedab6bef898ea53430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62715876936d44a69b78605f34a184a9",
            "placeholder": "​",
            "style": "IPY_MODEL_1dff2d9ad0774c1187058be064b7582e",
            "value": "100%"
          }
        },
        "5e14698d98134d1f952d0cd9b9f3f1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2010585b383746d09ab54e16bb3c26d3",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_659f74b2187a4cd0a466f9942b970b1b",
            "value": 24
          }
        },
        "8052d45e020b4d91b1e5e1375197f7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14d659e8100149c59554d386954b95a5",
            "placeholder": "​",
            "style": "IPY_MODEL_1dfccfdc433c41fab354a3155b7d0bef",
            "value": " 24/24 [00:13&lt;00:00,  1.89ba/s]"
          }
        },
        "fc369d8d05b149d792982454ba7e3e0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62715876936d44a69b78605f34a184a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dff2d9ad0774c1187058be064b7582e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2010585b383746d09ab54e16bb3c26d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "659f74b2187a4cd0a466f9942b970b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14d659e8100149c59554d386954b95a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dfccfdc433c41fab354a3155b7d0bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8430c276fac34ac0815e768501b8aee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f13c542ec2b24418844413297c56beb0",
              "IPY_MODEL_f02b8dc91c5b421b9b3ae9b5ccc7067a",
              "IPY_MODEL_2f7f4ad83b0c4268bcbf71355a74398e"
            ],
            "layout": "IPY_MODEL_5596b9c1741f43569f7820574f0ce620"
          }
        },
        "f13c542ec2b24418844413297c56beb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97388821d0d468cab5f950500aa21c7",
            "placeholder": "​",
            "style": "IPY_MODEL_79e8335658bc47e2ad5951570af4f031",
            "value": "100%"
          }
        },
        "f02b8dc91c5b421b9b3ae9b5ccc7067a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbf9e8ced3441f19f7532e8f0b2e7bb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dae2f0bb662641c4829a6c2ed02ca7f0",
            "value": 2
          }
        },
        "2f7f4ad83b0c4268bcbf71355a74398e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaa8abac6d994fb0b80c3d8a9c8c1de9",
            "placeholder": "​",
            "style": "IPY_MODEL_25b64cc026b54093a9e50127396fd0a7",
            "value": " 2/2 [00:00&lt;00:00,  2.19ba/s]"
          }
        },
        "5596b9c1741f43569f7820574f0ce620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d97388821d0d468cab5f950500aa21c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79e8335658bc47e2ad5951570af4f031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fbf9e8ced3441f19f7532e8f0b2e7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dae2f0bb662641c4829a6c2ed02ca7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaa8abac6d994fb0b80c3d8a9c8c1de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25b64cc026b54093a9e50127396fd0a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e500e6899224abdb155d5e38d74d328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b7c23d7ce9643a0b9ddab5824f766c7",
              "IPY_MODEL_38e0ff606bb84f4199f388d58ac1f634",
              "IPY_MODEL_b84a8d79d49048ff9ecdebf463bbbf23"
            ],
            "layout": "IPY_MODEL_26b9b8b97d914734b736c1844fa29d9e"
          }
        },
        "3b7c23d7ce9643a0b9ddab5824f766c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02612a6541b941abb99ef77bbd4f9a27",
            "placeholder": "​",
            "style": "IPY_MODEL_cc9797d4d2c34ef0b60cdd6d72e4756e",
            "value": "100%"
          }
        },
        "38e0ff606bb84f4199f388d58ac1f634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c3301059fc84ee193a100ca59097c64",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce1712204ae24f7abedf039693ea7748",
            "value": 800
          }
        },
        "b84a8d79d49048ff9ecdebf463bbbf23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c2b5efe7ae546869c5adc65e877fe5f",
            "placeholder": "​",
            "style": "IPY_MODEL_3569943e443147c7844924c3c03d3fef",
            "value": " 800/800 [05:18&lt;00:00,  3.25ba/s]"
          }
        },
        "26b9b8b97d914734b736c1844fa29d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02612a6541b941abb99ef77bbd4f9a27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc9797d4d2c34ef0b60cdd6d72e4756e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c3301059fc84ee193a100ca59097c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1712204ae24f7abedf039693ea7748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c2b5efe7ae546869c5adc65e877fe5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3569943e443147c7844924c3c03d3fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}